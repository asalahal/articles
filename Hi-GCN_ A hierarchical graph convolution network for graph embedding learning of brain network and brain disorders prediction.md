                                                                    Contents lists available at ScienceDirect
                                                     Computers in Biology and Medicine
                                                 journal           homepage:                          http://www.                     elsevier.co                m/locate/co                    mpbiomed
Hi-GCN:  A  hierarchical  graph  convolution  network  for  graph  embedding
learning  of  brain  network  and  brain  disorders  prediction
Hao  Jiang       a,  Peng  Caoa,b,*,  MingYi  Xu                 a,  Jinzhu  Yang         a,  Osmar  Zaiane          c
aComputer Science and Engineering, Northeastern  University, Shenyang, China
bKey Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, Shenyang, China
cAmii, University of Alberta, Edmonton, Alberta, Canada
ARTICLE                          INFO                     ABSTRACT
Keywords:                                                 Purpose: Recently, brain connectivity networks have been used for the classification of neurological disorder,
Alzheimer‚Äôs disease                                       such as Autism Spectrum Disorders (ASD) or Alzheimer‚Äôs disease (AD). Network analysis provides a new way for
Autism spectrum disorder                                  exploring the association between brain functional deficits and the underlying structural disruption related to
Brain network                                             brain disorders. Network embedding learning that aims to automatically learn low-dimensional representations
Graph classification                                      for brain networks has drawn increasing attention in recent years.
Graph convolutional network                               Method: In this work we build upon graph neural network in order to learn useful representations for graph
                                                          classification in an end-to-end fashion. Specifically, we propose a hierarchical GCN framework (called hi-GCN) to
                                                          learn the graph feature embedding while considering the network topology information and subject‚Äôs association
                                                          at the same time.
                                                          Results: To demonstrate the effectiveness of our approach, we evaluate the performance of the proposed method
                                                          on the Alzheimer‚Äôs Disease Neuroimaging Initiative (ADNI) dataset and Autism Brain Imaging Data Exchange
                                                          (ABIDE) dataset. Extensive experiments on ABIDE and ADNI datasets have demonstrated competitive perfor-
                                                          mance of the hi-GCN model. Specifically, we obtain an average accuracy of 73.1%/78.5% as well as AUC of
                                                          82.3%/86.5% on ABIDE/ADNI. The comprehensive experiments demonstrate that our hi-GCN is effective for
                                                          graph classification with brain disorders diagnosis.
                                                          Conclusion: The proposed hi-GCN method performs the graph embedding learning from a hierarchical perspective
                                                          while considering the structure in individual brain network and the subject‚Äôs correlation in the global population
                                                          network, which can capture the most essential embedding features to improve the classification performance of
                                                          disease diagnosis. Moreover, the proposed jointly optimizing strategy also achieves faster training and easier
                                                          convergence than both the hi-GCN with pre-training and two-step supervision.
1. Introduction                                                                                 ASD or AD dementia exhibit alterations of functional cortical connec-
                                                                                                tivity in rs-fMRI analyses. Brain functional connectivity (FC) derived
    Brain disorders such as Alzheimer‚Äôs disease (AD) [1‚Äì4] and Autism                           from rs-fMRI data has become a powerful approach to measure and map
spectrum disorder (ASD) [5,6] are considered in terms of disruptions of                         brain activity. Recent studies have shown that rs-fMRI based analysis of
the normal-range operation of brain functions. While psychiatric dis-                           brain connectivity is effective in helping understand the pathology of
orders are diagnosed based on symptom scores from clinical interview,                           brain diseases, such as autism spectrum disorder (ASD) and Alzheimer‚Äôs
there are no existing gold standards that can be used for definitive                            disease (AD) [1].
validation. Resting state functional MRI (rs-fMRI) provides us with in-                             The rs-fMRI data has complex structure, which are inherently rep-
formation about the default state of the brain, and allows us to evaluate                       resented as a network with a set of nodes and edges [7]. Many works
functional connectivity and its alterations in brain disorders. It is a                         focus on modeling the whole brain rs-fMRI as a network [8‚Äì10] and
method used to evaluate regional interactions that occur in a resting                           extracting representation from network [11‚Äì13]. Recently, functional
state when an explicit task is not being performed. The patients with                           connectivity networks constructed from rs-fMRI hold great promise for
 *Corresponding author. Computer Science and Engineering, Northeastern University, Shenyang, China.
    E-mail address: caopeng@cse.neu.edu.cn (P. Cao).
https://doi.org/10.1016/j.compbiomed.2020.104096
Received 1 July 2020; Received in revised form 24 October 2020; Accepted 24 October 2020

H. Jiang et al.
distinguishing brain disorder patients from NC(Normal control) [14,15].                                             individual brain functional network and the whole population network,
It  can  be  regarded  as  a  graph  classification  problem.  It is  important  to                                 respectively. A compact representation of brain functional network can
extract       a       useful       network       representation       from       the       brain       network       to be learned automatically by a graph-level embedding learning GCN, we
facilitate a range of learning tasks, such as brain network classification                                          called it f-GCN. Then, another GCN, called p-GCN, further updates each
and     network     visualization     [16,17].     The     commonly     used     features     are                   node‚Äô     s embedding of the graph data, via aggregating the representations
calculated       based       on       graph-theoretic       analysis,       such       as       clustering       co-of  its   neighbors  and  itself.   During  the   training  of   p-GCN  on   the  popu-
efficients     and     local    clustering     coefficients     [14],    which     are    calculated                lation network, a graph kernel is used directly to measure the similarity
based on each ROI‚Äô     s local connectivity pattern. However, hand-crafted                                          between           pairs           of           brain           networks.          It           can           capture           local           properties
graph  features  may  not  be  precise  enough  to  represent  or  characterize                                     considering the graph structure when calculating the similarity of nodes
the   network   [18,19].   In   recent   years,   the   high-level   feature   represen-                            with  network  structure.  Through  the  joint  learning  of  hi-GCN,  a  high-
tation of deep convolutional neural networks has proven to be superior                                              level       embedding      of      brain      network      representation       can       be      effectively
to      hand-crafted      low-level      and      mid-level      features      [20‚Äì          22].      However,     learned       by       deriving       the       structure       of       brain       regions       and       aggregating
convolutional      neural      networks      and      recurrent       neural      networks,      have               embedding  of  its  neighboring  subjects  in  an  end-to-end  fashion  with  a
mainly         focused         on         the         grid-structured         inputs         rather         than         network global          supervision          such          that          the          embedding          learned          is          useful          for
structure data.                                                                                                     classification.
     Network embedding [23‚Äì          25] is an approach that is used to transform                                        The main contributions of this paper can be summarized as follows:
nodes    in    the    network    into    a    lower    dimensional    representation    whilst
maximally            preserving            the            network            structure.            Embedding            a            brain 1.     In this paper, we focus on learning deep representations from fMRI
network into a meaningful low-dimensional representation can improve                                                     brain   connectivity   networks,   where   each   brain   network   represents
the classification performance of disease diagnosis. Kipf &               Welling [26]                                   the brain activity patterns of a particular subject. We propose a Hi-
proposed     graph     convolutional     networks     (GCN)     as     an     effective     graph                        erarchical GCN (hi-GCN) model which is capable of jointly learning
embedding     model     that    naturally    combines    structure     information    and                                the  graph  embedding  from  both  the  aspects  of  the  brain  functional
node  features in  the  learning process. Recent works  have  applied  GCN                                               network   and   the   population   network   at   the   same   time,   which   can
on   the   functional   network   derived   from   rs-fMRI   data   to   extract   latent                                extract the most useful spatial features and capture the most essential
features from graph [27‚Äì          30]. However, the previous works on network                                            embedding  features  coherently.  We  assume  that  the  brain  network
embedding learning of brain functional network consider each instance                                                    exhibit    s    network    structures    at    two    levels:    1)    the    region-to-region
(subject) independently in the learning process, ignoring the association                                                brain activity correlations in the brain network, and 2) the subject-
among  instances  (subjects).  The  association  among  the  instances  (sub-                                            to-subject  relationship  in  the  population  network.  The  hierarchical
jects) is critical and the neighborhood information should be considered                                                 structural patterns is crucial for learning more accurate representa-
during           the          embedding          learning.          Incorporating           and          preserving          the tions of the brain network. Specifically, our hi-GCN model has a hi-
intrinsic    association    can    promote    to    learn    a    better    embedding    of    the                       erarchical architecture with a two-level GCN. Its first level is to learn
brain functional network. The classification methods with modeling the                                                   a network embedding from the topological structure of the original
correlation among instances provide a natural framework to analyze the                                                   connectivity       network.       Its       second       level,       on       the       other       hand,       is       to
relationships    among    the   instances    in    the    data    set.    In    this    framework,                       incorporate    contextual    correlation    among    the    subjects    to    enhance
nodes   can   represent   individuals   (patients   or   healthy   controls)   accom-                                    the   semantic   information.   Through   the   joint   learning   of   hi-GCN,   a
panied  by  a set  of features, while the graph  edges incorporate associa-                                              high-level embedding of brain network representation can be effec-
tions        between        subjects.        The        edge        between        individuals        is        implicitly tively learned by  deriving the  structure of brain regions  and  aggre-
obtained   by   calculating   pairwise   similarities.   Although   no   explicit   as-                                  gating         embedding         of         its         neighboring         subjects         in         an         end-to-end
sociation   between   nodes   exists,   the   correlation   between   nodes   can   be                                   fashion with a global supervision such that the embedding learned is
implicitly  obtained  by  calculating  pairwise  similarities  to  improve  the                                          useful  for  classification.  With  the  two-level  GCN  architecture,  each
classification performance.                                                                                              brain network is mapped into an embedding.
     In our work, we formulate disease diagnosis as a graph classification                                          2.            We    develop    an    optimization    strategy    for    jointly    learning    the    pro-
problem and attempt to advance deep learning for graph-structured data                                                   posed    hierarchical    GCN    model    to    handle    network    data    with    more
with GCN. To consider the correlation among the samples in population,                                                   efficiency. The source code for the proposed architecture is publicly
the  population is treated as a network and the aim is to learn network                                                  available at: https://github.com/hao jiang1/hi-GCN.
embedding for subjects based on the graph structure. Accurate learning                                              3.     Extensive       experiments       on       two       real       medical       clinical       applications:
of  the  network  embedding  with  correlation  and  estimation  of  the  sub-                                           diagnosis of ASD and diagnosis of Alzheimer‚Äô     s disease, showing the
ject‚Äô     s   correlation   play   critical   roles   of   population   network   analysis   in                          effectiveness   of   the   proposed   framework.   The   experimental   results
improving the performance of graph data classification. We propose an                                                    demonstrate    that    network    embedding    learning    from    both    correla-
end-to-end    network   embedding    learning   network   for    brain   functional                                      tions within individual brain network and global population network
connectivity        networks        from        both        perspective        of        network        level        and can improve prediction performance.
population level. The framework can analyze and classify the functional
connectivity in fMRI data directly in an end-to-end trainable fashion. In                                                The rest of the paper is organized as follows. In Section 2, we provide
the present work, the aim is to achieve a mapping function from a brain                                             a      introduction      of      fMRI      network      and      GCN.      A      detailed      mathematical
network to a low-dimensional vector representation by preserving both                                               formulation of Hierarchical GCN is provided in Section 3. In Section 4,
topology structure within population network and the structure within                                               we      conducted      extensive      experiments      to      verify      the      advantage      of      our
individual brain functional network.                                                                                method for diagnosis of ASD and diagnosis of Alzheimer‚Äô     s disease. The
     To    learn    the    network    feature    embedding    considering    the    network                         conclusion is drawn in Section 5.
topology   information   and   subject‚Äô     s   association   at   the   same   time,   two
major challenges in building up such a joint graph analysis framework                                               2.                Preliminaries
are: how to build an unified framework for network embedding learning
and how to optimize the end-to-end network embedding framework? To                                                  2.1.                 Problem  setup
solve the two challenges, we proposed an effective learning strategy for
modeling          the          brain          connectivity          network          and          population          network We      define      an      undirected      graph      for      each      subject,             Ni=  {Ri,ùíúi},
simultaneously  in  a  hierarchical  GCN  framework  (called  hi-GCN).  The                                         where        Ri ={r1,‚Ä¶          ,rM}is     the     set     of    M     nodes,     and        Ai‚ààRM√óM is     the
                                                                                                                                           i        i
framework     involves     two     individual     GCNs     for     modeling     the     graph     of

H. Jiang et al.
                                                                             Fig.  1.            The  procedure of brain  FC network construction.
adjacency matrix describing the network‚Äô     s connectivity in the i-th sub-                                               where Cov(vi,vj)is the cross covariance between vi and vj, and œÉ                                     v denotes
ject, where M is the number of ROI. Here M   =  116. The embedding of                                                      the standard deviation of v.
each vertex in R                         is learned during the GCN training, therefore the initial                               The  final  constructed  network  is  a  graph  where  the  nodes/vertices
value of R       i is set to be one. In the graph classification setting, we have a                                        represent              brain              regions              and              the              edges              are              the              region-to-region
set of graphs {N         1,‚Ä¶          ,ND}, where D is the size of dataset. Each graph N                       i is        correlations.
associated with a label yi.
     In such setting of the population analysis, each subject acquisition is                                               2.3.                 Graph  convolutional networks
represented     by    a    node    and    the    pairwise    similarities     are    modeled    via
edges   connecting   the   nodes.   Given   a   collection   of   images   modeled   as                                          The  power  of  deep  learning  models  lies  in  enabling  automatic  dis-
graphs  N     i and the associated label yi, we construct a global population                                              covery  of  latent  or  abstract  higher-level  information from  high  dimen-
network  ÃÇN        ={ÃÇR          ,ÃÇùíú}, where  ÃÇùíú   is the adjacency matrix describing the                                  sional neuroimaging data, which can be an important step to understand
pairwise similarities between each pair of subjects with brain networks.                                                   complex     mental     disorders.     However,     Convolutional     Neural     Networks
Each       subject      is       represented       by       a       vertex  ÃÇr        and       is       associated       with      a (CNNs)   do   not   generalize   to   irregular   graphs   since   discretized   convo-
network data. The definition of the graph‚Äô     s edges is critical in order to                                             lutions     are     only     defined     for     regular     domains.     Therefore,     we     use     the
capture the underlying structure of the data and explain the similarities                                                  spectral  approach   which  provides   a  well-defined  localization  operator
between each pair of the N. We employ a graph kernel to estimate the                                                       on graphs to define graph convolutions.
S(N   i,N  j)between two network input of subjects. We will introduce it in                                                      Graph   Convolutional   Networks   (GCN)   aim   to   extend   the   data   rep-
Section    3.4.    The    diagnosis    with    brain    functional    network    is    a    typical                        resentation   and   classification   capabilities   of   convolutional   neural   net-
graph   classification  problem.  The  task   of   the   graph  classification   is  to                                    works,           which           are           highly           effective           for           signals           defined           on           regular
take      the     brain      network     as      input     and     predict      its     corresponding      label           Euclidean   domains,   e.g.   image   and   audio   signals,   to   irregular,   graph-
(clinical status), i.e. patient with disorder or normal control. The aim is                                                structured   data   defined   on   non-Euclidean   domains.   The   graph   convo-
to  learn  the  most  essential  embedding  by  taking  full  advantage  of  the                                           lution   is  employed  directly   on  graph  structured  data  to  extract  highly
correlation   and   structure   within   the   graph   and   accurately   predict   the                                    meaningful patterns and features in the space domain. Formally, we are
label of a given network.                                                                                                  given   the   adjacency   matrix ùíú‚ààR                     n√ón.   GCN   is   stacked   by   several   con-
                                                                                                                           volutional layers and a single convolutional layer can be written as:
2.2.                 Functional  connectivity  network                                                                                          (Àú    1   /2     1   /2          )
                                                                                                                           E  (l+1 )=ReLu         D        ùíúÀúÃÇD       E  (l)W  (l)  ,                                                                                                                                                                                                                                                                                                                                                                                                (2)
     Recent   advances   in   neuroimaging   have   led   to   significant   improve-
ments in the spatial resolution of fMRI. As a result, fMRI is currently a                                                  where   ÃÇùíú=ùíú+I              n  ,     ÀúDii= ‚àë   ùíúÃÇij,   W                            is  a  trainable  weight  matrix,   E(l+1)
widely-used       tool       that       measures       brain       activity       by       detecting       changes                                                      j
associated with blood flow. The network of the brain can be modeled as                                                     are the  node embeddings (i.e.,  ‚Äú        messages‚Äù               omputed after l steps of the
a    graph    consisting    of    brain    regions    as    the    nodes    and    their    functional                     GCN,         and         the         node         embeddings                     E(l)generated         from         the         previous
connectivity as the edges. Brain connectivity networks are widely-used                                                     message-passing step.
to model inter-regional functional connections, and are typically infer-                                                         GCN can be considered as a Laplacian smoothing operator for node
red from the input fMRI data of each subject.                                                                              features    over    graph    structures.    The    architecture    of    GCN    consists    of    a
     The    construction    of    brain    network     from    fMRI    involves    two    steps,                           series   of    convolutional   layers,   each   followed   by   Rectified   Linear   Unit
which is shown in Fig. 1. At first, the mean time series for a set of regions                                              (ReLU)  activation  functions  to  increase  non-linearity.  The  first  hidden
extracted from the automated anatomical labeling (AAL) atlas [31] are                                                      layer E     (0)is set to the input original node features. All layers share the
computed    and    normalised    to    zero    mean    and    unit    variance.    Then,    we                             same  adjacency  matrix.  A  full  GCN  run  L  iterations  of  Equation  (2)  to
compute the region-to-region brain correlations by choosing a metric for                                                   generate the final output node embeddings, E                               (L).
measuring similarity. Pearson‚Äô     s correlation (PC) analysis between BOLD                                                      To       localize       the       filter      and       reduce       the       number      of       parameters,       we
fMRI signals of any pair of ROIs is the most popular network construc-                                                     employ the Chebyshev Polynomials to approximate the convoluational
tion    method.    Each    vertex    ri        represents    a    brain    region,    and    the    corre-                 kernels. The computational complexity can be reduced with K-localized
sponding   time   series   is   indicated   as   vi.   Pearson   correlation   coefficient                                 convolutions      through      the      polynomial      approximation      [32].      By      recur-
between the fMRI time series vi at the vertex i and the fMRI time series vj                                                sively          applying          a          stack          of          graph          convolutions          with          the          1st-order
at the vertex j is given by                                                                                                approximation,    K-localized    convolutions     are    computed     to    exploit    the
          )=Cov             v,v)                                                                                           information from the K-order neighborhood of central nodes.
Q       r,r              i   j                                                                                (1)
     i   j          œÉ v œÉ v
                       i   j

H. Jiang et al.
                                                                Fig. 2.            The  architecture of the  proposed  hi-GCN model for  brain network  classification.
                                                                     Fig.  3.            An illustration  of the  procedure of network  embedding learning  in hi-GCN.

H. Jiang et al.
3.                 Hierarchical GCN                                                                                                              brain regions) denotes the assignment matrix, which indicates whether a
                                                                                                                                                 node belongs to a specific subnetwork, and ùíúcoar denote the adjacency
3.1.                 The  network  architecture  of  hierarchical  GCN                                                                           matrix of the coarsened subnetworks.
                                                                                                                                                                 H       (h )            (h ))T
      An       illustration       of       the       proposed       hierarchical       graph       convolutional                                 ùíúcoar     = ‚àë        C      ùíúext     C          ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (5)
networks (hi-GCN) is shown in Fig. 2 for graph representation learning.                                                                                         h =1
It consists of the following:
                                                                                                                                                 where ùíúext is an adjacency matrix only consisting of the edges between
1)     f-GCN:           learning           the           latent           embedding           representation           of           graph        subnetworks.
      instance   based   on    each   ROI‚Äô     s   connectivity   into   a   meaningful   low-                                                         For    the    subnetworks,    the    pooling    operator    tries    to    summarize    the
      dimensional representation for each brain network instance;                                                                                nodes‚Äô                   features         and         obtain         a         representation         for         the         corresponding
2)     Network      similarity      estimation      in      the    ÃÇN                  :      calculating      the      network                  supernode  of  the  subnetworks.  With  the  structure  of  the  subnetworks,
      similarity for each pair of network instances with graph kernel;                                                                           we   employ   a   pooling   operator   based   on   the   graph   spectral   theory   by
3)     p-GCN:    based    on    the    embedding    representation    learned    by    f-GCN                                                     facilitating the eigenvectors of the Laplacian matrix of the subnetworks.
      and the graph pairwise similarity, a new representation of a node is                                                                       Let Œò     c denote the pooling operator consisting of all the c-th up-sampled
      further learned by aggregating the embedding of all neighbors in the                                                                       eigenvectors from all the subnetworks.
      population network;                                                                                                                        E  c =Œò     T E                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (6)
                                                                                                                                                             c
4)     Both f-GCN and p-GCN are jointly updated via backpropagation.
                                                                                                                                                 where E         c ‚ààR      H√óPl    (Pl indicates the l-th embedding dimensionality) is the
      The     procedure    of    the     embedding    learning     of    the    brain     functional                                             pooled result using the c-th pooling operator.
network is shown in Fig. 3. It includes two phases: network embedding                                                                                  Finally,  we  obtain  the  node  features    E                             coar of    N    coar.  Instead  of  using
learning within each brain network and embedding learning within the                                                                             the results pooled by all the pooling operators, we choose to use the first
population  network.  F-GCN  produces  an  embedding     E                      for  all  network                                                Z of them as E             coar   =[E     0,‚Ä¶          ,EZ]is the final pooled results. With ùíúcoar and
instances,   then   the   learned   embedding   is   fed   to   the   second   model   (p-                                                       E  coar, we can learn higher level representations of the coarsened graph
GCN)    to    generate   a    refined    embedding   ÃÇE      ‚ààRD√óÃÇP                              where    the   d-th   row                       that exploits the subgraph structure as well as the node features of the
describes     a     latent     representation     of     the     brain     network     from     the     d-th                                     input graph.
subject    and  ÃÇP      is    the    dimensionality    of    the    final    network    embedding.
Intuitively,   ÃÇE                         with    leveraging    the    neighborhood    data    can    be    used    as                           3.3.                 p-GCN
features for the tasks of brain disorder disease diagnosis. It is important
to   learn   graph  representations   that  can   capture   rich  information   from                                                                   p-GCN       module        further        learns       the        graph        embedding        by        message
both the fMRI network and the population network. The goal of hi-GCN                                                                             passing according to 1) the network embedding E                    describing each sub-
for the graph classification task is to learn a nonlinear mapping from a                                                                         ject, and 2) the adjacent matrix between samples  ÃÇùíú. As shown in Fig. 3,
brain network to an embedding vector. The procedure is defined as:                                                                                                                                                                                                         0
                                                                                                                                                 the node features map of the input layer of p-GCN is defined as: ÃÇE                                                           =E     L,
hi       GCN                                                              :                 N                                       ‚Üí        ÃÇe    ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (3)  where  EL is the set of node embedding features learned by f-GCN. The
which involves two functions:                                                                                                                    main   idea   is   to   generate   a   node      N                ‚Äòs   representation   by   aggregating   its
                                                    (          )                                                                                 own features ÃÇe             i and neighbors‚Äô         features ÃÇe             j, where j  ‚ààNeighbor                       (i). p-GCN
f    GCN              (N    )=e         ;                                                                          p     GCNe   ,ÃÇA=[ÃÇe    ,ÃÇy  ]                                                                                                                                                                                                                                                                                                            (4) also stack multiple graph convolutional layers to extract high-level node
                                                                                                                                                 representations. The model inductively learns a node representation by
      By   training   the   entire   network   end-to-end,   the   hi-GCN   deduces   an                                                         recursively   aggregating   and   transforming   feature   vectors   of   its   neigh-
optimal network embedding for each brain network.                                                                                                boring subjects.
      In the next subsection, we introduce details about the two parts of hi-                                                                          The definition of the graph‚Äô     s edges is critical in order to capture the
GCN respectively.                                                                                                                                underlying structure of the data and explain the similarities between the
                                                                                                                                                 feature  vectors.  We  employ  a  graph  kernel  to  directly  measure  the  to-
3.2.                 F-GCN                                                                                                                       pological         similarity         between         functional         connectivity         networks.         The
                                                                                                                                                 graph kernel is one kind of kernel constructed on graphs that measures
      In   the   f-GCN,   multiple   GCN   layers   are   stacked.   Following   the   con-                                                      the topological similarity between graphs. More formally, given a pair of
volutional layers, a global average pooling operator produces coarsened                                                                          networks N            i and N      j, a graph kernel can be defined as S(N                                  i,N   j)=  ‚å©                 œÜ   (Ni),
graphs, which can naturally summarize the subgraph information while                                                                             œÜ   (N  j)‚å™       , which takes into account the topology of networks N                                             i and N       j.
utilizing    the    subgraph    structure.    Moreover,    the    pooling    layers    enable                                                          Assume we have a function SI for computing the similarity between
GCN to reduce the number of parameters by scaling down the size of the                                                                           the structure of two brain networks rather than the embeddings, and the
representations, and thus avoid overfitting. A readout layer collapses the                                                                       similarity score between a pair of brain networks N                                             i and N      j is denoted by
node representations of each graph into a graph representation.                                                                                  SI(N    i,N    j).  Our  hi-GCN  method  employs  a  graph  kernel  to  estimate  the
      When     multiple     GCN     layers     are     stacked,     information     about     larger                                             correlation          between          networks.          Kernel          methods          have          the          desirable
neighborhoods are integrated. However, it cannot summarize the node                                                                              property    that    they    do    not    rely    on    explicitly    characterizing    the    vector
information into the higher level graph representation. To address this                                                                          representation          œÜ   (x)of    data    points    in    the    feature    space    induced    by    a
challenge,      we      use      [33]      to      propose      eigenvector-based      pooling      layers                                       kernel   function,   but   access   data   only   via   the   Gram   matrix     K                          .   In   this
EigenPooling               to              hierarchically              summarize              node              information              and     setting, a  kernel k        :                 G       √óG                                            ‚Üí                                                   R                      is  called a  graph kernel,  which  can cap-
generate       a       graph       representation.       The       pooling       layers       consist       of       two                         ture     the     inherent     similarity     in     the     graph     structure     and     is     reasonably
components:     graph     coarsening     and     generating     a     coarsened     graph     by                                                 efficient  to  evaluate.  Distances  between  instances  with  the  q-th  kernel
treating subgraphs as supernodes. At first, we adopt spectral clustering                                                                         function       are      calculated      and       are      defined       as           K        (ri  ,ri),      where      ri      =
to group nodes into subnetworks. The number of clusters is indicated by                                                                          ‚àë                                                                                            q    a     b                      a
                                                                                                                                                      MA     i(a,u),  indicating  the  local  topology  of   nodes.  We  consider  each
H, which is further empirically investigated.                                                                                                         u
      Let            Ncoar denote     the     coarsened     graph,     which     consists     of     the     sub-                                subject  as  an  undirected  graph  and  employ  a  graph  kernel  to  measure
networks  and  their  connections,   C      ‚ààR                            M√óH (M  indicates the  number  of                                      the similarity. We assume that the relation structures of brain network s

H. Jiang et al.
                                                                                                Fig.  4.            The different  training  scheme.
belonging     to     the     same     class     are     relatively     more     similar,     while     those                     strategies to optimize the models of hi-GCN.
belonging to different classes are relatively more dissimilar. Like kernels
on            vector            spaces,            graph            kernels            can            be            calculated            implicitly            by 1.     Two-step training. Both f-GCN and p-GCN are supervised with their
computing    K                          .   If  the   RBF  kernel   function  is  chosen,  then  the   distance                        own       loss       function       independently.       The       optimization       procedure       is
                                                                                     (                )                                shown in Fig. 4(a).
                                                                  i   i                     ‚Äñria ri  ‚Äñ
between instances is calculated as: K       (ra,rb)=exp                                         2œÉ b     , where œÉ                2.     Jointly      training.      We      optimize      f-GCN      and      p-GCN      together      with      a
is a kernel parameter. To only consider the strong similarity, if the dis-                                                             single loss in hi-GCN as shown in Fig. 4(b).
tance          between          the          instances          rand          ris          smaller          than          T          (hyper-3.     Pre-training. We first optimize the node embedding features with the
                                                       a             b                                                                 loss function in f-GCN, then the overall hi-GCN is further trained on
parameter), K       (rai      ,ri )is set to 1, and 0 otherwise.
                                 b                                                                                                     the same data under the supervision with the loss in hi-GCN, which is
     To    capture    the   similarity    among    networks,    the    similarity   between                                            based on the f-GCN with trained weights as shown in Fig. 4(c).
networks N         i and N      j is calculated as:
              )=‚àë       M   ‚àë   M    w  iw  jK          xi,xj)                                                                   4.                Experiment
SI  N  i,N   j          a =1 ‚àëb =1      ‚àëa  b        a    b                                                         (7)
                                 ni w i     M   w  j
                                 M    a     b =1   b                                                                                   In  this  section,  we  present  several  sets  of  comparative  experiments
where wi        =‚àë           1          is associated with each brain region ri                         in N     with            both    on    ABIDE    and    ADNI    data    sets.    Next,    we    briefly    introduce    these
              a         M   K     (xia,xiu)                                                           a        i                 comparative      experiments.       The       experimental      settings       and       results       are
                        u=1
the q-th kernel function.                                                                                                        described in detail in the next subsections.
3.4.                 Training  scheme                                                                                            4.1.                 Databases  and  preprocessing
     For      overcoming      these      issues      of      optimization,      we      introduce      three                           We     apply     our     model     on     two     large     and     challenging     databases     for

H. Jiang et al.
Table 1                                                                                                                                                                                                                                                                                                                                                                                              4.2.                 Performance  on  hierarchical  GCN
The parameter settings of network training on ABIDE and ADNI database.
        parameter name                                                                                                                                                                                                                                                                                                                                                                                                    parameters In this experiment we evaluated the effectiveness of hi-GCN on both
        Optimizer                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Adaptive optimizer (Adam) the ADNI and ABIDE databases using a 10-fold stratified cross validation
        learning rate                                                                                                                                                                                                                                                                                                                                                                                                                                      0.001 strategy. The parameter setting of our model is shown in Table 1. Using
        dropout rate                                                                                                                                                                                                                                                                                                                                                                                                                                         0.5 the           above           setting,           we           carried           out           comprehensive           experiments           to
        batch size                                                                                                                                                                                                                                                                                                                                                                                                                                                                  900 demonstrate    the   performance   of    the   hi-GCN   model    for    graph   classifi-
        Training iterations                                                                                                                                                                                                                                                                                                                                                                              160 cation. The important hyperparameters in our proposed model are T  ‚àà
        l2 regularization                                                                                                                                                                                                                                                                                                                                                                                                 5.10 4  {0.3,0.45,0.6,0.47,0.9}, Œ≥    ‚àà{1,2,3,4,5}and H (varying from 0 to 1).
        L (Convolutional Layers for ABIDE &             ADNI)                                                                                                                         12 &             8                                                                                                                                                                                             To          select          optimal          parameters          of          our          approach          and          all          competing
        M(number of ROI)                                                                                                                                                                                                                                                                                                                                                                              116 methods,  we  further  perform  a  10-fold  nested  cross  validation  proced-
        P and ÃÇP  for ABIDE &             ADNI                                                                                                                                                                                                                                                                                       768 &             512
        Threshold T in graph kernel for ABIDE &             ADNI                                                                                         0.6 &             0.75                                                                                                                                                                                                                      ure.   In   each   trial,   a   nested   5-fold   CV   is   conducted   to   tune   the   hyper-
        Number of cluster H                                                                                                                                                                                                                                                                                                                                                             7            parameters      in      the      training      data.      To      test      whether      the      results      of      our
        Kernel parameter Œ≥                                                                                                                                                                                                                                                                                                                                                                                    3 method and those of each competing methodThe parameter settings of
        Z in graph pooling                                                                                                                                                                                                                                                                                                                                                                              1  networks are statistically different, we perform the Student‚Äô     s t-test (with
                                                                                                                                                                                                                                                                                                                                                                                                     the     significance     level     at     0.05)     on     the     metric     values     achieved     by     our
binary classification tasks. The ABIDE database (Autism Brain Imaging                                                                                                                                                                                                                                                                                                                                method and each competing method.
Data Exchange Database) investigates the neural basis of autism [34]. It                                                                                                                                                                                                                                                                                                                                              We evaluate our hi-GCN on the task of graph classification to answer
aggregates    data    from    different    17    acquisition    sites    and   openly    shares                                                                                                                                                                                                                                                                                                      the following three questions:
rs-fMRI and phenotypic data of 1112 subjects. We select the same set of
866 subjects used in Ref. [35], comprising 402 individuals with ASD and                                                                                                                                                                                                                                                                                                                                               Q1:       How       does       the       jointly       modeling       two       graphs       in       hi-GCN       help
464 healthy controls acquired at 20 different sites. We use the data from                                                                                                                                                                                                                                                                                                                                             improve   the   graph   representation   learning   ability   and   the   classifi-
the  Preprocessed  Connectome  Project  [36]  to  discriminate  individuals                                                                                                                                                                                                                                                                                                                                           cation accuracy?
with   Autism   Spectrum   Disorder   from   normal   controls.   The   ADNI   was                                                                                                                                                                                                                                                                                                                                    Q2:   Which   learning   strategy   is   effective   on   the   optimization   of   hi-
launched in 2003 by the National Institute on Aging (NIA), the National                                                                                                                                                                                                                                                                                                                                               GCN?
Institute  of  Biomedical  Imaging  and  Bioengineering  (NIBIB),  the   Food                                                                                                                                                                                                                                                                                                                                         Q3:     How     do     the     important     hyperparameters     in     hi-GCN    affect     the
and Drug Administration (FDA), private pharmaceutical companies and                                                                                                                                                                                                                                                                                                                                                   network performance?
non-profit   organizations,   as   a   $60   million,   5-year   public-private   part-
nership. We focus on using rs-fMRI to discriminate individuals with Mild                                                                                                                                                                                                                                                                                                                                              We          compare          our          hi-GCN          with          the          connectivity          features          based
Cognitive         Impairment         (MCI)         from         individuals         diagnosed         with         Alz-                                                                                                                                                                                                                                                                              method, Eigenpooling GCN [33] and Population GCN [27].
heimer‚Äô     s  Disease  (AD).  We  select  the  same  set  of  133  subjects  used  in                                                                                                                                                                                                                                                                                                                                Network       Based       Feature       is       a       linear       classification       using       a       ridge
Ref.  [35],  comprising  99  individuals  with  MCI  and  34  diagnosed  with                                                                                                                                                                                                                                                                                                                        classifier. In the connectivity networks, there exist a large number of low
Alzheimer‚Äô     s Disease (AD).                                                                                                                                                                                                                                                                                                                                                                       level   features   (i.e., M√ó(M 1),   where   M   is   the   total   number   of   ROIs)   are
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2
                                                                                                                                                                                                                                                                                                                                                                                                     extracted from the network as network features for subsequent feature
Table 2
Performance  comparison  of  various  methods.  Each  experiment  is  run  10  times  and  the  average  graph  classification  performance  (Accuracy,  AUC,  Sensitivity  and
Specificity) is reported. The best results are bold. The values marked by ‚àóindicate that our method achieves significantly different results compared with the competing
methods.
        ABIDE                                                                                                                                                                          Methods                                                                                                                                                                                                                                                                                                                                       ACC                                                                                                                                                                                                                                                                          AUC                                                                                                                                                                                                                                                                        Specificity                                                                                                                                                                                                                Sensitivity
                                                                                                              network based feature                                                                                                                                                                                                  0.559¬±0.001*                                                                                                                                                                     0.602¬±0.002*                                                                                                                                                                     0.578¬±0.001*                                                                                                                                                                     0.537¬±0.001*
                                                                                                              Eigenpooling GCN                                                                                                                                                                                                                                        0.586¬±0.001*                                                                                                                                                                     0.655¬±0.003*                                                                                                                                                                     0.596¬±0.001*                                                                                                                                                                     0.574¬±0.001*
                                                                                                              Population GCN                                                                                                                                                                                                                                                              0.635¬±0.002*                                                                                                                                                                     0.675¬±0.002*                                                                                                                                                                     0.651¬±0.002*                                                                                                                                                                     0.617¬±0.002*
                                                                                                              BrainNetCNN                                                                                                                                                                                                                                                                                       0.651¬±0.003*                                                                                                                                                                     0.728¬±0.002*                                                                                                                                                                     0.656¬±0.003*                                                                                                                                                                     0.624¬±0.005*
                                                                                                              CC                                                                                                                                                                                                                                                                                                                                                                                              0.607¬±0.002*                                                                                                                                                                     0.624¬±0.003*                                                                                                                                                                     0.587¬±0.001*                                                                                                                                                                     0.559¬±0.004*
                                                                                                              t-BNE                                                                                                                                                                                                                                                                                                                                                                 0.655¬±0.001*                                                                                                                                                                     0.708¬±0.001*                                                                                                                                                                     0.621¬±0.002*                                                                                                                                                                     0.608¬±0.002*
                                                                                                              Graph Boosting                                                                                                                                                                                                                                                                    0.646¬±0.002*                                                                                                                                                                     0.725¬±0.003*                                                                                                                                                                     0.639¬±0.003*                                                                                                                                                                     0.617¬±0.003*
                                                                                                              Ordinal Pattern                                                                                                                                                                                                                                                                    0.641¬±0.002*                                                                                                                                                                     0.729¬±0.002*                                                                                                                                                                     0.628¬±0.001*                                                                                                                                                                     0.611¬±0.002*
                                                                                                              Hi-GCN(two-step)                                                                                                                                                                                                                                             0.665¬±0.002*                                                                                                                                                                     0.721¬±0.003*                                                                                                                                                                     0.675¬±0.002*                                                                                                                                                                     0.653¬±0.003*
                                                                                                              Hi-GCN(pre-training)                                                                                                                                                                                                            0.671¬±0.003                                                                                                                                                                                0.743¬±0.003                                                                                                                                                                                0.681¬±0.003                                                                                                                                                                                0.659¬±0.003
                                                                                                              Hi-GCN(jointly learning)                                                                                                                                                                          0.672¬±0.003                                                                                                                                                                          0.745¬±0.003                                                                                                                                                                          0.684¬±0.003                                                                                                                                                                          0.659¬±0.003
        ADNI                                                                                                                                                                                    Methods                                                                                                                                                                                                                                                                                                                                       ACC                                                                                                                                                                                                                                                                          AUC                                                                                                                                                                                                                                                                        Specificity                                                                                                                                                                                                                Sensitivity
                                                                                                              network based feature                                                                                                                                                                                                  0.684¬±0.003*                                                                                                                                                                     0.691¬±0.002*                                                                                                                                                                     0.743¬±0.003*                                                                                                                                                                     0.512¬±0.003*
                                                                                                              Eigenpooling GCN                                                                                                                                                                                                                                        0.749¬±0.002*                                                                                                                                                                     0.711¬±0.004*                                                                                                                                                                     0.779¬±0.002*                                                                                                                                                                     0.662¬±0.004*
                                                                                                              Population GCN                                                                                                                                                                                                                                                              0.737¬±0.002*                                                                                                                                                                     0.726¬±0.005*                                                                                                                                                                     0.763¬±0.002*                                                                                                                                                                     0.661¬±0.003*
                                                                                                              BrainNetCNN                                                                                                                                                                                                                                                                                       0.728¬±0.002*                                                                                                                                                                     0.738¬±0.003*                                                                                                                                                                     0.767¬±0.002*                                                                                                                                                                     0.652¬±0.004*
                                                                                                              CC                                                                                                                                                                                                                                                                                                                                                                                              0.701¬±0.004*                                                                                                                                                                     0.715¬±0.004*                                                                                                                                                                     0.738¬±0.005*                                                                                                                                                                     0.526¬±0.005*
                                                                                                              t-BNE                                                                                                                                                                                                                                                                                                                                                                 0.716¬±0.006*                                                                                                                                                                     0.722¬±0.006*                                                                                                                                                                     0.755¬±0.003*                                                                                                                                                                     0.663¬±0.003*
                                                                                                              Graph Boosting                                                                                                                                                                                                                                                                    0.731¬±0.005                                                                                                                                                                                0.726¬±0.004*                                                                                                                                                                     0.766¬±0.004*                                                                                                                                                                     0.671¬±0.004*
                                                                                                              Ordinal Pattern                                                                                                                                                                                                                                                                    0.729¬±0.003*                                                                                                                                                                     0.737¬±0.006*                                                                                                                                                                     0.769¬±0.004*                                                                                                                                                                     0.675¬±0.004*
                                                                                                              hi-GCN(two-step)                                                                                                                                                                                                                                                0.754¬±0.003                                                                                                                                                                                0.756¬±0.005*                                                                                                                                                                     0.785¬±0.003                                                                                                                                                                          0.664¬±0.004*
                                                                                                              hi-GCN(pre-training)                                                                                                                                                                                                                0.756¬±0.002                                                                                                                                                                                0.778¬±0.003*                                                                                                                                                                     0.785¬±0.003                                                                                                                                                                          0.672¬±0.003*
                                                                                                              hi-GCN(jointly learning)                                                                                                                                                                              0.756¬±0.002                                                                                                                                                                          0.789¬±0.004                                                                                                                                                                          0.777¬±0.002                                                                                                                                                                                0.695¬±0.003

H. Jiang et al.
                                                                                         Fig. 5.            The comparative  boxplots on  the  ABDIE  dataset.
selection     and    classification.     Moreover,    Recursive     Feature     Elimination                                                        BrainNetCNN [37]is                                          composed of novel edge-to-edge, edge-to-node
(RFE)  is  used for  feature selection.  For  fair comparison, the  amount of                                                                and     node-to-graph     convolutional     filters     that     leverage     the     topological
features     selected     is     equal     to     the     dimension     of     the     embedding     vector                                  locality of structural brain networks.
generated by f-GCN.                                                                                                                                Besides, we also compare hi-GCN with four state-of-the-art methods,
      Eigenpooling       GCN       [33]                                                 is       an       end-to-end      trainable      GCN      with      a including  2  topology-based  representation  approaches  (i.e.,  Clustering
pooling operator EigenPooling.                                                                                                               Coefficient     (CC)     [38]                                               and     t-BNE     [39]),                                               and     2     subgraph-based     repre-
      Population     GCN      [27]                                                is     a     node     level      GCN     for     brain     analysis      in sentation    approaches    (i.e.,    Graph    Boosting    [40],                                              and    Ordinal    Pattern
populations. The significant difference from the work in Ref. [27]                                         are: 1)                           [41]).
node:                                                                the embedding features of the nodes in the population is learned              Clustering           Coefficient           (CC)           [38]                                                     extracts           local           clustering           co-
automatically     rather     than     extracted;     2)     edge:                                                                 the     similarity     between efficients  as  features of the  brain network,  by measuring  the degree to
nodes     is     calculated     considering     the     structure     of     the     brain     functional                                    which      nodes      in      a      network      tend      to      cluster      together      (a      measure      that
network when constructing the population network. The apparent lim-                                                                          quantifies    the   cliquishness    of   the    nodes).   Then,    Each   network    is    con-
itation of such model is that they can only learn on the vectorized node,                                                                    verted into a feature vector to train a classifier.
which     cannot     effectively     generalize     the     condition     that     the     node     is     a                                       Tensor-based   Brain   Network   Embedding   (t-BNE)   [39]                                             is   a   con-
graph describing the functional connectivity. The graph representation                                                                       strained tensor factorization model for brain network embedding. It can
techniques   have   recently  shifted   from   hand-crafted  kernel   methods   to                                                           handle            partially            symmetric            tensors,            incorporates            side            information
deep  learning  based  end-to-end  methods,  which  achieve  better  perfor-                                                                 guidance  and  orthogonal  constraint  to  obtain  informative  and  distinct
mance           in          graph-structured          learning           tasks.           Moreover,          the           featured          latent           factors,           and           the           obtained           vectorized           features           are           used           for
extracted    prior    to    the    classification    may    not    be    appropriate    for    GCN                                           classification.
classification due to lacking the capacity of jointly learning.                                                                                    Graph Boosting (GB) [40]                                         first mines subgraphs from each network

H. Jiang et al.
                                                                                   Fig.  6.            The  comparative  boxplots on  the  ADNI dataset.
as    features,    and    then    learns    subgraph-based    decision    stumps    as   weak                                      1.     For   ABIDE,   we   obtain   an   average   accuracy/AUC   of   66.5%/72.1%,
learners.   Finally,   a   Boosting   algorithm   in   which   subgraph-based   deci-                                                   67.1%/74.3%    and    67.2%/74.5%    in    hi-GCN    with    different    optimi-
sion stumps are used as weak learners is trained.                                                                                       zation             scheme             respectively,             outperforming             the             other             methods
     Ordinal         Pattern         (OP)         [41]                                                   first         mines         frequent         subgraphs         via including            network-based            feature            method(55.9%/60.2%),            Eigen-
ordinal    patterns    that    contains    a    sequence    of    weighted    edges    in    brain                                      pooling  GCN  (58.6%/65.5%)  and  Population  GCN  (63.5%/67.5%).
connectivity networks of patients and normal controls. This method can                                                                  Results     obtained    for     the     ADNI    database     also     show    an     increase     in
simultaneously            model            both            weight            information            (i.e.,            connectivity      performance with respect to the competing methods. It demonstrates
strength)     and    ordinal    relationship     of    weighted     edges     in    a    brain     con-                                 that  our  hi-GCN  is  effective  for  graph  classification  with  brain  dis-
nectivity network while relying on the subsequent classifiers.                                                                          orders diagnosis regardless of the optimization.
     Note that SVM with an RBF kernel is employed as the base classifier                                                           2.     The         embedding         learning         methods         with         deep         learning         generally
for   the  features  extracted   by  both  CC  and  OP.  Both  the  regularization                                                      obtain            better            prediction            results            than            the            traditional            extracted
and         the         kernel         parameters         of         SVM         are         tuned         by         10         nested         cross network   connectivity   features.   Moreover,   subgraph-based   methods
validation.                                                                                                                             (i.e.,       GB       and       OP)       generally       obtain       better       performance       than       the
     Experimental  results  are  reported  in  Table  2  where  the  best  results                                                      topology-based methods, but their overall performance is not as good
are boldfaced. Moreover, comparative boxplots across all folds between                                                                  as   hi-GCN‚Äôs.                                                                                                    It   implies   that   considering   the   correlation   among   the
the comparable approaches are shown in Fig. 5 and Fig. 6 for both da-                                                                   instances helps promote the learning performance.
tabases. As can be seen, our proposed method consistently showed the                                                               3.     Previous          studies          typically          utilize          human-engineered          features          to
best  performance  over  the  baseline  methods,  and  achieves  statistically                                                          represent brain connectivity networks, but these features may not be
significant    results    on    most    of    the    results.    These    results    reveal   several                                   well coordinated with subsequent classifiers. It is worth noting that
interesting points:                                                                                                                     both       topology-based       and       subgraph-based       representation       methods

H. Jiang et al.
                                                                                                                           Fig.  7.            Convergence behavior of  three optimization  methods  for hi-GCN on  ABDIE  dataset.
            usually     first     extract     particular     network     representation     from     brain                                                                                                                                                                                                       that   analyzing   the   population   graph   can   greatly   benefit   the   graph
            connectivity  networks,  and  then reply  on pre-defined  classifiers  for                                                                                                                                                                                                                           embedding for GCN.
            brain disease diagnosis. The feature extraction and classifier training                                                                                                                                                                                                                   5.     For the optimization in hi-GCN, the scheme of jointly training is more
            are      treated      as      two      separate      tasks      in      these      methods,      so      potential                                                                                                                                                                                   effective for the graph embedding learning. This experiment further
            inconsistency between features and classifiers may degrade the final                                                                                                                                                                                                                                 demonstrates that optimizing the graph embedding with considering
            performance of these methods. To address this issue, in our hi-GCN,                                                                                                                                                                                                                                  both the network structure and the neighborhood in the population
            the representation learning and classifier training are blended into a                                                                                                                                                                                                                               simultaneously resulted in a better solution of graph embedding and
            unified     optimization     problem,     and     the     supervision     information     is                                                                                                                                                                                                         a better prediction performance.
            introduced  to  the  representation  learning  process,  so  that  discrimi-
            native representations can be obtained.                                                                                                                                                                                                                                                              To         evaluate         the         quality         of         the         solution         provided         by         our         three
 4.     Compared with Eigenpooling GCN and Population GCN, our hi-GCN                                                                                                                                                                                                                               different     optimization     algorithms,     we     compare     them     with     respect     to
            performs               the               graph              embedding              learning              from               a              hierarchical                                                                                                                                 convergence behavior. According to Fig. 7, we can see both pre-training
            perspective     considering     the     structure     in     individual     brain     network                                                                                                                                                                                           and jointly training converge faster than the two-steps. The pre-training
            and the subject‚Äô     s correlation in the global population network, which                                                                                                                                                                                                              method  even  starts  convergence  faster,  but  gets   a  slower  convergence
            can   capture   the   most   essential   embedding   features   to   improve   the                                                                                                                                                                                                      speed              and              almost              equal              accuracy             compared              to              jointly             training.
            classification          performance          of          disease          diagnosis.          The          population                                                                                                                                                                   Furthermore, it is also observed that the jointly training finally achieves
            network     provides     a     powerful     means     for     representing     complex     in-                                                                                                                                                                                          better validation accuracy.
            teractions    between    subjects.   The    correlation   among   instances   pro-
            vides                       rich                       information                       that                       can                       be                       leveraged                       to                       more
            comprehensively characterize the graph embedding. It demonstrates
Table 3
Performance  comparison  of  various  methods  based  on  imaging  and  non-imaging  information.  The  best  results are  bold.  The  values  marked  by  *  indicate  that  our
method achieves significantly different results compared with the competing methods.
      ABIDE                                                                                                                                                                           Methods                                                                                                                                                                                                                                                                                                                                    ACC                                                                                                                                                                                                                                                                           AUC                                                                                                                                                                                                                                                                         Specificity                                                                                                                                                                                                                 Sensitivity
                                                                                   Population GCN                                                                                                                                                                                                                                                           0.664¬±0.003*                                                                                                                                                                      0.723¬±0.004*                                                                                                                                                                      0.675¬±0.003*                                                                                                                                                                      0.652¬±0.004*
                                                                                   hi-GCN(two-step)                                                                                                                                                                                                                                             0.719¬±0.002*                                                                                                                                                                      0.797¬±0.003*                                                                                                                                                                      0.732¬±0.002*                                                                                                                                                                      0.704¬±0.003*
                                                                                   hi-GCN(pretraining)                                                                                                                                                                                                                    0.728¬±0.003                                                                                                                                                                                 0.817¬±0.004*                                                                                                                                                                      0.744¬±0.003                                                                                                                                                                                 0.710¬±0.004*
                                                                                   hi-GCN(jointly learning)                                                                                                                                                                           0.731¬±0.003                                                                                                                                                                           0.823¬±0.004                                                                                                                                                                           0.746¬±0.003                                                                                                                                                                           0.714¬±0.003
      ADNI                                                                                                                                                                                     Methods                                                                                                                                                                                                                                                                                                                                    ACC                                                                                                                                                                                                                                                                           AUC                                                                                                                                                                                                                                                                         Specificity                                                                                                                                                                                                                 Sensitivity
                                                                                   Population GCN                                                                                                                                                                                                                                                           0.756¬±0.001*                                                                                                                                                                      0.813¬±0.002*                                                                                                                                                                      0.781¬±0.001*                                                                                                                                                                      0.683¬±0.002*
                                                                                   hi-GCN(two-step)                                                                                                                                                                                                                                             0.779¬±0.002*                                                                                                                                                                      0.832¬±0.003*                                                                                                                                                                      0.798¬±0.002*                                                                                                                                                                      0.724¬±0.002*
                                                                                   hi-GCN(pretraining)                                                                                                                                                                                                                    0.782¬±0.002*                                                                                                                                                                      0.843¬±0.003*                                                                                                                                                                      0.803¬±0.002                                                                                                                                                                                 0.720¬±0.003*
                                                                                   hi-GCN(jointly learning)                                                                                                                                                                           0.785¬±0.002                                                                                                                                                                           0.865¬±0.004                                                                                                                                                                           0.805¬±0.002                                                                                                                                                                           0.726¬±0.003

H. Jiang et al.
Table 4                                                                                                                                                                                                   Pdist. All of them consider the multi-modality data during the similarity
Performance   comparison   of   various   network   similarity   estimation   in   the   con-                                                                                                             estimation according to Eq. (9). Metric learning is a method for learning
struction of population network. The best results are bold. The values marked by                                                                                                                          a similarity metric between irregular graphs. In our work, we follow the
* indicate that our method achieves significantly different results compared with                                                                                                                         work in Ref. [29] to choose siamese graph convolutional neural network
the competing methods.                                                                                                                                                                                    (s-GCN) to learn a graph similarity metric in a supervised setting. Pdist
    ABIDE                                                                                                      Methods                                                                                                                                                                             ACC                                                                                                                                                                                                      AUC  computes the correlation distance between graph embedding vectors  u
                                          metric Learning                                                                                                      0.721¬±0.004*                                                                                                 0.819¬±0.002   and v              learned by the f-GCN. That is
                                          Pdist                                                                                                                                                                                                                0.719¬±0.004*                                                                                                 0.811¬±0.003*   (u      u   )‚ãÖ (v     v  )
                                          graph kernel                                                                                                                                     0.731¬±0.003                                                                                                      0.823¬±0.004  Kpdist(u     ,v  )=‚Äñ(v     v  )‚Äñ‚Äñ(v     v  )‚Äñ,                                                                                                                                                                                                                                                                                                                                                                                                                           (10)
    ADNI                                                                                                                Methods                                                                                                                                                                             ACC                                                                                                                                                                                                      AUC  22
                                          metric Learning                                                                                                      0.772¬±0.003*                                                                                                 0.861¬±0.010   where u                 and v              is the mean of the elements in vector.
                                          Pdist                                                                                                                                                                                                                0.768¬±0.004*                                                                                                 0.847¬±0.011*   At first glance at the results in Table 4, we can see that graph kernel
                                          graph kernel                                                                                                                                     0.785¬±0.008                                                                                                      0.865¬±0.009   consistently    outperforms    all    other    compared    methods,    which    demon-
                                                                                                                                                                                                          strates the good behavior of the graph similarity considering the struc-
4.3.                 Performance  on  different  construction in  population  network                                                                                                                     ture without embedding features compared with other related methods
                                                                                                                                                                                                          found  in  the  literature.  Most  of  the  existing  works  focus  on  preserving
1)     The effect of similarity estimation scheme with auxiliary information                                                                                                                              network structures and properties in embedding vectors. However, some
                                                                                                                                                                                                          useful   structural    information   may   inevitably   be    lost.   Direct   similarity
        Clinical        and        research        studies        commonly        acquire        complementary                                                                                            estimation     with     graph     embedding     is     not     appropriate.     Different     from
brain images for a more accurate and rigorous assessment of the disease                                                                                                                                   graph kernel and Pdist, metric learning is trained to automatically pre-
status    and    likelihood    of    progression.    To    estimate    the    effectiveness    of                                                                                                         dict the measure of similarity between the brain networks. In the metric
combining multi-modality, we follow the same work as in Ref. [27] by                                                                                                                                      learning, the label of matching (same class) or non-matching (different
introducing   the   non-imaging   complementary    data   (gender   or    acquisi-                                                                                                                        classes)           is           used           to           supervise           Siamese           graph           convolutional           neural
tion site) in the estimation of subject‚Äô     s similarity.                                                                                                                                                network. However, it is difficult for Siamese graph convolutional neural
        For    the    ABIDE    population    graph,    we    choose    subject‚Äô     s    gender    and                                                                                                    network to learn the exact similarity for complex network structure with
acquisition site as the non-imaging modality; For the ADNI population                                                                                                                                     insufficient training data, which demonstrates that it may not be able to
graph, we choose age and gender of subjects. Let M be the non-imaging                                                                                                                                     sufficiently   capture   underlying   patterns   by   learning   from   network   in-
information  of  each subject.  The similarity of  non-imaging  modality  is                                                                                                                              stances. The result indicates the similarity estimation of brain network is
calculated as:                                                                                                                                                                                            critical for the population network construction.
                                   {       1                                                                                       if    ‚Éí‚Éí
S       M       ,M       )=                          ‚ÉíM     i   M         j‚Éí<        T                                                                                                                    4.4.                 The  influence  of  the  hyperparameters  of  Hi-GCN
   NI          i        j               0                                                                                                                                                                                 otherwise                                                     ,                                                                                                                                                                                                                                                                                                                                                                                                        (8)
                                                                                                                                                                                                                   In the hi-GCN model, three important parameters are the number of
where T  is a threshold value, and T  = 2 in our experiment.                                                                                                                                              clusters in the f-GCN (H), the threshold of graph kernel in constructing
        Then,             we             integrate             both             imaging             and             non-imaging             similarity                                                    the population network (T) and the kernel parameter in the graph kernel
together  to  be the  integrated  similarity score,  which  can be  defined  as                                                                                                                           (Œ≥        ).  In  order  to  evaluate  the  impact  of  these  parameters  on  the  perfor-
follows:                                                                                                                                                                                                  mance of hi-GCN, we conduct two experiments with various values for
S    =Œ±     S      +(1     Œ±            )S        ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (9)  H, T and Œ≥        . Table 5 depicts the changes of accuracy as we vary the value
               I                             NI                                                                                                                                                           of H, we can observe that the best classification performance is achieved
where Œ±                is a parameter to adjust the contribution of the two similarity                                                                                                                    when      H   =       7.      It       demonstrates      that      the      hi-GCN       with      an      appropriate
scores S                and S                towards the integrated similarity score S. In our study,                                                                                                     cluster   number   can   achieve   optimal   classification   performance.   More-
                    I                 NI                                                                                                                                                                  over,  the  optimal  threshold  T  and    Œ≥               of  the  graph  kernel  are  unknown,
Œ±     = 0.5.                                                                                                                                                                                              and  they  play  a  vital  role  for  the  performance  of  graph  similarity.  We
        From the results in Table 3, it is clear that the method with auxiliary                                                                                                                           varied the value of the threshold T  ‚àà{0.3,0.45,0.6,0.75,0.9}as well as
information  outperforms  the  methods  using  one  single  fMRI  modality.                                                                                                                               Œ≥    ‚àà{1,2,3,4,5},     and     investigated     the     variation     of     performance     with
This        validates       our        assumption        that       the        complementary       information                                                                                            multiple      values.     When     the     threshold     value     T     increases,     the     network
among    different    modalities    is    helpful    for    constructing    the    population
network.      For      the      experiments      shown      in      Table      3,      the      hi-GCN(jointly                                                                                            becomes sparser for representing the most reliable correlation. Another
learning)   model   gave   the   best   results,   and   we   used   this   model   as   our                                                                                                              important conclusion is that the effect of threshold T is more significant
main learning model in the following experiments.                                                                                                                                                         than   Œ≥        .  From  Table  6  and  Table  7,  it  can  be  seen  that  the  best  classifi-
                                                                                                                                                                                                          cation performance is achieved when T  = 0.6 for ASD and T  = 0.75 for
2)     The effect of different similarity estimation scheme.                                                                                                                                              AD. It implies that many edges are not helpful and reducing these edges
                                                                                                                                                                                                          does not influence the performance. The results further demonstrate that
        The definition of the graph‚Äô     s edges is critical in order to capture the                                                                                                                      topological      similarity      between      functional      connectivity      network      with
underlying structure of the data and explain the similarities between the                                                                                                                                 appropriate      sparsity      and      kernel      parameter      is      important      for      network
feature vectors. The influence of edge weight on the prediction perfor-                                                                                                                                   embedding learning.
mance       was       also       investigated.       In       this       section,       we       compare       different
similarity  estimation  methods  of  brain  networks in  the  construction  of
the    population    network,    involving    graph    kernel,    metric    learning    and
Table 5
The varying performance with the respect to cluster number H during the Eigenpooling. The best results are bold.
    H                                                                                                                                                                                                                                           1                                                                                                                                            2                                                                                                                                            3                                                                                                                                            4                                                                                                                                            5                                                                                                                                            6                                                                                                                                            7                                                                                                                                               8                                                                                                                                            9                                                                                                                                            10
    ABIDE database                                                                                                 0.602                                                                                                 0.658                                                                                                 0.699                                                                                                 0.722                                                                                                 0.730                                                                                                 0.729                                                                                                 0.731                                                                                                 0.731                                                                                                 0.708                                                                                                 0.711
    ADNI database                                                                                                           0.557                                                                                                 0.657                                                                                                 0.663                                                                                                 0.687                                                                                                 0.729                                                                                                 0.783                                                                                                 0.785                                                                                                 0.782                                                                                                 0.784                                                                                                 0.742

H. Jiang et al.
Table 6
The varying performance with respect to T and Œ≥            during the estimation of node similarity with graph kernel. The best results are bold.
        T/Œ≥                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Œ≥    = 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Œ≥    = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Œ≥    = 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Œ≥    = 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Œ≥    = 5
        T = 0.3                                                                                                                                                                                                                0.632¬±0.002                                                                                                                                                                                              0.657¬±0.002                                                                                                                                                                                               0.651¬±0.003                                                                                                                                                                                                    0.642¬±0.002                                                                                                                                                                                              0.629¬±0.001
        T = 0.45                                                                                                                                                                                                    0.689¬±0.002                                                                                                                                                                                              0.696¬±0.003                                                                                                                                                                                               0.698¬±0.002                                                                                                                                                                                                    0.687¬±0.003                                                                                                                                                                                              0.679¬±0.002
        T = 0.6                                                                                                                                                                                                                0.721¬±0.002                                                                                                                                                                                              0.725¬±0.003                                                                                                                                                                                               0.731¬±0.003                                                                                                                                                                                              0.719¬±0.004                                                                                                                                                                                              0.716¬±0.003
        T = 0.75                                                                                                                                                                                                    0.671¬±0.002                                                                                                                                                                                              0.698¬±0.003                                                                                                                                                                                               0.703¬±0.003                                                                                                                                                                                                    0.711¬±0.002                                                                                                                                                                                              0.702¬±0.002
        T = 0.9                                                                                                                                                                                                                0.663¬±0.002                                                                                                                                                                                              0.678¬±0.003                                                                                                                                                                                               0.685¬±0.004                                                                                                                                                                                                    0.698¬±0.002                                                                                                                                                                                              0.671¬±0.002
Table 7
The varying performance with respect to T and Œ≥            during the estimation of node similarity with graph kernel on ADNI database. The best results are bold.
        T/Œ≥                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Œ≥    = 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Œ≥    = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Œ≥    = 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Œ≥    = 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Œ≥    = 5
        T = 0.3                                                                                                                                                                                                                0.632¬±0.004                                                                                                                                                                                              0.621¬±0.004                                                                                                                                                                                               0.602¬±0.005                                                                                                                                                                                                    0.594¬±0.004                                                                                                                                                                                              0.572¬±0.003
        T = 0.45                                                                                                                                                                                                    0.691¬±0.004                                                                                                                                                                                              0.709¬±0.006                                                                                                                                                                                               0.693¬±0.006                                                                                                                                                                                                    0.668¬±0.006                                                                                                                                                                                              0.652¬±0.005
        T = 0.6                                                                                                                                                                                                                0.749¬±0.006                                                                                                                                                                                              0.762¬±0.007                                                                                                                                                                                               0.763¬±0.008                                                                                                                                                                                                    0.754¬±0.006                                                                                                                                                                                              0.752¬±0.006
        T = 0.75                                                                                                                                                                                                    0.743¬±0.006                                                                                                                                                                                              0.756¬±0.007                                                                                                                                                                                               0.785¬±0.008                                                                                                                                                                                              0.781¬±0.007                                                                                                                                                                                              0.759¬±0.006
        T = 0.9                                                                                                                                                                                                                0.719¬±0.004                                                                                                                                                                                              0.743¬±0.005                                                                                                                                                                                               0.752¬±0.006                                                                                                                                                                                                    0.763¬±0.006                                                                                                                                                                                              0.726¬±0.004
Table 8
The comparison among different classifiers with previous methods for ASD vs. NC on ABIDE dataset.
        Method                                                                                                                                       Feature                                                                                                                                                                                                            Classifier                                                                                                                                                                                                                                                                                                                                                                                 Data                                                                                                                                                        CV                                                                                                                                                                                                                                                                                                                                                                                                                         ACC
        Sarah2018 [27]                                                        Brain connectivity                                                                                                                           GCN                                                                                                                                                                                                                                                                                                                                                                                                                            403 ASD vs. 468 10-CV                                                                                                                                                                                                                                                                                                                                                                                          69.5%
                                                                                                   feature                                                                                                                                                                                                                                                                                                                     NC
        Abraham2017                                                                                Tangent space                                                                                                           l2-regularized classifiers (SVC-l2 and ridge                                                                                                                                                        403 ASD vs. 468                                                                       10-CV                                                                                                                                                                                                                                                                                                                                                                                          67%
                 [42]                                                                              embedding                                                                                                               classifier).                                                                                                                                                                                        NC
        Dadi2019 [35]                                                                 Network connectivity                                                                                                                 l2-regularized logistic regression                                                                                                                                               402 ASD vs. 464                                                                                          random 100-CV (75% for training, and                                                                                                                                                        69.7%
                                                                                                   feature                                                                                                                                                                                                                                                                                                                     NC                                                                                    25% for test)
        Eslami 2019 [43]                                        No feature extraction                                                                      ASD-DiagNet (with augmentation)                                                                                                                             505 ASD vs. 530                                                                                                                                                                               10-CV                                                                                                                                                                                                                                                                                                                                                                                          69.2%
                                                                                                                                                                                                                                                                                                                                                                                                                               NC.                                                                                                                                                                                                                                                                               (70.1%)
        Heinsfeld 2018                                                                             No feature extraction                                                                      Two stacked denoising autoencoders                                                                                                       505 ASD vs. 530                                                                                                                                                               10-CV                                                                                                                                                                                                                                                                                                                                                                                          70%
                 [5]                                                                                                                                                                                                                                                                                                                                                                                                           NC
        Our method                                                                                            No feature extraction                                                                      hi-GCN(jointly learning)                                                                                                                                                                                                                              402 ASD vs. 466                                       10-CV                                                                                                                                                                                                                                                                                                                                                                                          73.1%
                                                                                                                                                                                                                                                                                                                                                                                                                               NC
4.5.                 Comparisons  with  prior  works                                                                                                                                                                                                                                                                                                                                                 Table 9
                 Table  8  compares  the  results  of  our  ASD  vs.  NC  classification  with                                                                                                                                                                                                                                                                                                       The   comparison   among  different   classifiers   with   previous  methods  for   AD   vs.
prior works in terms of accuracy as reported in the respective references.                                                                                                                                                                                                                                                                                                                           MCI on ADNI dataset.
In      general,      two     types      of      the     methods      are      usually      developed      for     the                                                                                                                                                                                                                                                                                        Method                                                               Feature                                                                                                   Classifier                                                                              Data                                            CV                                                                                                                                   ACC
diagnosis of the disorder diseases: (1) the traditional machine learning                                                                                                                                                                                                                                                                                                                                      Dadi2019                                                    Network                                                                     l2-                                                                     40                                       random 100-                                                          72.2%
procedure         (feature         extraction         and         classification)         and         (2)         the         deep                                                                                                                                                                                                                                                                                    [35]                                                connectivity                                                                regularized                                                             AD                                       CV (75% for
learning  procedure  (an  end-to-end  procedure).  For  the  traditional  ma-                                                                                                                                                                                                                                                                                                                                                                                             feature                                                                     logistic                                                                vs. 96                                   training, and
chine        learning        procedure,       a        straightforward        solution       that       has       been                                                                                                                                                                                                                                                                                                                                                                                                                                regression                                                              MCI                                      25% for test)
                                                                                                                                                                                                                                                                                                                                                                                                              Our                                                         No feature                                                                  hi-GCN                                                                  34                                       10-CV                                                                                                   78.5%
extensively explored is to first derive features from brain networks. Dadi                                                                                                                                                                                                                                                                                                                                            method                                              extraction                                                                  (jointly                                                                AD
[35]      conducts      a      sufficient      comparison      (8      different      ways      of      defining                                                                                                                                                                                                                                                                                                                                                                                                                                      learning)                                                               vs. 99
regions-either          pre-defined          or          generated          from          the          rest-fMRI          data-3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              MCI
measures to build functional connectomes from the extracted time-series
and  10  classification  models  to  compare  functional  interactions  across                                                                                                                                                                                                                                                                                                                       interaction   and   similarity   between   subjects,   which   can   reduce   perfor-
subjects).    Through    the    comparison,    the    optimal    choices    in    functional                                                                                                                                                                                                                                                                                                         mance. Compared with the traditional machine learning procedure, the
connectivity   prediction   pipeline   is   brain   regions   defined   with   regions                                                                                                                                                                                                                                                                                                               expressive    power    of    deep    learning    to    extract    the    underlying    complex
using                     DictLearn,                     connectivity                     matrices                     parametrized                     by                     their                                                                                                                                                                                                                 patterns from data has been well recognized. The power of deep learning
tangent-space   representation,  and   an  l2-regularized  logistic   regression                                                                                                                                                                                                                                                                                                                     lies    in    automatically    learning    relevant    and    powerful    features    for    any
as a classier. The best accuracy is 72.5% (median) and 84.5% (the 95th                                                                                                                                                                                                                                                                                                                               perdition task, which is made possible through end-to-end architectures.
percentile)   over   cross-validation   folds   (n =    100)   on   the   ADNI   dataset                                                                                                                                                                                                                                                                                                             Heinsfeld et al. [5] and Eslami et al. [43] have proposed a joint learning
with the same samples. On the ABIDE dataset, the best accuracy is 71.1%                                                                                                                                                                                                                                                                                                                              procedure  using  an  autoencoder  and  a  single  layer  or  multi-layer  per-
(median) and 75.6% (the 95th percentile). Abraham also employed the                                                                                                                                                                                                                                                                                                                                  ceptron     which     results     in     improved     quality     of     extracted     features     and
same strategy with the best pipeline and obtained a similar performance                                                                                                                                                                                                                                                                                                                              optimized parameters for the model. From Table 9, we can observe that
(accuracy is 67%) [42]. The most important limitation of the traditional                                                                                                                                                                                                                                                                                                                             our hi-GCN usually achieves competitive performance against both deep
machine          learning          procedure          is          that          feature          extraction          and          model                                                                                                                                                                                                                                                              learning methods proposed in Ref. [5,43]. The underlying reason could
learning are treated as two separate tasks in these methods, so potential                                                                                                                                                                                                                                                                                                                            be that 1) the brain network has complicated structures that contain rich
inconsistency   between   human-engineered   features   and   classifiers   may                                                                                                                                                                                                                                                                                                                      underlying    information,    although    an    autoencoder    network    is    used    to
degrade       the       final       performance       of       these       methods.      Moreover,       relying                                                                                                                                                                                                                                                                                     learn a network embedding and classification in an unified fashion, they
solely      on      subject-specific      imaging      feature      vectors      fails      to      model      the                                                                                                                                                                                                                                                                                   are applied on the extracted connectivity feature rather than the original

H. Jiang et al.
                                                                                                                     proposed  in Ref.  [27], indicating  simultaneously taking  both the  brain
                                                                                                                     region        and       subject       correlations       into       account       is        important.       We       also
                                                                                                                     compare    our    method    with    several    recent    state-of    the-art    methods    re-
                                                                                                                     ported in the literature using rs-fMRI data for AD vs. MCI classification.
                                                                                                                     Few works for discriminating AD and MCI with functional brain network
                                                                                                                     exist      in      the      literature.      The      only      comparable      method      is      proposed      in
                                                                                                                     Ref. [35]. Experimental results on the ADNI database demonstrate that
                                                                                                                     our method substantially outperforms the existing classification method
                                                                                                                     for AD vs. MCI. Results obtained for the ADNI database show an increase
                                                                                                                     in      performance      with      respect      to      the      competing      methods      in      Table      9.
                                                                                                                     Overall,            the            results            demonstrate            that            hi-GCN            can            improve            upon
                                                                                                                     state-of-the-art algorithms not just in traditional classification methods,
                                                                                                                     but also in deep learning methods.
                                                                                                                     4.6.                 Ablation  study  and  discussion
                                                                                                                           To our knowledge, this is the first attempt to incorporate the region-
                                                                                                                     to-region brain activity correlations and the subject interactions among
                                                                                                                     population     into     a     unified     model     for     functional     connectivity     network
                                                                                                                     analysis.       In       our       work,       the       population       network       provides       a       powerful
                                                                                                                     means for representing complex interactions between subjects. The key
                                                                                                                     to   our   hi-GCN   model   is   the   subject‚Äô     s   initial   embedding   and   similarity
                                                                                                                     among the subjects in the population network. In order to investigate the
                                                                                                                     role of them in our model intuitively, we made sufficient ablation study
                                                                                                                     by     varying     the     threshold     of     the     population     network,     exchanging     the
                                                                                                                     strategies  of  subjects‚Äô             initial  embedding  and  the  similarity  estimation,
                                                                                                                     fusing   the   network   embedding   with   the   graph  properties   as   node   fea-
                                                                                                                     tures,   employing   the   traditional   classification   method,   and   employing
                                                                                                                     other graph neural networks for the population network.
                                                                                                                      1)     Varying the threshold of the population network
                                                                                                                           Network   similarity   is   a   quantitative   measurement   of   topology   and
                                                                                                                     attribute     characteristics     between     subjects    in     the    population     network.
                                                                                                                     Different    thresholds    determine    their    corresponding    different    levels    of
                                                                                                                     topological     structure     in     the     population     network.     In     other     words,     the
                                                                                                                     thresholded connectivity networks with larger threshold often preserve
                                                                                                                     fewer  connections  and  thus  are  sparser  in  connections.  Therefore,  it  is
                                                                                                                     important to identify the optimal trade-off between the information gain
                                                                                                                     by the removal of the noisy correlations and the loss due to removal of
                                                                                                                     potentially   useful   weak   correlations.   Both   the   population   networks   of
                                                                                                                     AD and ASD with the optimal T value are shown in Fig. 8.
          Fig. 8.            The illustration  of population  network  for two  datasets.                                  We can see from Fig. 9 that when the T value increases, the perfor-
                                                                                                                     mance of hi-GCN first goes up, and when T goes beyond 0.6 for ABIDE
network data; 2) they do not consider the contextual information in the                                              and 0.7 or 0.8 for ADNI, the performance (ACC and AUC scores) starts to
population. Although this comparison is not done on the same propor-                                                 decline. It demonstrates that more neighborhood information help learn
tion   of   the   database,   our   hi-GCN   exceeds   previously   published   ABIDE                                better    node     embedding.    Nevertheless,    too     much    neighborhood     inevi-
findings. The best accuracy of previous works is 0.701, which is obtained                                            tably  leads  to  over-smoothing. If  T  is  too large,  nodes  (subjects) of  the
by   an   autoencoder   with   data   augmentation   in   Ref.   [5].   Moreover,   we                               population   network   cannot   get   sufficient   information    from   correlated
find that the deep learning model with the end-to-end manner performs                                                nodes      (subjects).      When      T   =       1,      it      is      equivalent      to      the      case      without
as well or worse than the traditional learning methods with connectivity                                             considering the correlation among the subjects.
feature      extraction     combined     with     ridge     or     SVM.     Few     brain     network                 2)     Exchanging   the   strategies   of   subjects‚Äô             initial   feature   and   similarity
classification     studies     have      explored     the     correlation     among     the     brain                      estimation
regions and subjects, which are critical in neuroimaging research. To the
best of our knowledge, the work of Parisot et al. [27] is currently rele-                                                  To   better   evaluate  and   understand   the   effectiveness   of   the   learned
vant  with  ours  for  ASD  diagnosis  on  the  whole  ABIDE  dataset.  Parisot                                      embedding by  f-GCN and the  similarity of the  network structure of hi-
et  al.  define  a  subject‚Äô     s  feature  vector  as  its  vectorized  functional  con-                           GCN,  we  conduct  the  comparison  by  exchanging  the  strategies  of  sub-
nectivity         matrix,         and         employ         a         ridge         classifier         to         select         the         most jects‚Äô             initial   embedding   and   the   similarity   estimation   in   this   group   of
discriminative     features     from     the     training     set.     With     the     selected     con-            experiments, including (1) embedding (subjects‚Äô           initial feature)+graph
nectivity     information     as     the     subject‚Äô     s     feature,     a     population     is     repre-      kernel      (similarity),      which      is      the      chosen      strategy      in      our      hi-GCN;      (2)
sented   as   a   graph  where   its   vertices   are   associated  with   the   extracted                           embedding +learned embedding by f-GCN (similarity); (3) graph kernel
image-based     feature     vectors     and     the     edges     encode     phenotypic     infor-                   (subjects‚Äô                            initial                  feature)+embedding(similarity),                  in                  which                  each
mation. However, the GCN model is trained on the traditional network                                                 network   is   represented   by   a   feature   vector   corresponding   to   the   simi-
features rather than on the original network. Experiments demonstrate                                                larity    between    this    network     with    the    other     ones    with    the    WL    Graph
that  the  proposed  hi-GCN  model  performs  better  than  the  GCN  model                                          Kernel (WLGK) [44]. The experimental results are reported in Table 10.

H. Jiang et al.
                                                                                                          Fig. 9.            Performance of  different thresholds  (T) for  the  population network.
Table 10                                                                                                                                                                                                      Table 12
Performance    comparison    of    various    strategies    of    subjects‚Äô             initial    feature    and                                                                                             Performance comparison of embedding obtained by our hi-GCN and f-GCN for
similarity estimation. The best results are bold.                                                                                                                                                             various classifiers. The best results are bold.
    ABIDE                                                                                                                                                                                                                                                                                                                                                                       ADNI ABIDE                                 Methods                                                                                                                                                                                                              SVM                                              Random Logistic
    Methods                                                                                                                                                  ACC                                                 AUC                                               Methods                                                                                                                                                  ACC                                                 AUC forest regression
    embedding + graph                                    0.731                                 0.823                                 embedding + graph   0.785                                 0.865                                     embedding obtained by f-                                           0.772                                    0.796                                                                                                         0.757
         kernel                                                                                     kernel                                                                                                                               GCN
    graph kernel +                                       0.719                                    0.817                                    graph kernel +0.746                                    0.844                                  embedding obtained by hi-                                          0.808                                 0.814                                                                                                      0.789
         embedding                                                                                  embedding                                                                                                                            GCN
    embedding +                                          0.710                                    0.809                                    embedding +   0.738                                    0.829            ADNI                                           Methods                                                                                                                                                                                                              SVM                                              Random Logistic
         embedding                                                                                  embedding                                                                                                                                                                                                                    forest                               regression
                                                                                                                                                                                                                                         embedding obtained by f-                                           0.818                                    0.811                                                                                                         0.805
                                                                                                                                                                                                                                         GCN
                                                                                                                                                                                                                                         embedding obtained by hi-                                          0.845                                 0.846                                                                                                      0.828
Table 11                                                                                                                                                                                                                                 GCN
Performance       comparison        of       fusing       the       embedding       with       the        various       graph
properties as node features. The best results are bold.                                                                                                                                                       compared  with  the  only  embedding,  which  demonstrates  that external
    ABIDE                                                                                                                                                                                                                                                                                                                                                                       ADNI information except CC is complementary to the learned embedding and
    Methods                                                                                                                                                  ACC                                                 AUC                                               Methods                                                                                                                                                  ACC                                                 AUC can  help  improve  the  representation  of  the  brain  network.  Integrating
    Only embedding                                                                    0.731                                    0.823                                    Only embedding                                                                     0.785                                    0.865 knowledge regarding the graph property could facilitate the learning of
    embedding + CC                                                                0.728                                    0.823                                    embedding + CC                                                                0.776                                    0.861 the network embedding.
    embedding + t-BNE                                    0.740                                    0.832                                    embedding + t-BNE                                    0.789                                 0.872
    embedding +                                          0.756                                 0.857                                 embedding +         0.784                                    0.871        4)     Evaluating the embedding with the various traditional classifiers
         Ordinal Pattern                                                                            Ordinal Pattern
                                                                                                                                                                                                                       In  order  to  investigate  the  effectiveness  of  the  learned  feature  rep-
As anticipated, our strategy adopted in hi-GCN provides the best results.                                                                                                                                     resentations, we compare the embedding obtained by our hi-GCN and f-
                                                                                                                                                                                                              GCN    using    various    traditional    classification    models.    For    the    regulari-
3)     Fusing the embedding with the graph properties as node features                                                                                                                                        zation and the kernel parameters of SVM, the regularization parameter
                                                                                                                                                                                                              of    logistic    regression,    and    the    number     of    trees    of    Random    forest    are
         In        order        to        test        whether        the        performance        can        be        improved        by                                                                    tuned       by       a       10-fold       nested       cross       validation.       The       result       is       shown       in
combining with the graph properties as node features or not, the features                                                                                                                                     Table 12. Overall, the results suggest that embeddings obtained by our
describing the graph properties are extracted and incorporated into the                                                                                                                                       hi-GCN can enable more accurate capacity as compared to the ones by f-
node    features    in    the    population    network.    Besides    the    embedding    ob-                                                                                                                 GCN for different classification models. It confirms our initial hypothesis
tained  by  f-GCN,  we  try  to  incorporate  the  topology-based  representa-                                                                                                                                that      the      joint      learning      from      two      levels      enables      us      to      learn      a      latent
tion        or        subgraph-based       representation        into        the        learned       embedding.                                                                                              embedding such that both the structural and relational properties of the
From    Table    11,    we    can    see    that    the    proposed    ‚Äòembedding  +     Ordinal                                                                                                              network can be encoded and preserved.
Pattern‚Äô            and  ‚Äòembedding +  t-BNE‚Äô            generally  improve  the  performance

H. Jiang et al.
Table 13                                                                                                                                       Acknowledgment
Performance      comparison     of      various     methods     with      different     version      of     GCN
models. The best results are bold.                                                                                                                    This research was supported by the National Natural Science Foun-
   ABIDE                                                                                                                                                                                                                                                                                                                                                                           ADNI dation   of   China   (No.62076059)   and   the   Fundamental   Research   Funds
   Methods                                                                                                                                      ACC                                                         AUC                                                       Methods                                                                                                                                      ACC                                                         AUC for the Central Universities (No. N2016001).
   GCN                                                                                                                                                                            0.731                                            0.823                                            GCN                                                                                                                                                                            0.785                                            0.865 References
   GAT                                                                                                                                                                             0.735                                            0.837                                            GAT                                                                                                                                                                             0.792                                            0.878
   Higher-order GCN                                         0.742                                         0.841                                         Higher-order GCN                                         0.796                                         0.882  [1]             J. Cummings,  G. Lee, A. Ritter, M. Sabbagh, K. Zhong, Alzheimer‚Äô    s disease drug
                                                                                                                                                        development pipeline: 2019, Alzheimer‚Äô    s  Dementia: Translational Research  &
5)     Evaluating   the   hierarchical   embedding   learning   with   various   GCN                                                                    Clinical Interventions 5 (2019) 272‚Äì        293.
                                                                                                                                                 [2]             E. Nichols, C.E. Szoeke, S.E. Vollset, N. Abbasi, F. Abd-Allah, J. Abdela, M.T.
      models                                                                                                                                            E. Aichour, R.O. Akinyemi, F. Alahdab, S.W. Asgedom, et al., Global, regional, and
                                                                                                                                                        national burden of  alzheimer‚Äô    s disease and other dementias, 1990‚Äì        2016: a
      The  GCN  model  is  employed  in  the  population  network,  and  it  is  a                                                                      systematic analysis for the global burden of disease study 2016, Lancet Neurol. 18
                                                                                                                                                        (1) (2019)  88‚Äì        106.
first-order         model         and         no         attention         mechanism.         Recently,         attention                        [3]             A. Association, 2019 alzheimer‚Äô    s disease facts and figures, Alzheimer‚Äô    s Dementia
mechanisms    have    been    widely    used    in    various    tasks.    The    goal    of    the                                                     15 (3) (2019)  321‚Äì        387.
attention mechanism is to select information that is relatively critical to                                                                      [4]             Q. Li, X. Wu, L. Xu, K. Chen, L. Yao, R. Li, Multi-modal discriminative dictionary
                                                                                                                                                        learning for alzheimer‚Äô    s disease and mild cognitive impairment, Comput. Methods
the current task from all input. We employ GAT (Graph Attention Net-                                                                                    Progr. Biomed. 150 (2017) 1‚Äì        8.
works)     [45]     with     leveraging     masked     self-attentional     layers     to     assign                                             [5]             A.S. Heinsfeld, A.R. Franco, R.C. Craddock, A. Buchweitz, F. Meneguzzi,
different    weights    to    different    nodes    in    a    neighborhood.    On    the    other                                                      Identification of autism spectrum disorder using deep learning and the abide
                                                                                                                                                        dataset, Neuroimage: Clinical 17 (2018) 16‚Äì        23.
hand,                   we                   perform                   higher-order                   convolutions                   by                   incorporating [6]             G.S. Bajestani, M. Behrooz, A.G. Khani, M. Nouri-Baygi, A. Mollaei, Diagnosis of
higher-order      proximity      via      random      walks      in      GCN.      Integrating      more                                                autism spectrum disorder based on complex network features, Comput.  Methods
inherent information into the proposed hierarchical embedding learning                                                                                  Progr. Biomed. 177 (2019) 277‚Äì        283.
                                                                                                                                                 [7]             M. Khosla, K. Jamison, G.H. Ngo, A. Kuceyeski, M.R. Sabuncu, Machine learning in
framework,    which    is    expected    to    gain    better    results.    Specifically,    we                                                        resting-state fmri analysis, Magn. Reson. Imag. 64 (2019) 101‚Äì        121.
employ        a        random        walk        sampling        process        on        graphs        to        obtain        the              [8]             S. Qi, S. Meesters, K. Nicolay, B.M. ter Haar Romeny, P. Ossenblok, The influence of
higher-order proximity representations. In our framework, the random                                                                                    construction methodology on structural brain network measures: a review,
walk strategy helps incorporate the higher-order structural information                                                                                 J. Neurosci. Methods 253 (2015) 170‚Äì        182.
                                                                                                                                                 [9]             Y. Zhu, S. Qi, B. Zhang, D. He, Y. Teng,  J. Hu, X. Wei, Connectome-based
into the graph representations, which further allows for the higher-order                                                                               biomarkers predict subclinical depression and identify abnormal brain connections
graph convolutions to capture community and organizational structure                                                                                    with the thalamus and lateral habenula, Front. Psychiatr. 10 (2019) 371.
of the population network. In Table 13, we report the comparison results                                                                       [10]             S. Qi, Q. Gao, J. Shen, Y. Teng, X.  Xie, Y. Sun,  J. Wu, Multiple frequency bands
                                                                                                                                                        analysis of  large scale intrinsic brain networks and its application in schizotypal
of      the      different      versions      of      GCN      models.      We      can      observe      that      both                                personality disorder, Front. Comput. Neurosci. 12 (2018) 64.
higher-order   GCN  and  GAT  perform   better  than  the   GCN  model,  indi-                                                                 [11]             A. Khazaee, A. Ebrahimzadeh, A. Babajani-Feremi, Application of advanced
cating that more correlation or attention mechanisms applied in popu-                                                                                   machine learning methods on resting-state fmri network for identification of mild
                                                                                                                                                        cognitive impairment and alzheimer‚Äô    s disease, Brain imaging and behavior 10 (3)
lation    network   are   more   effective   in    capturing   more    information   and                                                                (2016) 799‚Äì        817.
learning         more         accurate         brain         network         representations.         The         results                      [12]             W. Mier, D. Mier, Advantages in functional imaging  of the brain, Front. Hum.
further demonstrate the advantage of a hierarchical embedding learning                                                                                  Neurosci. 9 (2015) 249.
                                                                                                                                               [13]             R. Salvador, J. Suckling, M.R. Coleman, J.D. Pickard, D.  Menon, E. Bullmore,
model with two level GCN models. Furthermore, the results suggest that                                                                                  Neurophysiological architecture of functional magnetic resonance images of
devising      effective      strategies      to      learn      the      network      embedding      in      the                                        human brain, Cerebr. Cortex 15 (9)  (2005) 1332‚Äì        1342.
population       graph       is       essential       to       facilitate       the       learning       of       the       brain              [14]             J. Wang, X.  Zuo, Y. He, Graph-based  network analysis  of resting-state functional
network embedding and the diagnosis performance.                                                                                                        mri, Front. Syst. Neurosci. 4 (2010)  16.
                                                                                                                                               [15]             J.D. Medaglia, Graph  theoretic analysis of  resting state functional mr imaging,
                                                                                                                                                        Neuroimaging Clinics 27 (4)  (2017) 593‚Äì        607.
5.                 Conclusion                                                                                                                  [16]             T.-E. Kam, H. Zhang, Z. Jiao, D.  Shen, Deep learning of  static and dynamic brain
                                                                                                                                                        functional networks for early mci detection, IEEE Trans.  Med. Imag..
                                                                                                                                               [17]             D. Wen, Z. Wei, Y. Zhou, G. Li, X. Zhang, W. Han, Deep learning methods to process
      Recently,       functional       connectivity       networks       constructed       from       the                                               fmri data and their application in the diagnosis of  cognitive impairment: a brief
functional     magnetic     resonance     image     (f-MRI)     hold     great     promise     for                                                      overview and our opinion, Front. Neuroinf. 12 (2018)  23.
distinguishing      the      patients      with      neurological      disorders      from      Normal                                         [18]             X. Chen, H. Zhang, S.-W. Lee, D. Shen, A.D.N. Initiative, et al., Hierarchical high-
                                                                                                                                                        order functional  connectivity networks and selective feature fusion for mci
controls.   Network   embedding   is   aimed   at   learning   compact   node   rep-                                                                    classification, Neuroinformatics 15 (3)  (2017) 271‚Äì        284.
resentations   based  on  network  topology  to  facilitate  the   task  of  graph                                                             [19]             H. Guo, L. Liu, J. Chen, Y. Xu, X. Jie, Alzheimer classification using a minimum
classification. In order to achieve a better graph embedding from brain                                                                                 spanning tree of high-order functional network on fmri dataset, Front. Neurosci. 11
                                                                                                                                                        (2017) 639.
networks,   we    develop    a   novel    and   principled   framework   for    network                                                        [20]             M.A. Ebrahimighahnavieh, S. Luo, R. Chiong, Deep learning to detect alzheimer‚Äô    s
embedding    learning    by    efficiently    integrating    correlations    among    the                                                               disease from neuroimaging: a systematic literature review, Comput. Methods
subjects  in  a  population  with  GCN.  We  conduct  extensive  experiments                                                                            Progr. Biomed. 187 (2020)  105242.
                                                                                                                                               [21]             A.S. Lundervold, A. Lundervold, An overview of deep learning in medical imaging
on    real-world    information    networks    to    verify    the   effectiveness    of   our                                                          focusing on mri, Z. Med. Phys. 29 (2) (2019) 102‚Äì        127.
model,     which     demonstrates     its     superior     performance     compared     with                                                   [22]             G. Litjens, T. Kooi, B.E. Bejnordi, A.A.A. Setio, F. Ciompi, M. Ghafoorian, J.A. Van
state-of-the-art       baselines.       It       also       achieves       faster       training       and       easier                                 Der Laak, B. Van Ginneken, C.I. S¬¥anchez, A survey on deep learning in medical
                                                                                                                                                        image analysis, Med.  Image Anal. 42 (2017)  60‚Äì        88.
convergences.                                                                                                                                  [23]             X. Yue, Z. Wang, J. Huang, S. Parthasarathy, S. Moosavinasab, Y. Huang, S.M. Lin,
                                                                                                                                                        W. Zhang, P. Zhang, H. Sun, Graph embedding on biomedical networks: methods,
Declaration  of competing  interest                                                                                                                     applications and evaluations, Bioinformatics 36 (4) (2020)  1241‚Äì        1251.
                                                                                                                                               [24]             A. Grover, J. Leskovec, node2vec: scalable feature learning for networks, in:
                                                                                                                                                        Proceedings of the 22nd  ACM SIGKDD International Conference on Knowledge
      We declare that we have no financial and personal relationships with                                                                              Discovery  and Data Mining, 2016, pp. 855‚Äì        864.
other     people     or     organizations     that     can     inappropriately     influence     our                                           [25]             J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, Q. Mei, Line: large-scale information
                                                                                                                                                        network embedding, in: Proceedings of  the 24th International Conference on
work, there is no professional or other personal interest of any nature or                                                                              World Wide Web, 2015,  pp. 1067‚Äì        1077.
kind in any product, service and/or company that could be construed as                                                                         [26]             T.  N. Kipf, M. Welling, Semi-supervised Classification with Graph Convolutional
influencing  the position  presented in, or  the review of, the  manuscript                                                                             Networks, arXiv preprint  arXiv:1609.02907.
                                                                                                                                               [27]             S. Parisot, S.I. Ktena, E. Ferrante, M. Lee, R.G. Moreno, B. Glocker, D. Rueckert,
entitled,  ‚Äú        Hi-GCN: A hierarchical graph convolution network for graph                                                                          Spectral graph  convolutions for  population-based disease prediction, in:
embedding learning of brain network and brain disorders prediction‚Äù        .

H. Jiang et al.
       International Conference on Medical Image Computing and Computer-Assisted                                                [36]             C. Craddock, Y. Benhajali, C. Chu, F. Chouinard, A. Evans, A. Jakab, B. S.
       Intervention, Springer, 2017, pp. 177‚Äì        185.                                                                               Khundrakpam, J. D. Lewis, Q. Li, M. Milham,  et al., The neuro bureau
[28]             X.  Li, N.C. Dvornek, Y.  Zhou, J. Zhuang,  P. Ventola, J.S. Duncan, Graph  neural                                     preprocessing initiative: open sharing of  preprocessed neuroimaging data and
       network for  interpreting task-fmri  biomarkers, in: International Conference on                                                 derivatives, Neuroinformatics 41.
       Medical Image Computing and Computer-Assisted Intervention, Springer, 2019,                                              [37]             J. Kawahara, C.J. Brown, S.P. Miller, B.G. Booth, V. Chau, R.E. Grunau, J.
       pp. 485‚Äì        493.                                                                                                             G. Zwicker,  G. Hamarneh, Brainnetcnn: convolutional neural networks for brain
[29]             S.I. Ktena, S. Parisot, E. Ferrante, M. Rajchl, M. Lee, B. Glocker, D. Rueckert, Metric                                networks; towards  predicting neurodevelopment, Neuroimage 146 (2017)
       learning with spectral graph convolutions on brain connectivity networks,                                                        1038‚Äì        1049.
       Neuroimage 169 (2018) 431‚Äì        442.                                                                                   [38]             C.-Y. Wee, P.-T. Yap, D. Zhang, K. Denny, J.N. Browndyke, G.G. Potter, K.A. Welsh-
[30]             S. Parisot, S.I. Ktena, E. Ferrante, M. Lee, R. Guerrero, B. Glocker, D.  Rueckert,                                    Bohmer, L. Wang, D. Shen, Identification of mci individuals using structural and
       Disease prediction using graph convolutional networks: application to autism                                                     functional connectivity networks, Neuroimage 59 (3) (2012) 2045‚Äì        2056.
       spectrum disorder and alzheimer‚Äô    s disease, Med. Image Anal. 48 (2018) 117‚Äì        130.                               [39]             B. Cao, L. He, X. Wei, M. Xing, P.S. Yu, H. Klumpp, A.D. Leow, t-bne, Tensor-based
[31]             N.  Tzourio-Mazoyer, B. Landeau, D. Papathanassiou, F. Crivello, O. Etard,                                             brain network embedding, in: Proceedings of  the 2017 SIAM International
       N. Delcroix, B. Mazoyer, M. Joliot, Automated anatomical labeling of activations in                                              Conference on Data Mining, SIAM, 2017, pp. 189‚Äì        197.
       spm using a macroscopic anatomical parcellation of the mni mri  single-subject                                           [40]             T.  Kudo, E. Maeda, Y. Matsumoto, An application of boosting to graph
       brain, Neuroimage 15 (1) (2002)  273‚Äì        289.                                                                                classification, in: Advances in Neural Information Processing Systems, 2005,
[32]             M. Defferrard, X.  Bresson, P. Vandergheynst, Convolutional neural networks on                                         pp. 729‚Äì        736.
       graphs with fast localized spectral filtering,  in: Advances in Neural Information                                       [41]             D. Zhang, J. Huang, B. Jie, J. Du, L. Tu, M. Liu, Ordinal pattern: a new descriptor
       Processing Systems, 2016, pp. 3844‚Äì        3852.                                                                                 for brain connectivity networks, IEEE Trans. Med. Imag. 37 (7) (2018) 1711‚Äì        1722.
[33]             Y. Ma, S. Wang, C.C. Aggarwal, J. Tang, Graph convolutional networks with                                      [42]             A. Abraham, M.P. Milham, A. Di Martino, R.C. Craddock,  D. Samaras, B. Thirion,
       eigenpooling, in: Proceedings of  the 25th ACM SIGKDD International Conference                                                   G. Varoquaux, Deriving reproducible biomarkers from multi-site resting-state data:
       on Knowledge Discovery  &              Data Mining, 2019, pp. 723‚Äì        731.                                                   an autism-based example, Neuroimage 147 (2017) 736‚Äì        745.
[34]             A. Di Martino, C.-G. Yan, Q. Li, E. Denio, F.X. Castellanos, K. Alaerts, J.S. Anderson,                        [43]             F. Saeed, T. Eslami, V. Mirjalili, A. Fong, A. Laird, Asd-diagnet, A hybrid learning
       M. Assaf, S.Y. Bookheimer, M. Dapretto, et al., The autism brain imaging data                                                    approach for detection of  autism spectrum disorder using fmri data, Front.
       exchange: towards a large-scale evaluation of  the intrinsic brain architecture in                                               Neuroinf. 13 (2019) 70.
       autism, Mol. Psychiatr. 19 (6)  (2014) 659‚Äì        667.                                                                  [44]             N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, K. M. Borgwardt,
[35]             K. Dadi, M. Rahim, A. Abraham, D. Chyzhyk, M. Milham, B. Thirion, G. Varoquaux,                                        Weisfeiler-lehman graph  kernels., J. Mach. Learn. Res. 12 (9).
       A.D.N.  Initiative, et al., Benchmarking functional connectome-based predictive                                          [45]             P. VeliÀáckovi¬¥c, G. Cucurull, A. Casanova, A. Romero, P. Lio, Y. Bengio, Graph
       models for  resting-state fmri, Neuroimage 192 (2019)  115‚Äì        134.                                                          Attention Networks, arXiv preprint arXiv:1710.10903.

