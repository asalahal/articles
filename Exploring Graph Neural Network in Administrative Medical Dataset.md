                                              2022 International Conference on Technologies and Applications of Artificial Intelligence (TAAI)
                Exploring                              Graph                              Neural                              Network                              in                              Administrative
                                                                                                                                                      Medical                              Dataset
                                                            Wei-Chen             Liu                                                                                                Chih-Chieh             Hung                                                                                                      Wen-Chih             Peng
                 National               Yang               Ming               Chiao               Tung               University                                National               Chung               Hsing               University                                     National               Yang               Ming               Chiao               Tung               University
                                                         Hsinchu,            Taiwan                                                                                                   Taichung,            Taiwan                                                                                                    Hsinchu,            Taiwan
                                       rikowendy.cs08@nycu.edu.tw                                                                                                           smalloshin@nchu.edu.tw                                                                                                        wcpeng@cs.nycu.edu.tw
                     Abstract—Administrative                          medical                          dataset                          contains                          medical                                              prediction              methods              are              used              on              recommendation              system              [5],
             records                    of                    patients.                    Using                    administrative                    medical                    dataset                    can                [6].        However,        none        of        these        methods        are        used        in        administrative
             extract              disease              association              to              help              ﬁnding              comorbidity.              Previous                                                       medical            dataset.
             methods                     only                     use                     statistics                     on                     administrative                     medical                     dataset                 In            this            work,            we            proposed            two            ways            to            ﬁnd            such            informa-
             such                  as                  computing                  probabilities                  of                  disease                  occurrence                  and                  are             tion.              One              is              to              use              k-nearest              neighbors              algorithm              after              node
             limited                by                the                capability                of                statistics.                To                enhance                hidden                infor-
             mation             usage             of             administrative             medical             dataset,             we             propose             two                                                    embedding            from            GNN            to            ﬁnd            similar            diseases.            We            formulate
             different                    methods                    based                    on                    graph                    neural                    networks                    to                    exploitadministrative                medical                dataset                to                sequences                and                construct                a
             hidden                     information                     in                     administrative                     medical                     dataset.                     One                     is          graph.               The               other               one               is               to               use               Gated               graph               neural               network
             using                     graph                     neural                     networks                     to                     generate                     disease                     embeddings            (GGNN)             to             combine             past             records             of             patients             and             recent             con-
             and                    pass                    through                    kNN                    algorithm                    to                    ﬁnd                    similar                    diseases                    forditions.              We              formulate              this              as              a              sequence              prediction              problem
             suggestion                    to                    physicians.                    The                    other                    one                    is                    that                    we                    formulate
             sequence         prediction         problem         and         use         gated         graph         neural         network                                                                                    since               every               health               record               of               a               single               patient               can               be               viewed
             to                       model                       every                       disease                       sequence                       by                       forming                       session                       graphs.as            a            sequence.            Speciﬁcally,            we            use            global            and            local            sequence
             Different                    from                   previous                    methods                   for                    sequence                   prediction                    that                    embedding            to            collect            disease            information            in            sequences            of            pa-
             only                 consider                 current                 sequence,                 we                 also                 consider                 all                 sequences                    tients.           Moreover,           we           use           soft-attention           mechanism           to           leverage
             in             dataset             at             the             same             time.             Besides,             we             use             position-aware             soft-                         the             importance             of             global             and             local             information.             Furthermore,
             attention             mechanism             to             aggregate             disease             embeddings             to             session
             embeddings          and          predict          the          next          disease          of          a          patient.          We          conduct                                                        we             add             reverse             position             embedding             to             preserve             disease             order
             extensive                  experiments                  on                  two                  methods                  and                  show                  its                  ability                  toin               every               sequence.               Finally,               we               predict               next               possible               disease
             outperform              several              baselines.                                                                                                                                                           with            each            sequence            embedding.
                     Index                               Terms—graph                       neural                       networks,                       sequence                       prediction,                                     To            summarize,            our            contributions            is            as            follows:
             medical              datas
                                                                                                                                                                                                                                       •                       We                     propose                     a                     graph                     based                     model                     taking                     global                     and
                                                                                I.              I NTRODUCTION                                                                                                                                  local             information             into             account             simultaneously             to             predict
                     Administrative           medical           dataset           is           one           of           health           care           dataset                                                                              possible            diseases            in            the            future            for            patients.
                                                                                                                                                                                                                                       •                       In        our        experiments,        we        demonstrate        that        graph        embedding
             which              contains              medical              records              of              patients              such              as              diseases,                                                              methods                  are                  capable                  to                  extract                  disease                  relationship                  in
             treatments           and           drugs.           Deeper           insights           of           administrative           medi-                                                                                               administrative            medical            dataset.
             cal            dataset            can            let            physicians            make            any            decision            of            treatment                                                          •                       To            the            best            of            our            knowledge,            we            are            the            ﬁrst            to            extract
             more                precisely.                One                implicit                information                is                the                relationship                                                             disease                      relationship                      by                      formulating                      it                      as                      a                      sequence
             between           different           diseases           such           as           comorbidity           and           complica-                                                                                                prediction            task            and            using            GNN.
             tion.               However,               directly               understanding               every               disease               requires
             massive               domain               knowledge,               and               cost               plenty               of               time.               Tradi-                                                                                                         II.              R  ELATED              W ORK
             tionally,            we            use            statistic            methods            to            measure            disease            associa-                                                            A.                      Graph               Neural               Network
             tion.            Yang            [1]            considers            ranking            among            diseases            and            entropy
             computed             by             occurrence             probability             and             do             multiplication             to                                                                           Nowadays,               graph               neural               network               (GNN)               has               been               widely
             get             distance             between             diseases.             These             methods             are             simple             but                                                       applied           in           different           research           ﬁelds.           Perozzi           et           al.           [2]           proposed
             are            limited            in            deeper            insights.                                                                                                                                       a          random-walk          based          method          to          capture          graph          information          by
                     Recently,                 Graph                 neural                 network                 (GNN)                 shows                 its                 power                                      skip-gram        model.        Tang        et        al.        [7]        takes        ﬁrst-order        neighbors        and
             in              different              ﬁelds,              such              as              nature              language              processing,              com-                                              second-order            neighbors            into            account            while            computing            hidden
             puter              vision,              data              mining              and              so              on.              Different              from              statistic                                vectors          of          nodes          in          graph.          Similar          to          [2],          Grover          et          al.          [8]          add
             methods,         GNN         can         model         non-grid         structured         datas         like         social                                                                                      two              additional              hyperparameters              to              control              how              random              walk
             networks               [2],               citation               networks               [3]               and               rating               networks               [4].                                      be            generated            to            extract            local            and            global            structure            information
             These             methods             capture             node             relations             and             transfer             to             hidden                                                       in            graph.
             message             in             latent             vectors             of             each             node,             which             is             unavailable                                                  Inspired                by                convolutional                neural                network,                Kipf                et                al.                [3]
             for                non-graph                methods.                On                the                other                hand,                many                sequence                                   proposed            a            model            derived            from            spectral            theorem.            Though            the
2376-6824/22/$31.00 ©2022 IEEE                                                                                                                                                                                   107
DOI 10.1109/TAAI57707.2022.00028
     Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on February 18,2025 at 13:21:59 UTC from IEEE Xplore.  Restrictions apply.

       model                  architecture                  is                  simple,                  it                  performs                  better                  than                  other                                 B.                      Graph               Centrality
       methods                  and                  bring                  new                  insights                  such                  as                  local                  aggregation                                             We                  introduce                  three                  different                  graph                  centrality                  which                  is                  a
       of             neighbors             into             GNN.             Hamilton             et             al.             [9]             further             proposed                                                             concept            to            identify            the            importance            of            nodes            in            a            graph.            For            a
       learnable             aggregation            functions             to             enhance            information             prop-                                                                                                  certain            node      v ,            the            graph            centralities            are            deﬁned            as            follows:
       agation               across               graph               and               make               it               capable               to               learn               on               large-                                      •                            Degree                             centrality                      :                      the                      fraction                      of                      nodes                      that           v                       is
       scale                 graphs.                 As                 attention                 mechanism                 grows,                 Veli ˇckovi ´cet                                                                                         connected            to.
       al.                [10]                use                self                attention                to                determine                importance                between                                                          •                            Closeness               centrality            :            reciprocal            of            the            average            shortest
       different                     node                     neighbors                     while                     aggregating                     node                     features.                                                                    path            distance            to      v             over            all            other            reachable            nodes.
       These                    methods                    give                    a                    large                    improvement                    in                    graph                    neural                               •                            Betweenness             centrality           :           sum           of           the           fraction           of           all-pairs
       networks.                                                                                                                                                                                                                                            shortest            path            that            pass            through      v .
       B.                      Session               Based               Recommendation                                                                                                                                                    We                   show                   the                   importance                   of                   diseases                   by                   computing                   graph
                Sequence                      prediction                      is                      highly                      related                      to                      session                      based                  centrality            on            global            graph            in            our            experiment.
       recommendation.              Hidasi              et              al.              [11]              proposed              a              model              which                                                                   C.                      Problem               Statement
       stacks                    several                    recurrent                    neural                    network                    layers                    in                    encoder-                                              We            propose            two            different            tasks            in            this            article.
       decoder                 form                 and                 use                 parallel                 training                 to                 resolve                 sequence                                                   Problem                 Statement              1             :             Let      v               be             a             query             disease.             Given      v ,
       alignment          for          different          sequence          lengths.          Xu          et          al.          [12]          brings                                                                                    we           aim           to           ﬁnd           a           similar           disease           list           L     =     {l                1,l2,  ...,  lk }           which
       convolutional        neural        network        (CNN)        into        session        based        recom-                                                                                                                       is            ranked            by            similarity            where      lk               denotes            different            diseases.
       mendation              and              apply              on              session              embedding              to              take              different                                                                           Problem                      Statement                  2                 :                 Let        V             =         {v                1,v2,  ...,  vn}                 denotes
       features        and        items        into        account        simultaneously.        These        methods                                                                                                                      all                         diseases                         appear                         in                         dataset.                         Given                         a                         sequence            S                 =
       basically            rely            on            RNN.                                                                                                                                                                             {v                 s,1        ,vs,2           ,  ...,  vs,k }            where      vs,i     ∈        V        ,            the            goal            is            to            predict            the
                Different        from        above        approaches,        Wu        et        al.        [13]        make        use        of                                                                                          next            possible            disease      vs,k+1 .
       gated             graph             neural             network             [14]             to             get             node             embedding             and
       generate               session               embedding               for               each               session               graph               by               soft-                                                                                                                        IV.              P ROPOSED       METHOD
       attention         mechanism         instead         of         using         RNN         on         session         directly.                                                                                                                In             this             section,             we             introduce             how             we             ﬁnd             similar             diseases
       In        order        to        enhance        the        ability        of        node        embedding        from        session                                                                                                how           we           predict           possible           disease.           Figure           1           shows           the           architec-
       graph,             Wang             et             al.             [15]             not             only             use             session             graphs             but             also                                    ture            of            our            proposed            method.
       takes          global          graph          training          into          account          to          generate          additional
       node           embedding.           They           also           proposed           reverse           position           embed-                                                                                                    A.                      Similar               Diseases
       ding               for               attention               to               preserve               the               order               of               items               in               session.                                    We                ﬁrst                construct                a                global                graph             Gg    =(Vg ,   Eg )                contain-
       Most                    of                    the                    approaches                    in                    session                    based                    recommendation                                         ing                 all                 disease                 sequences                 as                 we                 mentioned                 in                 III-A                 where
       follows         an         particular         architecture.         These         methods         ﬁrst         generate                                                                                                             Vg            =          {v                1,v2,  ...,  vn}                  contain         n                  nodes                  and         Eg                     denotes                  all
       session          embedding          with          different          technology          for          each          session,                                                                                                        direct               edges               in               global               graph.               Then,               we               apply               DeepWalk               [2]
       then             predict             to             every             target             item             individually.             Our             work             also                                                           and            node2vec            [8]            to      Gg               and            generate            disease            embeddings            for
       follow            this            architecture            to            predict            next            possible            disease.                                                                                             every        disease.        After        that,        we        use        k-NN        algorithm        [16]        on        query
                                                                            III.              P    RELIMINARIES                                                                                                                            disease         to         determine         which         diseases         are         closer         to         it.         We         output
                                                                                                                                                                                                                                           a                 top-k                 distance                 list                 which                 is                 seem                 to                 be                 an                 indication                 of
                In                    this                    section,                    we                    introduce                    how                    we                    preprocessed                    our              comorbidity            for            the            query            disease.
       dataset          and          construct          graphs.          Besides,          we          also          introduce          some
       point             of             view             to             graphs.             Finally,             we             show             how             we             formulate                                                  B.                      Session               Graph
       our            work            as            a            sequence            prediction            problem.                                                                                                                                 As                we                mentioned                in                III-A,                we                model                medical                records                to
       A.                      Graph               Construction                                                                                                                                                                            a                    sequence                    for                    every                    single                    patient.                    Each                    sequence          S             =
                                                                                                                                                                                                                                           {v                 s,1        ,vs,2           ,  ...,  vs,k }           can           then           be           formulated           to           a           session           graph
                The              original              datas              of              administrative              medical              dataset              is              the                                                        Gl     =(Vl,   El)                     where          {v                 s,1        ,vs,2           ,  ...,  vs,k}∈V                                l                       and                     edge
       medical                   records                   including                   disease                   codes,                   patient                   ID,                   time,                                            (vs,i,vs,i+1                 )   ∈E     l            for     i   =0,   1,  ...,  k           −      1.          Same          as          global          graph,
       hospital            and            others.            We            ﬁrst            extract            datas            of            patients            individ-                                                                  we              aggregate              duplicated              edges              to              a              single              edge              with              weight
       ually.           Then,           we           create           a           sequence           of           diseases           for           each           patient                                                                  to           avoid           multi-graph.           For           each           node     v     ∈       Vl ,           we           embed           it           to           a
       sorted                 by                 time.                 In                 administrative                 medical                 dataset,                 there                 are                                        latent            vector           v     ∈            Rd .
       several               diseases               in               a               single               medical               record.               The               ﬁrst               one               is                            C.                      Local               Disease               Embedding
       the               main               disease               and               the               rest               are               secondary               diseases               which
       can                 be                 ignored.                 Thus,                 We                 only                 consider                 the                 main                 disease.                                     We        then        show        how        we        get        local        disease        embedding        by        graph
       Besides,         we         remove         continuous         identical         diseases         since         we         put                                                                                                       neural              network.              This              graph              neural              network              was              proposed              by
       our                attention                on                the                relationship                between                different                diseases.                                                              Li          et          al.          [14]          which          combines          gated          recurrent          unit          (GRU).          The
       Finally,                  we                  combine                  all                  sequences                  to                  form                  a                  directed                  and                   gated            graph            neural            network            is            suitable            for            our            goal            since            it            is
       weighted            global            graph.                                                                                                                                                                                        designed              for              sequences.              We              use              the              same              strategy              as              Wu              et
                                                                                                                                                                                                                            108
Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on February 18,2025 at 13:21:59 UTC from IEEE Xplore.  Restrictions apply.

         Fig.         1.                   An         overview         of         disease         prediction.    ⊕                   denotes         element-wise         sum.         First,         a         global         graph         containing         all         sequences         and         session         graph         for         each         sequence
         are           formed.           Then,           we           apply           gated           graph           neural           network           on           both           graph           and           get           node           embeddings.           After           that,           each           session           embedding           is           obtained           through
         attention          with          position          information          and          ﬁnally          come          to          prediction          for          next          disease.
         al.          [13]          to          model          session          graph.          For          node     vi ,          the          methodology
         of            gated            graph            neural            network            are            shown            as            follows:
                                          at                                         :                        [vt−                  1             −                  1
                                               in,i     =         Alin,i                                     1                 ,  ...,     vtn                ]⊤ Win    +       bin,                                                                                                       (1)
                                      at                                                                                                              −                  1
                                           out,i     =         Alout,i:[vt−         11                 ,  ...,     vtn                ]⊤ Wout    +       bout,                                                             (2)
                                                    ati      =    Concat(atin,i,     atout,i),                                                                                                                                                                                                                                                                                                                      (3)
                                                    zt                                                                                      −                  1
                                                         i      =     σ (Wz  ati     +       Uz  vti                   ),                                                                                                                                                                                                                                                                                                                          (4)
                                                    rti      =     σ (Wr  ati     +       Ur  vt−         1i                   ),                                                                                                                                                                                                                                                                                                                           (5)
                                                   vt˜                    (                                                                    Wo at                               −                  1,                                                                                                                                                                                             (6)
                                                         i      =     tanh                                              i     +       Uo(rti     ⊙       vti                   ))
                                                   vti    =(1    −                zti )    ⊙       vt−         1i                       +       zti     ⊙        ˜vti ,                                                                                                                                                                                                                                     (7)
                   where               Win,     Wout         ∈                   Rd×d ,               Wz ,     Wr ,     Wo         ∈                   Rd×2d                                 are
         parameter                 matrices                 and               vi                  denotes                 latent                 vector                 of                 node        i.                                                                                Fig.         2.                   An         example         of         adjacency         matrix         Ain           and         Aout           with         sequence         S      =
         zti                 and              rti                 are               update               and               reset               gate               respectively.       σ (·)               is               the                                                           {v            1 ,v2 ,v3 ,v2 ,v4 ,v3 ,v2}.          Red          edge          denotes          edge          weight=2.
         sigmoid             function             and      ⊙             denotes             element-wise             multiplication.
         Alin,i:                        ,     Alout,i:          ∈                   Rn×                 n                   are                 the        i-th                 column                 of                 Adjacency
         matrix                 Ain                    and                 Aout                    corresponding                  to                  node         vi .                  Figure                  2                                                                       E.                      Reverse               Position               Embedding
         shows            how            adjacency            matrix           Alin              and           Alout              are            formed.                                                                                                                                           Through               session               (local)              and               global               graph,               we              obtain               both
                   For           a           session           graph     Gl ,           we           generate           latent           vectors          atin,i            and                                                                                                          local          disease          embedding         vl            and          global          disease          embedding         vg  .
         atout,i           from         two         different         views         by        Alin           and        Alout .         The         reason                                                                                                                               We            combine            them            by            summation,
         is           that           for           a           directed           graph,           incoming           and           outgoing           edges           can                                                                                                                                                                                                               v′     =         vl    +       vg .                                                                                                                                                                                                                                                                                                                    (8)
         provide          different          message,          i.e.,          the          difference          between          starting
         and          destination          node.          Then,          we          concatenate          two          latent          vectors                                                                                                                                           For             a             sequence            S     =     {v                1,v2,  ...,  vk },             we             obtain             its             latent             vec-
         and               pass               through               update               and               reset               gate.               Finally,               we               get               the                                                                         tors             of             each             node      vi      ∈               S,             i.e.,            S   =[v′1,     v′2,  ...,     v′k ].             Then,             we
         latent            vector           vl,i              of            node      vi .                                                                                                                                                                                               use         a         learnable         position         embedding         matrix        P   =[p1,     p1,  ...,     pl]
                                                                                                                                                                                                                                                                                         where               pi         ∈                  Rd .                We                concatenate                latent                vectors                of                nodes                in
         D.                      Global               Disease               Embedding                                                                                                                                                                                                    sequence           S            with            position            embedding            vectors:
                   Different                  form                  Wang                  et                  al.                  [15],                  we                  apply                  the                  identical                                                                                                                             vi     =    Concat(v′i,     pl−         i+1     ).                                                                                                                                                                                                                         (9)
         gated          graph          neural          network          to          local          graph          on          our          global          graph
         instead                of                using                attention                mechanism.                For                global                graph        Gg  ,                                                                                                    Please                 notice                 that                 here                 we                 use                 reverse                 position                 embedding
         we                 formulate                 adjacency                 matrix                Agin                   and                Agout .                 For                 node                                                                                         instead                   of                    normal                    position                    embedding.                    The                   reason                    is                   that
         vi     ∈G     g  ,                   we                   can                   get                   its                   global                   node                   embedding                  vg,i                     by                                              disease                   sequences                   are                   not                   in                   equal                   length                   since                   some                   pa-
         equation            (1)            to            (7)            with            different            set            of            parameters.                                                                                                                                   tients                   are                   healthier                   well                   others                   are                   easier                   to                   get                   sick.                   For
                                                                                                                                                                                                                                                                      109
Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on February 18,2025 at 13:21:59 UTC from IEEE Xplore.  Restrictions apply.

         example,               considering               sequence              S1        =       {v                1,v2,v3}               and              S2        =                                                                                                                                                                                      TABLE          I
         {v                1,v2,v3,v4,v5}.                 v2                  should                have                greater                inﬂuence                to                pre-                                                                                                                                S TATISTICS     OF     THE     DATASET               .
         diction            in           S1             than           S2             since            it’s            farther            from            end            of            sequence.                                                                                                                                             statistics                                                                                                                                               NHIRD
         Besides,                the                inﬂuence                of             v2                  in               S1                  and        v4                  in               S2                  should                be                                                                                       #          of          diseases                                                                                                                                 4,639
         same.             With             reverse             position             embedding,             we             can             obtain             more                                                                                                                                                                        #          of          edges                                                                                                                                 143,589
         proper            node            embedding.                                                                                                                                                                                                                                                                               #          of          sequences                                                                                                               33,542
                                                                                                                                                                                                                                                                                                                     Avg.          length          of          sequences                                                         13.09
         F.                      Session               Embedding               and               Prediction                                                                                                                                                A.                      Dataset
                 To             predict             the            labels,             we             consider             local             and            global             session                                                                              To                     evaluate                    the                     performance                     of                    our                     model,                     we                    use                     an
         embedding,              which              provide              current              session              status              and              overall                                                                                            administrative           medical           dataset           collected           from           National           Health
         session                      hidden                      information                      respectively.                      For                      local                      session                                                          Insurance            Research            Database            (NHIRD)            in            Taiwan.            NHIRD            is
         embedding,           we           simply           adopt           latent           vector          vk             of           the           last           node                                                                                 a            dataset            that            records            ambulatory            care            expenditures            by            visits.
         in              sequence             S,              i.e.,             sl       =            vk  .              For              global              session              embedding,                                                              It                  contains                  International                  Classiﬁcation                  of                  Diseases                  (usually
         we            aggregate            all            latent            vectors            in            sequence           S.            Since            different                                                                                  called         ICD9         code)         which         represent         different         diseases         in         certain
         nodes         may         provide         different         information,         we         use         soft-attention                                                                                                                            clustering.         We         query         from         NHIRD         for         whole         year         in         2013         and
         mechanism              to              aggregate              latent              vectors              of              nodes              in              sequence                                                                                collect            patient            ID,            visit            date,            ICD9            code            for            our            experiments.
         S:                                                                                                                                                                                                                                                Table            I            shows            statistics            of            the            dataset.
                                                             α    i     =         q⊤σ (Wl vk     +       Wg  vi    +       c)         ,                                                                                                                    B.                      Baselines
                                                                                 ∑k                                                                                                                                                                                 For           ﬁnding           similar          disease,           we           use           two          well-known           models
                                                             sg      =                         α    i vi,                                                                                                                                                                                                                                                                                                                                                                                                                              (10)for            comparison            as            follows:
                                                                                  i=1                                                                                                                                                                               •                            DeepWalk                 [2]:                 DeepWalk                 learns                 node                 embeddings                 by
         where         q     ∈            Rd            and         Wl,     Wg      ∈            Rd×d            are          learnable          parameters.                                                                                                                 using                 random                 walk                 to                 generate                 random                 paths                 and                 use
         Then,               we               use              a               single               linear               transform               layer              to               produce               the                                                               skip-gram            model            to            learn            node            representations.
         ﬁnal            session            embedding:                                                                                                                                                                                                              •                            node2vec                   [8]:                   node2vec                   inherits                   the                   main                   concept                   of
                                                                                                                                                                                                                                                                             DeepWalk            [2]            and            bring            two            additional            hyperparameters
                                                                           sh     =         WhConcat(sl,     sg )                                                                                                                                                                                                                   (11)into                     random                     walk                     generation                     to                     simulate                     binary                     ﬁrst
                                                                             d                            is            a            learnable            parameter            matrix.                                                                                       search            (BFS)            and            depth            ﬁrst            search            (DFS).
         where           Wh     ∈            Wd×2                                                                                                                                                                                                                   •                            Comorbidity             Analysis             Platform           [1]:           This           platform           uses
                 Finally,              we              compute              the              probability              of              next              disease           ˆyi                by                                                                              a               distance               function               motivated               from               Yang               et               al.               [17]               by
         multiplying           session           embedding          sh             to           every           node          vi   ∈V     g              and                                                                                                                 computing            ranking            and            entropy            of            diseases.
         pass            through            a            softmax            function:                                                                                                                                                                               For          sequence          prediction,          we          compare          our          work          with          several
                                                                               yi     =     sof tmaxˆ            (s                                                 ⊤                                                                                      commonly            used            models            as            follows:
                                                                                                                                             h      vi).                                                                                                                                                                                                                                  (12)•                            LSTM             [18]:             LSTM             is             a             variant             of             recurrent             neural             net-
         To                 train                 the                 model,                 we                 use                 cross                 entropy                 loss                 as                 our                 loss                           work        (RNN)        to        leverage        gradient        vanishing        and        exploding
         function.               The               loss               function               of               the               total               model               is               deﬁned               as                                                             since            original            RNN            can’t            handle            sequences            which            is            too
         follows:                                                                                                                                                                                                                                                            long.
                                                                    n                                                                                                                                                                                               •                            GRU                 [19]:                 Based                 on                 LSTM,                 GRU                 speeds                 up                 training
                                                                ∑                                                                                                                                                                                                            procedure             by             combining             forget             and             input             gate             to             update
                          L( ˆy)=     −                                       yilog(   ˆyi)+(1        −                yi)log(1   −            ˆyi),                                                                     (13)                                                gate.
                                                                 i=1                                                                                                                                                                                                •                            GRU4REC             [11]:             GRU4REC             stacks             several             GRU             layers
         where             y               denotes               the               one-hot               encoding               vector               of               the               ground                                                                               and                      train                      with                      session-parallel                      mini-batches                      to                      avoid
         truth            disease.                                                                                                                                                                                                                                           session            fragmentation.
                                                                                                                                                                                                                                                                    •                            SR-GNN         [13]:         SR-GNN         uses         gated         graph         neural         network
                                                                                      V.                                 E XPERIMENTS                                                                                                                                        (GGNN)                      on                      session                      graphs                      to                      generate                      session                      em-
                                                                                                                                                                                                                                                                             beddings                and                soft-attention                mechanism                for                information
                 In               this               section,               we               ﬁrst               introduce               the               dataset               used               in               our                                                      priority            in            session            embeddings.
         experiments.                     Next,                     we                     simply                     introduce                     the                     baselines                     we                                                        •                            GCE-GNN                [15]:                GCE-GNN                not                only                generate                session
         used                   in                   experiments                   for                   comparison.                   After                   that,                   We                   show                                                             embeddings                   for                   each                   session                   but                   also                   apply                   attention
         the                   experiment                   results                   of                   our                   problem                   1                   :                   ﬁnding                   similar                                          mechanism        on        global        graph        to        enrich        item-item        transition.
         diseases.                   Then,                   we                   present                   three                   different                   experiments                   for                                                          C.                      Experiment               Settings
         further                discussion,                which                is                the                overall                performance                of                our
         model,                   the                   performance                   on                   different                   important                   nodes                   with                                                                     We                      follow                      the                      settings                      of                      SR-GNN                      [13].                      For                      training
         speciﬁc                 setting,                 and                 the                 effectiveness                 of                 different                 sequence                                                                      and               testing               set,               we               randomly               split               dataset               to               80%               and               20%
         length.                                                                                                                                                                                                                                           respectively.
                                                                                                                                                                                                                                          110
Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on February 18,2025 at 13:21:59 UTC from IEEE Xplore.  Restrictions apply.

        D.                      Evaluation               Metrics
                We          follow          previous          works                    [13],          [15]          by          adopting          following
        metrics:
                •                            MRR@k:                  Mean                  reciprocal                  rank                  (MRR)                  is                  deﬁned                  as
                        the               average               of               the               reciprocal               ranks               of               label               diseases               in
                        predictions.
                •                            Recall@k:               Recall               is               a               commonly               used               metric               in               many
                        prediction            and            classiﬁcation            problems.            It            represents            how
                        the            diseases            are            detected            in            top-k            prediction            list.
        Besides,         we         also         adopt         accuracy         as         our         evaluation         metric         since
        prediction            correctness            is            also            important            in            medical.
                                                                                                                                                                                                                                                                   Fig.          3.                    Performance          of          different          test          sequence          ﬁxed          length
        E.                      Similar               Disease               Detection
                Table           II           shows           a           case           study           of           our           result           of           similar           disease
        detection.            From            table            II,            we            observe            that            DeepWalk            performs
        the                     best                     as                     the                     comorbidity                     analysis                     platform                     does                     since
        four                  of                  top-5                  diseases                  are                  relevant                  to                  query                  diseases.                  This
        indicates                   that                   we                   can                   extract                   disease                   relationships                   through
        graph              structure.              However,              node2vec              performs              worse              on              both
        weighted            and            unweighted            graph            with            irrelevant            diseases.            The
        reason         is         that         node2vec         regards         nodes         which         have         similar         local
        structure              as              similar              nodes              and              thus              structural              information              of
        these               nodes               was               considered,               while               these               diseases               should               be
        treated            as            irrelevant            diseases.
        F.                      Next               Disease               Prediction                                                                                                                                                                                                Fig.          4.                    Performance          distribution          to          Recall@20
                For               next               disease               prediction,               we               follow               experiment               setting
        of            SR-GNN            [13]            which            generates            additional            sequences            while                                                                                                     The            results            are            shown            in            Table            III.            For            important            diseases,
        training.                 Speciﬁcally,                 we                 split                 a                 sequence                 of                 length              k                  to                 k          the                   performance                   are                   lower                   then                   overall                   performance                   since
        sequences.                    Each                    of                    them                    is                    a                    subsequence                    from                    the                    ﬁrst  important             diseases             are             inﬂuenced             by             many             diseases             and             thus
        disease                to                different                diseases.                For                example,                a                sequence               s        =                                           few        diseases        may        not        be        enough        to        provide        proper        information.
        {v                1,c2,  ...,  vk }             is             split             to             a             set             of             sequences             with             new             labels                         For            unimportant            diseases,            the            performance            are            unstable            since
        ({v                1},v                2),   ({v                1,v2},v                3),  ...,   ({v                1,v2,  ...,  vk−         1},v                 k ).                                                           these         diseases         appear         only         few         times         in         sequences.         As         a         result,
                Table               IV               shows               the               overall               performance               of               baselines               and                                                    the            performance            can            be            best            or            worst.
        our                   methods.                   For                   LSTM                   and                   GRU,                   since                   our                   disease                   se-
        quence                 length                 can                 be                 very                 different,                 simple                 architecture                 of
        RNN                can’t                perform                well                and                trend                to                predict                all                sequences                                   H.                      Performance               to               Sequence               Length
        to                similar                results.                SR-GNN                outperforms                RNN                methods                and                                                                            We                   analyze                   the                   inﬂuence                   of                   sequence                   length                   to                   perfor-
        demonstrates        the        effectiveness        of        graph        neural        networks.        GCE-                                                                                                                     mance.                     Figure                     3                     shows                     the                     performance                     of                     different                     test
        GNN             performs             better             than             SR-GNN             since             it             further             consider                                                                          length                      sequence.                      We                      slice                      the                      sequences                      to                      ﬁxed                      length
        global        information        and        reverse        position        embedding.        Our        model                                                                                                                      subsequences.           For           example,           to           slice           sequences           to           ﬁxed           length
        takes                     the                     advantage                     of                     global                     graph                     and                     reverse                     position           l,                    we                    slice                    all                    sequences                  si             =            {v                1,v2,  ...,  vk },k                     ≥                 l                     to
        embedding                  and                  use                  gated                  graph                  neural                  network                  for                  global                                    s′                  v                 k−         l+1,vk−         l+2,  ...,  vk }               and               discard               sequences               shorter
        graph,                 which                 is                 more                 suitable                 for                 sequence                 prediction                 than                                         then            length      l.            The            result            shows            that            the            performance            severelyi        =       {
        attention          mechanism          since          RNN          is          capable          to          preserve          hidden                                                                                                drops            when            ﬁxed            length            is            smaller            than            5.
        information              for              several              steps.              Hence,              our              model              outperforms                                                                                     We                 analyze                 how                 sequence                 length                 affect                 our                 performance
        all            baselines            in            our            experiment.                                                                                                                                                       from                  another                  view.                  The                  performance                  of                  different                  sequence
        G.                      Next               Disease               Prediction               of               Query               Disease                                                                                             length                  are                  shown                  in                  ﬁgure                  4.                  We                  compute                  Recall@20                  for
                We          ﬁrst          determine          which          diseases          are          important          diseases          by                                                                                         each                 sequence                 length                 in                 testing                 set                 individually.                 As                 shown
        computing               graph               centrality               and               deﬁne               diseases               with               highest                                                                       in                     ﬁgure                     4,                     the                     performance                     drops                     for                     sequences                     that                     is
        centrality            as            important            diseases.                                                                                                                                                                 shorter           than           5,           while           being           unstable           for           long           sequences.           Both
                We               formulate               next               disease               prediction               of               query               disease               as                                                   experiment          shows          that          short          sequences          cannot          provide          enough
        follows:            given            a            subsequence           s     =     {v                1,v2,v3,v∗ ,v4},            we            aim                                                                                capability           of           information           while           some           sequences           are           very           long
        to              predict              next              possible              disease              where       v∗                  is              a              query              disease                                        relatively.               On               the               other               hand,               long               sequences               are               fewer               and
        and      v1,v2,v3,v4              are            diseases            next            to      v∗   .                                                                                                                                thus            heavily            inﬂuence            the            performance            up            and            down.
                                                                                                                                                                                                                           111
Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on February 18,2025 at 13:21:59 UTC from IEEE Xplore.  Restrictions apply.

                                                                                                                                                                                                                                                                   TABLE          II
                                                                                                                                                                                                    C ASE     STUDY     OF     SIMILAR     DISEASE     DETECTION                                    .
                                                                                                                                Type                                       共病關係分析平台                                                                                       DeepWalk                                                                                              node2vec
                                                                                                                                                                                                                                                                     (unweighted)                                                              weighted                                                unweighted
                                                                                                                 Query          disease                                                                                                                                         急性上呼吸道感染
                                                                                                                                                                                                   感冒                                                                             齲齒                                                      持續性嘔吐                                                                  齲齒
                                                                                                                                                                                   急性支氣管炎                                                                         急性扁桃腺炎                                                                  鼻腔內息肉                                                          軀幹損傷
                                                                                                                Top-5          diseases                                                急性胃腸炎                                                                          急性鼻竇炎                                                                   咽結膜熱                                               浸潤性肺結核
                                                                                                                                                                                       流行性感冒                                                          急性鼻咽炎（感冒）                                                                               夜間遺尿                                           社會化行為障礙
                                                                                                                                                                                           急性喉炎                                                                   急性支氣管炎                                                                      鼻之異物                                                           脂肪瘤
                                                                                                                                                                                                                                                                  TABLE          III
                                                                                                                                                                                                                     P ERFORMANCE     OF     QUERY     DISEASES                           .
                                                                                                                   Metric                                                                   Important          disease                                                                                                                      Unimportant          disease
                                                                                                                                                           急性上呼吸道感染本                                                                                                                   態性高血壓      迫切早產                                                                       帶狀疹併合併症                                                                                                                   腦動脈阻塞
                                                                                                              Accuracy                                                          0.1783                                                                                                                                                                     0.21320.7143                                                                                                                          0.2857                                                                                                                                        0.0000
                                                                                                             MRR@20                                                             0.2683                                                                                                                                                                     0.29040.7298                                                                                                                          0.3156                                                                                                                                        0.1572
                                                                                                            Recall@20                                                           0.5705                                                                                                                                                                     0.55121.0000                                                                                                                          0.5714                                                                                                                                        0.5000
                                                                                                                      TABLE          IV                                                                                                                                                       [4]                 A.        Sankar,        Y.        Wu,        L.        Gou,        W.        Zhang,        and        H.        Yang,        “Dysat:        Deep        neural
                                                      P ERFORMANCE     OF     BASELINES     AND     OUR     MODEL                                   .                                                                                                                                                       representation         learning         on         dynamic         graphs         via         self-attention         networks,”
                                                                                                                                                                                                                                                                                                            in            Proceedings            of            the            13th            international            conference            on            web            search            and
                                   Method                                                                                            Accuracy                                                                                           MRR@k                                                                                                                                      Recall@kdata            mining,          2020,          pp.          519–527.
                                                                                                                                    k=5                                                                      k=20                                                                      k=5                                                                      k=20[5]                 F.              Yu,              Q.              Liu,              S.              Wu,              L.              Wang,              and              T.              Tan,              “A              dynamic              recurrent
                             LSTM          [18]                                                                                     0.0961                                                          0.1292                                           0.1468                                           0.1906                                           0.3671model              for              next              basket              recommendation,”              in                        Proceedings                 of                 the                 39th
                               GRU          [19]                                                                                             0.1255                                                          0.1531                                           0.1706                                           0.2091                                           0.3810International            ACM            SIGIR            conference            on            Research            and            Development            in
                      GRU4REC          [11]                                                          0.0243                                                          0.0351                                           0.0392                                           0.0550                                           0.0977Information            Retrieval,          2016,          pp.          729–732.
                         SR-GNN          [13]                                                                     0.2442                                                          0.3211                                           0.3396                                           0.4518                                           0.6354[6]                 Q.                 Liu,                 Y.                 Zeng,                 R.                 Mokhosi,                 and                 H.                 Zhang,                 “Stamp:                 short-term
                      GCE-GNN          [15]                                                          0.1920                                                          0.2823                                           0.3034                                           0.4450                                           0.6499attention/memory              priority              model              for              session-based              recommendation,”
                              Our          model                                                                                                          0.2555                                                                     0.3406                                                   0.3592                                                   0.4832                                                   0.6630in             Proceedings             of             the             24th             ACM             SIGKDD             international             conference             on
                                                                                                                                                                                                                                                                                                            knowledge            discovery            &            data            mining,          2018,          pp.          1831–1839.
                                                                                                                                                                                                                                                                                              [7]                 J.               Tang,               M.               Qu,               M.               Wang,               M.               Zhang,               J.               Yan,               and               Q.               Mei,               “Line:
                                                                                                                                                                                                                                                                                                            Large-scale       information       network       embedding,”       in                Proceedings         of         the         24th
                                                                                                  VI.              C  ONCLUSION                                                                                                                                                                             international            conference            on            world            wide            web,          2015,          pp.          1067–1077.
                                                                                                                                                                                                                                                                                              [8]                 A.             Grover             and             J.             Leskovec,             “node2vec:             Scalable             feature             learning             for
                     In           this           paper,           we           explore           hidden           relationship           in           administra-                                                                                                                                           networks,”               in                          Proceedings                   of                   the                   22nd                   ACM                   SIGKDD                   international
           tive         medical         dataset         from         two         aspect         and         formulate         a         sequence                                                                                                                                                            conference            on            Knowledge            discovery            and            data            mining,          2016.
                                                                                                                                                                                                                                                                                              [9]                 W.             L.             Hamilton,             R.             Ying,             and             J.             Leskovec,             “Inductive             representation
           prediction                 problem.                  We                 proposed                  a                 new                 model                  consists                 of                                                                                                       learning          on          large          graphs,”          in                   NIPS,          2017.
           gated                   graph                   neural                   network                   and                   soft                   attention                   mechanism.                                                                                         [10]                 P.                 Veli ˇckovi ´c,                 G.                 Cucurull,                 A.                 Casanova,                 A.                 Romero,                 P.                 Li  `o,                 and
                                                                                                                                                                                                                                                                                                            Y.          Bengio,          “Graph          attention          networks,”          in                           International             Conference             on
           Speciﬁcally,                 we                 model                 every                 disease                 to                 disease                 embedding                                                                                                                         Learning            Representations,          2018.
           with               reverse               position               embedding               and               aggregate               to               a               session                                                                                                     [11]                 B.                   Hidasi,                   A.                   Karatzoglou,                   L.                   Baltrunas,                   and                   D.                   Tikk,                   “Session-
           embedding                     by                     soft                     attention                     individually.                     In                     addition,                     we                                                                                            based         recommendations         with         recurrent         neural         networks,”                  arXiv           preprint
                                                                                                                                                                                                                                                                                                            arXiv:1511.06939,          2015.
           enhance                     disease                     embedding                     by                     considering                     global                     effects.                                                                                               [12]                 C.             Xu,             P.             Zhao,             Y.             Liu,             J.             Xu,             V.             S.             S.             S.             Sheng,             Z.             Cui,             X.             Zhou,
           We                   conduct                   several                   experiments                   on                   administrative                   medical                                                                                                                             and           H.           Xiong,           “Recurrent           convolutional           neural           network           for           sequential
           dataset                     and                     demonstrate                     the                     effectiveness                     of                     graph                     neural                                                                                            recommendation,”          in                   The            world            wide            web            conference,          2019.
                                                                                                                                                                                                                                                                                          [13]                 S.          Wu,          Y.          Tang,          Y.          Zhu,          L.          Wang,          X.          Xie,          and          T.          Tan,          “Session-based
           networks            with            case            studies.            Besides,            we            discuss            the            inﬂuence                                                                                                                                             recommendation              with              graph              neural              networks,”              in                        Proceedings                 of                 the
           of          different          setting.          The          experimental          results          show          our          method                                                                                                                                                           AAAI            conference            on            artiﬁcial            intelligence,          2019.
           outperforms              previous              methods.              We              leave              considering              gender                                                                                                                                        [14]                 Y.                 Li,                 D.                 Tarlow,                 M.                 Brockschmidt,                 and                 R.                 Zemel,                 “Gated                 graph
                                                                                                                                                                                                                                                                                                            sequence          neural          networks,”                   arXiv            preprint            arXiv:1511.05493,          2015.
           and               ages               of               patients               and               the               time               interval               to               next               predicted                                                                       [15]                 Z.         Wang,         W.         Wei,         G.         Cong,         X.-L.         Li,         X.-L.         Mao,         and         M.         Qiu,         “Global
           disease            for            future            work.                                                                                                                                                                                                                                        context       enhanced       graph       neural       networks       for       session-based       recommenda-
                                                                                                                                                                                                                                                                                                            tion,”          in                    Proceedings             of             the             43rd             international             ACM             SIGIR             conference
                                                                                                                                                                                                                                                                                                            on            research            and            development            in            information            retrieval,          2020.
                                                                                                            R EFERENCES                                                                                                                                                                   [16]                 N.              S.              Altman,              “An              introduction              to              kernel              and              nearest-neighbor              non-
                                                                                                                                                                                                                                                                                                            parametric             regression,”                       The                American                Statistician,             vol.             46,             no.             3,             pp.
                                                                                                                                                                                                                                                                                                            175–185,          1992.
               [1]                 A.                      C.-C.                      Yang.                      (2021)                      Comorbidity                      analysis                      platform.                      [Online].                                      [17]                 A.          C.-C.          Yang,          C.-K.          Peng,          H.-W.          Yien,          and          A.          L.          Goldberger,          “Infor-
                             Available:          https://dmc.nycu.edu.tw/comorbidity/index.php                                                                                                                                                                                                              mation        categorization        approach        to        literary        authorship        disputes,”                  Physica
               [2]                 B.              Perozzi,              R.              Al-Rfou,              and              S.              Skiena,              “Deepwalk:              Online              learning                                                                                   A:            Statistical            Mechanics            and            its            Applications,          2003.
                             of             social             representations,”             in                       Proceedings                of                the                20th                ACM                SIGKDD                                                       [18]                 S.           Hochreiter           and           J.           Schmidhuber,           “Long           short-term           memory,”                            Neural
                             international                    conference                    on                    Knowledge                    discovery                    and                    data                    mining,                                                                          computation,          vol.          9,          no.          8,          pp.          1735–1780,          1997.
                             2014,          pp.          701–710.                                                                                                                                                                                                                         [19]                 K.             Cho,             B.             Van             Merri ¨enboer,             D.             Bahdanau,             and             Y.             Bengio,             “On             the
               [3]                 T.          N.          Kipf          and          M.          Welling,          “Semi-supervised          classiﬁcation          with          graph                                                                                                                    properties         of         neural         machine         translation:         Encoder-decoder         approaches,”
                             convolutional         networks,”         in                  International           Conference           on           Learning           Rep-                                                                                                                                 arXiv            preprint            arXiv:1409.1259,          2014.
                             resentations            (ICLR),          2017.
                                                                                                                                                                                                                                                                       112
Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on February 18,2025 at 13:21:59 UTC from IEEE Xplore.  Restrictions apply.

