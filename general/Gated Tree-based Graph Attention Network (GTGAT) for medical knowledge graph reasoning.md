                                                                Contents lists available at ScienceDirect
                                                    Artificial Intelligence In Medicine
                                                      journal           homep               age:      www.el               sevier.com/l                    ocate/artmed
Research  paper
Gated  Tree-based  Graph  Attention  Network  (GTGAT)  for  medical
knowledge  graph  reasoning
Jingchi  Jiang       a,  Tao  Wang        a,  Boran  Wang        b,*,  Linjiang  Ma        a,  Yi  Guan     a
aDepartment of Computer Science and Technology, Harbin Institute of Technology, China
bDepartment of Computer Science and Technology, Harbin Institute of Technology, Shenzhen,  China
ARTICLE                          INFO                  ABSTRACT
Keywords:                                              Knowledge graph (KG) is a multi-relational data that has proven valuable for many tasks including decision
Medical knowledge graph                                making and semantic search. In this paper, we present GTGAT (Gated Tree-based Graph Attention), a method for
Graph attention network                                tackling the problems of transductive and inductive reasoning in generalized KGs. Based on recent advancement
Disease diagnosis                                      of graph attention network (GAT), we develop a gated tree-based method to distill valuable information in
                                                       neighborhood via hierarchical-aware and semantic-aware attention mechanism. Our approach not only ad-
                                                       dresses several key challenges of GAT but is also capable of undertaking multiple downstream tasks. Experi-
                                                       mental results have revealed that our proposed GTGAT has matched state-of-the-art approaches across
                                                       transductive benchmarks on the Cora, Citeseer, and electronic medical record networks (EMRNet). Meanwhile,
                                                       the inductive experiments on medical knowledge graphs show that GTGAT surpasses the best competing methods
                                                       for personalized disease diagnosis.
1. Introduction                                                                               Meanwhile, there is an increasing interest in generalizing convolu-
                                                                                          tions to non-Euclidean domains. Recent development of GNN in this
    A KG normally consists of nodes corresponding to entities and multi-                  direction can be divided into spectral and non-spectral approaches.
semantic edges corresponding to facts, each of which is represented by                    Spectral approaches [9] define the convolution operations in spatial
two connected entities in the form of {<subject >   < predicate >   <                     domain by the eigendecomposition of the Laplacian spectrum and
object>}. A great number of generalized KGs are large-scale, containing                   employ Fourier transform to compute eigenvectors of the Laplacian in
millions to billions of triples. With the enrichment of knowledge, re-                    the spectral domain. Since then, there have been improvements [10],
searchers face the challenges of complex inference and knowledge                          extensions [11], and approximations [12] on spectral-based GNNs by
expansion. Therefore, some domain-specific KGs are constructed to                         introducing parameterization of spectral filters with smooth co-
address particular tasks, e.g., medical semantic network (UMLS) [1],                      efficients, approximating the filters by means of Chebyshev expansion
social networks (SocNet) [2] and life sciences (Bio2RDF) [3]. KGs could                   on the graph Laplacian. Non-spectral approaches [13,14], on the other
be irregular since their nodes may have an inconsistent number of                         hand, perform convolutions directly in spatial domain, aggregating local
neighbors and are connected with different semantic edges, therefore                      information of spatially close neighbors through sampling and normal-
leveraging traditional deep neural networks to deal with arbitrary                        izing a fixed-size neighborhood of each node, or feeding all the sampled
structured graphs is considered non-trivial. To solve this problem, graph                 neighbors' feature vectors into a recurrent neural network. Recently,
neural networks (GNN) [4,5] were proposed, which adopt message                            graph attention network (GAT) [15] has obtained state-of-the-art per-
propagation mechanism to learn node representations over entire graph                     formance across several transductive and inductive benchmarks. The
until an equilibrium state is reached. These pre-training node repre-                     hidden representation of the nodes are computed by an iterative process
sentations are then used for downstream tasks, such as knowledge                          of transforming and aggregating the node representations from neigh-
completion [6], and logic reasoning [7]. To control what information is                   bors. To exploit graph topological details in GAT, an adaptive structural
transmitted between nodes, an improved GNN with gated recurrent                           fingerprint (ADSF) [16] was proposed to improve subsequent attention
units were proposed by [8].                                                               layer as well as the convergence of learning, and to mine rich structural
 *Corresponding author.
    E-mail address: wangboran@hit.edu.cn (B. Wang).
https://doi.org/10.1016/j.artmed.2022.102329
Received 12 September 2021; Received in revised form 28 April 2022; Accepted 29 May 2022

J. Jiang et al.
Fig. 1.            An illustration of information distillation by RNN on two receptive fields. The representations of nodes in the flatten graph are initialized by GAT encoder.
Through  hierarchical  transmission, the  distilled representations  of two central nodes  are generated at  the  root node of their  respective Huffman tree.
information. One of the key benefits of GATs is the attention mechanism                                              relevant, ignoring the actual relation between a node and its neighbors.
could   handle   variable-sized   neighbors   and   focus   on   the   most   relevant                               In       this       paper,       we       provide       a       hierarchical-aware       and       semantic-aware
neighbors.                                                                                                           attention             mechanism             to             improve             the             transductive             and             inductive
     For            knowledge            graph            reasoning,            knowledge            graph            completion reasoning in generalized KGs. We are the first to consider gated RNN to
[17–          19] and alignment [20–          22] are two important reasoning problems.                              gather      the      information      in      the      neighborhood      and      adopt      the      residual
Most       earlier       representation-based       approaches,       such       as       TransE       [23],         structure to fuse the original attention of GAT with the purified attention
DistMult    [24],    and    RotatE    [25],    use    a    predefined    scoring    function    to                   of RNN. Inspired by the arbitrary length of sentences in syntactic trees,
measure    the    confidence    of    given    triplet,    where    the    inputs    depend    on                    we organize all neighbors of a receptive field into a Huffman tree, and
their  entity embeddings.  These  models demonstrate  good performance                                               distill their semantic contents in a bottom-up manner. More importantly,
in            transductive            settings.            For            automatically            completing            knowledge the proposed model integrates prior statistical knowledge into Huffman
graphs, however, missing triples are assumed to mention only constants                                               Tree         and         guides         each         node         to         carry         out         information         aggregation.
already occurring in the incomplete KG. A key limitation of these models                                             Alternatively,   we   design   two   gated   units   to   avoid   the   loss   of   valuable
is that they are not applicable in inductive settings, where missing triples                                         information          over          long-distance          propagation.          Our          contributions          are
may involve constants unseen during training. This setting is especially                                             summarized as:
relevant in practive since KGs are evolving: they may be extended with
triples    describing    new    objects    or    integrated    with    external    KGs.    Moti-                       •We propose a gated tree-based graph model (GTGAT), which is the
vated by the shortcomings of existing approaches, we introduce a kind of                                                   first consideration of combining gated recursive neural network with
dynamically scalable tree structure to learn representations of both seen                                                  graph attention network for distilling the neighbor representations.
and unseen entity . Through organizing neighbors of unseen entity into a                                               •The         multi-semantic         contents         can         be         reasonably         utilized         in         our
hierarchical tree, we not only aggregate neighborhood information into                                                     approach, which is particularly beneficial in handling complex real-
the embeddings of unseen entities but also formalize the multi-semantic                                                    world knowledge graphs.
relationships          between          entities.          Although          some          researchers          [26,27] •Our  approach  shows  better  performance  than  the  previous  state-of-
proposed KG reasoning based on reinforcement learning, which can also                                                      the-art   on   the   dense   and   multi-semantic   graphs   for   both   the   trans-
be applicable to multi-relational transductive and inductive settings, the                                                 ductive and inductive reasoning.
heuristic  reward  function  in  reinforment  learning  is  difficult  to  gener-
alize for different tasks or datasets. Furthermore, reinforcement learning                                           2.                Background
requires sufficient historical experience to learn a good policy is harder
to     train     than     supervised     learning     methods.     Therefore,     our     goal     is     to               In this section, we first review the recursive neural networks and the
propose a general approach based supervised learning that is adaptable                                               compositionality           of           semantic           vectors           by           tree           structure.           Next,           we
to       multiple       datasets       (e.g.,       cite       networks       and       medical       datasets)       and compare the information loss of Huffman trees with random binary trees
multiple tasks (e.g., node classification and personalized diagnosis).                                               in the bottom-up aggregating process. Finally, we present the proposed
     Although each node representation in GAT contains the contents and                                              network structure by combining GAT with RNN shaped as Huffman tree.
structures  of  the  one-order  or  high-order  neighborhoods,  these  models                                        The detail architecture of our method is shown in Fig. 1.
only         apply        to         relatively         simple         graphs        where         the         node         content        is

J. Jiang et al.
2.1.                 Recurisve  Neural  Networks  (RNNs)                                                                                     decay with their distance from the root node in Huffman tree, such that
                                                                                                                                             wi(1   δ    di) ≤ wi+1(1   δ    di+1) if wi = wi+1 and di ≥ di+1.
      A   recursive   neural   network   [28]   is   a   kind   of   deep   neural   network                                                       Proof     of     Theorem.     Let     w         and     w      be     the     semantic     contents     of     two
created  by  applying  the  same  set  of  weights  recursively  over  a  binary                                                                                                                j              k
tree   structure.   Given   the   initial   representations   of   the   leaves,   e.g.   the                                                exchanged nodes, respectively. From Lemma 1, we may assume wj ≤wk
terminal nodes with word vectors in a syntactic tree, RNN computes the                                                                       and dj   dk = dx. The global contents of H                              and B                             can be expressed as:
representation at each internal node η as follows:                                                                                           {   C   (H      )= X    +w      1     δd     )+w        (1     δd     )
                                                                                                                                                                           j             j          k             k)                                                                                                                                                                                                                                                                                                                                                  (3)
                            T  )                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (1)  C   (B       )= X    +wj(1     δdk)+wk1     δdj
vη  = f  W        [vl,vr]
                                                                                                                                             where X =∑               n              w   (1    δ    d  ).
                                                                                                                                                                      i=1,   i∕=j,   k  i             i
where vl and vr are d-dimentional representations of left and right chil-                                                                          In order to prove that the inequality C(H                       ) ≥C(B                        ) is ture, we need to
dren of the parent η, W ∈ℝd×2d is the shared weight matrix, and f =tanh                                                                      verify w       [1    δ    (d      +  d    )] +  w       (1    δ    d   ) ≥  w[1    δ    (d           +  d    )] +  w       (1  
                                                                                                                                                           j                k        x             k               k          j                k        x             k
is a standard element-wise nonlinear function. To captain the inherent                                                                       δ    dk). According to Lemma 2, we simplify this inequality as δ    dx(wk   wj)
meaning of two children, a recursive neural tensor network (RNTN) [29]                                                                       ≥ 0. As known the given condition wj ≤ wk, Theorem is obviously true.
was further proposed with tensor-based composition function:                                                                                       The   above   theoretical   verification   illustrate   that   Huffman   tree   is   a
          ([v       ]T           [v     ]         [v     ])                                                                                  kind    of    optimal    binary    trees    that    aggregates    richer    information    from
vη  = f           l      V [1       :d ]l +W           l                                                                      (2)            terminal nodes than others'.
                vr                  vr               vr
                [1:d]            2d×2d×d                                                                                                     3.                Methodology
where      V             ∈    ℝ                 is      the      tensor      that      defines      multiple      bilinear
forms. In order to classify these representations into n classes, a simple                                                                         In   this   section,   we   describe   a   gated   RNN   algorithm,   which   distills
Softmax classifier was added on top of root node in RNNs to predict the                                                                      valuable    semantic    information    from    neighbors'    initial    representations
class       distribution:      ylabel         =       softmax(Wsvroot),      where       Ws             ∈    ℝn×d       is      the
classification matrix.                                                                                                                       pre-encoded by a single graph attentional layer. We then describe how the
      Notably, one of the main advantages of RNN and its derived models                                                                      distilled         representations         are         utilized         in          transductive         and         inductive
is  the  ability  to  learn  compositional  vector  representations  for  phrases                                                            reasoning.
and words of arbitrary semantic type and number. The root node vector                                                                        3.1.                 Gated  tree-based  RNN  for  semantic  distillation
is the aggregation of valuable information of all terminal nodes, and has
strong       abilities       in       predicting       sentiment,      learning      propositional       logic                                     A   knowledge   graph   is   a   graph   that   represents   multi-relational   data,
representation, and classifying semantic relationships between nouns in                                                                      which is expressed in the form of G =  (V,   E =  {ℰ                                         ,       ℰ,   ..,       ℰ}). Nodes V
a sentence.                                                                                                                                                                                                                             1      2         p
                                                                                                                                             and edges E correspond to entities and their relationships respectively, p
2.2.                 Optimal  skeleton Huffman  trees                                                                                        is   the   number   of   types   of   edges   (predicates),   and      ℰ                         m   denotes   a   set   of
                                                                                                                                             edges of type m ∈1, …                       , p. Let {h     }n       be the d-dimensional raw features
                                                                                                                                                                                                         i  i=1
      To review the composition functions of two children in RNNs, [30]                                                                      for each entity. We follow the basic attention mechanism of GAT algo-
                                                                                                                                             rithm to generate initial representations {h′}n                                       . This process of learning
has presented their most general function in the form of η =f(v                                                    ,   v,   R,   K),                                                                                      i   i=1
                                                                                                                  l   r                      representation hi′of node i is commonly expressed as [15]:
where     R     is     the     priori     known     semantic     relation     and     K     is     background                                                                              )|                   )
knowledge.        Both        of        them       are        used        to        construct       the        tree        structure.        A   i←                      ATTENTION                             { hi,hjj  ∈N       (i)}
Similarly, the “        closeness”             between the central node and its neighbors in                                                 A   i←                      NORMALIZATION                                 (Ai)
a  local  receptive  field  of graph  should be  determined by  both their  se-                                                              h ′←                      TRANSFORM                                 h,h,A)                                                                                                                                                                                                                                                                                                                                                                                                                                                             (4)
mantic contents and priori knowledge, such as the structural properties,                                                                       i                                i    N    (i)     i
statistical     confidence,     and     expert     experience.     Taking     all     neighboring                                            where             ATTENTION            function            is             a            single-layer             feedforward            neural
nodes   with   respect   to   a   central   node   as   the   leaves   of   a   tree,   their   se-                                          network parametrized by a weight vector W ∈ℝd×d, NORMALIZATION
mantic   representations   can   be   aggregated   hierarchically   into   the   root                                                        is   a   normalized   function,   TRANSFORM   is   to   merge   the   neighborhood
vector by RNN.                                                                                                                               features            with            the            central            node's            features            by            an            attention            linear
      Let       W  =       (w1,   w2,       …                       ,   wn)      be      a      fixed       set      of       non-negative      semantic combination.
contents corresponding to the n terminal nodes of a rooted tree T                            and di                                                Inspired by the success of the gated recurrent neural network [32],
the length of the path between a terminal node vi and the root node vroot,                                                                   we      introduce      a      recursive     neural      network     based      on     two      gated      units,
we define the global content of T                             as the sum of wi with a linear distance                                        namely “        reset             gate”             and “        update             gate”        , to further purify partial semantic
decay of the n terminal nodes: C(T       )=∑                                 n    wi(1  δ    di), where 0   <           δ     <                                             ′n
                                                                             i=1                                                             contents from {hi               }i=1. Intuitively, the gated mechanism decides what to
1/depth(T       )is the decay factor.                                                                                                        preserve when combining the children's features. At each recursion layer
Theorem. A.                           Huffman tree H                             and a binary tree B                        . C(H                       ) ≥ C(B                        ) are ℓ, the hierarchical gated units can be formalized as:
defined if the following conditions are satisfied.                                                                                           r(ℓ  )=σ      (W    [v  ,v   ]);    z(ℓ)=σ      (W    [v  ,v   ])
                                                                                                                                                      (ℓ )      r   l    r     [                  z   l    r
                                                                                                                                                    ̃                              (ℓ)*
      (i)            H                             and B                            have the same set of terminal nodes with WH = WB =              h      = tanh      {W     ̃h r      (v l,v  r)]}                                                                       (5)
                                                                                                                                                   (ℓ)                 (ℓ ))                     (ℓ)*̃(ℓ   )
            (w1 ≤  w2…         ≤  wn);                                                                                                           y      = 1     z             *[vl,vr]+z              h
    (ii)        B                             is obtained by exchanging any one pair of terminal nodes (vj
            and vk) of the Huffman tree H                       .                                                                            where vl and vr are d-dimentional representations of left and right chil-
                                                                                                                                             dren     in ℓ       layer;     Wr,     Wz,     and     W̃∈ℝ2d×2d                  are     the     shared     parameter
      To prove Theorem, we first state two lemmas which are well-known                                                                                                                                     h
[31] in the case of t-ary Huffman trees. The proof is omitted due to its                                                                     matrices for every layer. Update Eq. (1) gives the representation at each (ℓ) (ℓ)
similarity with the known case.                                                                                                              internal   node η             as   vn η    =   f(Wy          ).   Note   that   the   representations   of   all n
                                                                                                                                             terminal nodes {vi}i=1                    in Huffman trees are initialized to {hi′}i=1.
Lemma 1.                           There exists a Huffman tree for a given set of weights W =
(w1 ≤ w2…         ≤ wn) such that wi ≤ wi+1 implies di ≥ di+1.
Lemma 2.                           Clearly, the semantic contents of the terminal nodes should

J. Jiang et al.
Fig.   2.            Model   architecture   used   in   knowledge   graph   reasoning,   including   the   transductive   and   inductive   tasks.   The   red   and   green   work   flows   implements   node
classification and graph classification, respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of
this  article.)
3.2.                 GTGAT  for  transductive  and inductive  reasoning                                                           representation at the root node could be regarded as the aggregation of
                                                                                                                                  the     valuable     partial     information     of     neighborhoods     in     a     one-order     or
     Next, we will follow the basic structure of GAT algorithm and discuss                                                        high-order receptive field.
the advantages of our algorithms for distilling partial semantic contents                                                               Here, we will describe how these distilled partial semantics are used
in a neighborhood and fine-tuning the original attention coefficients of                                                          to generate fine-grained attentions. Firstly, we evaluate the similarity of (⃦        )
GAT,      respectively.      The       detailed      architecture      of      GTGAT      is      shown      in                   distilled     representations,        S             =     aT   Wvi        ⃦Wvj          ,    where     vi       and     vj
                                                                                                                                                                                   ij                  root⃦       root                    root             root
Fig. 2.                                                                                                                           are the root representations of two gated trees corresponding to node i
     Through analyzing the attention mechanism of GAT, we found that
each    scalar    attention    coefficient      A            i,j acts    on    global    features    hj of    the                 and   j.   We   denote   the   attention   matrix A                            of   GAT   as      “        global   attention”        ,
corresponding      neighbor      vj.      However,      in      multi-semantic      graphs,      it      is                       while   the   normalized   attention   matrix S         = NORMALIZATION(S       )fo-
necessary to decompose the global features of each neighor and allocate                                                           cuses on partial semantics in our method is denoted as “        partial attention”        .
fine-grained     attentions     to     their     partial     semantics.     In     GTGAT,     the     se-                         Secondly,    we    employ    the    partial    attention    matrix S                               to    fine-tune    the
mantic  contents  of  two  children  nodes  are  merged  and  preserved  into                                                     global   attention   matrix A                      :        F         =    βS        + (1  β )A                      ,   where   β          adjusts   the
their                    parent                    node.                    As                    the                    bottom-up                    distilling                    process,                    the importance  of  two  attention  matrices  before  combining  them.  Finally,

J. Jiang et al.
we perform message passing to update the representations of each node                                                                                               Table 1
as:                                                                                                                                                                 Statistics of the graph-structured datasets used in our experiments.
{              }           (∑                            )                                                                                                                                                    Cora                                                                                          Citeseer                                                                                      EMRNet                                                                                      MKG
     ′(t+1 )                                       ′(t)
   h  i             =σ                F    ijWh      i       .                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (6) # Nodes                                                                                                                          2708                                                                                 3327                                                                                                                               4064                                                                                                  1100 graphs
                               j∈N   i                                                                                                                                  # Edges                                                                                                                             5429                                                                                 4732                                                                                                             19,581                                                                                                                                                                                                                                      –
       On the other hand, when all nodes in a graph G                        are organized into a                                                                       # Features                                                                                                    1433                                                                                 3703                                                                                                                               2560                                                                                                  256
                                                                                         Groot                                                                          # Classes                                                                                                                                                      7                                                                                                                     6                                                                                                                                                                    6                                                                                                   4
tree,     the     distilled     root     representations     v                                   can     be     considered     as     the                               # Training                                                                                                               140                                                                                             120                                                                                                                                           200                                                                                                  900
abstraction  of  the  entire  graph.  Following  the  classification  method  of                                                                                        # Validation                                                                                             500                                                                                             500                                                                                                                                           300                                                                                                                                                                                                                                      –
RNN model, we feed directly vGroot                                       into a Softmax classifier for inductive                                                        # Testing                                                                                                               1000                                                                                 1000                                                                                                                               1000                                                                                                  200
reasoning (e.g., several graph classification tasks), which is denoted as )
ylabel     =  softmax WsxGroot                     . In brief, GTGAT allows one tree per receptive                                                                  corresponds to a single-relational citation link between two documents.
field to be used for updating the representations of each node and one                                                                                              In EMRNet, each node represents one electronic medical record (EMR)
tree     per     graph     for     extracting     the     features     of     an     entire     graph.     In     the                                               corresponding  to  a  patient;  the  raw  features  of  each  EMR  are  bags-of-
former       case,       we       employ       the       node       representations       to       achieve       trans-                                             words  representations.  Each  EMR  node  has  a  class  label  refered  to  the
ductive  node  classification,  while  the  graph-structured  representations                                                                                       patient's department. If the number of  the same identical symptoms in
are used for inductive graph classification in the latter case.                                                                                                     two patients exceeds the threshold, there is an edge between their EMRs.
                                                                                                                                                                    We       closely      follow       [35]       to       randomly       split       the       dataset      into       training,
3.3.                 Huffman  tree  based  on  priori  knowledge                                                                                                    validation, and testing.
                                                                                                                                                                           To       verify        the        performance       of        comparative       methods        on        EMRNet
       As      mentioned      in      Section      2.2,      in      order      to      avoid      information      loss                                            dataset,        we        reproduce        the        strong        baselines        and        state-of-the-art        ap-
caused   by   long-distance   propagation,   the   Huffman   tree   is   constructed                                                                                proaches  with  reference  to  the  original  studies,  including  the  iterative
based on the principle that more important neighbor nodes (which are                                                                                                classification  algorithm  (ICA)  [36],  the  semi-supervised  graph  embed-
more similar to the central node) are placed closer to the root of the tree.                                                                                        ding (Planetoid) [35], the graph convolutional network (GCN) [9], the
Therefore, we could adopt various priori knowledge (e.g., the number of                                                                                             mixture  model  networks  (MoNet)  [37],  the  gated  graph  convolutional
common neighbors or the structural relevance between nearby nodes in                                                                                                network           (GGCN)           [8],           the           relational           graph           convolutional           network
graph, and the co-occurrence frequency or the semantic similarity of two                                                                                            (RGCN) [38], the heterogeneous graph transformer (HGT) [39], and the
entities in external knowledge bases) to evaluate the correlation score wi,                                                                                         single-head           graph          attention          network          (GAT),           the          four-heads          graph
j between the center node vi and its neighbor node vj. Meanwhile, these                                                                                             attention network (GAT*).
scores are considered as weights of terminal nodes to build  a Huffman
tree.                                                                                                                                                               4.1.2.                 Inductive  learning
                                                                                                                                                                           We          adopt          a          multi-relational          medical          knowledge          graphs          (MKG)
3.4.                 Loss  function                                                                                                                                 dataset  [40],  consisting  of  3  types  of  medical  entities  (namely  disease,
                                                                                                                                                                    symptom, test) and 13 semantic relationships between them. Each MKG
       For    transductive    learning,    each    tree    corresponds    to    a    node    in    the                                                              corresponds to a patient. Given the symptoms and test results of a spe-
graph. In node classification task, we directly feed the root representa-                                                                                           cific patient, all potential diseases that cause these clinical evidences are
tion of each tree into a softmax layer to output a confidence distribution                                                                                          mined       by       our       medical       knowledge       bases.       Through       connecting       these
of  node  types.  Then  we  define  cross  entropy  loss  for  all  node  of  KG  as                                                                                medical    entities    by    semantic    relationships,    a    personalized    knowledge
follows:                                                                                                                                                            graph can be build. The MKG dataset contains 900 graphs (patients) for
                n    (           C                                                                                                                                  training and 200 for tesing. Each medical entity in MKGs is a 256-dimen-
L    =1       ∑            ∑          y    logSoftmax              v          ))                                                                   (7)              sional  embedding  initialized  by  Bert  [41].  For  each  graph,  there  are  4
         n                              i,j                      j     root    ,i                                                                                   labels    refering    to    different    ICD-10    diagnostic    codes    (namely    J15.900,
               i=1             j=0
                                                                                                                                                                    J18.900, J44.900, J45.900). By claasifying these MKGs, the personalized
where n is the number of nodes in the knowledge graph, C is the number                                                                                              diagnosis task could be accomplished. An overview of the experimental
of node types, y(i,   j) is the ground-truth label of the i-th node on the j-th                                                                                     datasets is given in Table 1.
catagory,  v(root,   i)  is  the  distilled  root representations,  Softmaxj(v(root,
i)) represents the predicted probability that the i-th node belongs to the j-                                                                                       4.1.3.                 Transductive  and inductive
th    category.    For    inductive    diagnosis    task,    each    tree    corresponds    to    a                                                                        For the inductive learning, we compare our approaches against eight
patient, which is independent with each other. The definition of the loss                                                                                           text  classification   models  and  three  graph  models,  including  Bert  (the
function is consistent with the node classification, except that n and C is                                                                                         bidirectional encoder representations from transformers) [41], TextCNN
the  number  of  patients  and  disease  types,  respectively,  v(root,   i)  repre-                                                                                (taking convolutional neural networks for sentence classification) [42],
sents the abstraction of i-th patient.                                                                                                                              Bert-CNN (combining Bert encoder with convolutional neural networks)
                                                                                                                                                                    [9],   RCNN    (recurrent   convolutional   neural    networks)   [43],   the   gated
4.                 Experiments                                                                                                                                      graph      convolutional      network      (GGCN)      [8],      the      relational      graph      con-
                                                                                                                                                                    volutional network (RGCN) [38], the heterogeneous graph transformer
       In        this        section,        we        report        experimental        results        of        the        proposed                               (HGT)       [39],       DPCNN      (deep       pyramid       convolutional      neural       networks)
method    and    state-of-the-art    algorithms    using    graph-based    datasets    for                                                                          [44], and GCN, GAT, GAT* (as mentioned in transductive learning task).
transductive and inductive reasoning problems, respectively.                                                                                                        These models have demonstrated strong classification capabilities based
                                                                                                                                                                    on entities, sentences and graphs.
4.1.                 Datasets  and  comparative  methods
                                                                                                                                                                    4.2.                 Experimental  setup
4.1.1.                 Transductive  learning
       We        utilize        three        generalized        knowledge        graphs        for        transductive                                                     For    transductive    learning,    each    tree    corresponds    to    a    node    in    the
classification,   two   of   which   are   citation   networks   (Cora   and   Citeseer)                                                                            graph.      We      use      the      root      representations      of      all      trees      to      calculate      the
[33] and one is an electronic medical record network (EMRNet) [34]. In                                                                                              attention          coefficients          between          nodes          and          combine          them          with          the
citation      networks,      each      node      denotes      one      document      and      each      edge                                                        attention       coefficients       between       nodes       generated       in       the       original       GAT

J. Jiang et al.
                                                                                                                                  Fig. 3.            Schematic  of unseen entity   “       type  2 diabetes”             in inductive  setting.
                                  Table 3                                                                                                                                                                                                             Table 2
                                  Experimental   results   in   terms   of   micro-averaged   accu-                                                                                                                                                   Experimental   results   on   the   Cora,   Citeseer   and   EMRNet   test   sets.   The    micro-
                                  racy on the MKG test sets.                                                                                                                                                                                          averaged    accuracy   serves    as   the   evaluation   standard.   The    reproduced    results
                                       Method                                                                                                                                                                                                                                                                                                                                                           MKG of   comparative  methods   on   three   datasets  are   showed.  The   best   scores   are  in
                                                                                                                                                                                                                                                      bold.
                                       LR                                                                                                                                                                                                                                                                                                                                                                                                           52.3 ± 0.2 % Method                                                                                                                                                         Cora                                                                                                                                                                              Citeseer                                                                                                                                              EMRNet
                                       SVM                                                                                                                                                                                                                                                                                                                                                                                        55.5 ± 0.2 %
                                       Bert [41]                                                                                                                                                                                                                                                                                                                                            58.7 ± 0.7 % MLP                                                                                                                                                                                       55.1 ± 0.2 %                                                                                           46.5 ± 0.3 %                                                                                           60.1 ± 0.2 %
                                       TextCNN [42]                                                                                                                                                                                                                                                                                            69.1 ± 0.3 % ICA [36]                                                                                                                                             75.1 ± 0.5 %                                                                                           69.1 ± 0.2 %                                                                                           58.2 ± 0.4 %
                                       Bert-CNN [9]                                                                                                                                                                                                                                                                                                    68.2 ± 0.6 % Planetoid [35]                                                                                      75.7 ± 0.3 %                                                                                           64.7 ± 0.5 %                                                                                           62.3 ± 0.5 %
                                       RCNN [43]                                                                                                                                                                                                                                                                                                                        62.8 ± 0.5 % GCN [9]                                                                                                                                                 81.5 ± 0.4 %                                                                                           70.3 ± 0.6 %                                                                                           66.5 ± 0.2 %
                                       GGCN [8]                                                                                                                                                                                                                                                                                                                                    61.4 ± 0.2 % MoNet [37]                                                                                                                81.7 ± 0.5 %                                                                                           55.9± 0.3 %                                                                                                 62.8 ± 0.5 %
                                       RGCN [38]                                                                                                                                                                                                                                                                                                                         58.4 ± 0.4 % GGCN [8]                                                                                                                                  57.5 ± 0.2 %                                                                                           43.5± 0.5 %                                                                                                 58.4 ± 0.6 %
                                       HGT [39]                                                                                                                                                                                                                                                                                                                                       69.8 ± 0.4 % RGCN [38]                                                                                                                      67.1 ± 0.4 %                                                                                           46.5± 0.4 %                                                                                                 49.6 ± 0.3 %
                                       DPCNN [44]                                                                                                                                                                                                                                                                                                           66.5 ± 0.6 % HGT [39]                                                                                                                                     83.1 ± 0.4 %                                                                                           66.9± 0.6 %                                                                                                 67.2 ± 0.4 %
                                       GCN [9]                                                                                                                                                                                                                                                                                                                                                   63.1 ± 0.4 % GAT [15]                                                                                                                                      82.5 ± 0.6 %                                                                                           67.5 ± 0.8 %                                                                                           66.8 ± 0.6 %
                                       GAT [15]                                                                                                                                                                                                                                                                                                                                         70.3 ± 0.6 % GAT*                                                                                                                                                                             83.7 ± 0.7 %                                                                                           69.7 ± 0.6 %                                                                                      68.1 ± 0.6 %
                                       GAT*                                                                                                                                                                                                                                                                                                                                                                               73.9 ± 0.5 % GTGAT                                                                                                                                                          85.1 ± 0.6 %                                                                                      68.5 ± 0.8 %                                                                                           68.6 ± 0.6 %
                                       GTGAT                                                                                                                                                                                                                                                                                                                                                             75.0 ± 0.6 %
encoding    layer.    In    the    inductive    learning,    each    tree    corresponds    to    a                                                                                                                                                   4.3.                 Results
subgraph independent of each other. Therefore, we do not use the tree's                                                                                                                                                                                          Results are summarized in Tables 2 and 3. For the transductive tasks,
root      representation      to      calculate     the     subgraphs'      attention     matrix.      We                                                                                                                                             we reproduce the effects of all the benchmark models on Cora, Citeseet
utilize a single-head attention layer to generate the initial representation                                                                                                                                                                          and   our   EMRNet   dataset.   The   mean   classification   accuracy   of   GTGAT
of       each       node      in       GAT      encoding       layer,      and      optimize       the      single-head                                                                                                                               and reproduced models on the test nodes over 10 runs are reported. As
attention   matrix    in   fine-tuning   layer.    To   fairly   evaluate    the   effects   of                                                                                                                                                       can be seen in Table 2, our proposed GTGAT reaches up to 85.1 % and
all     methods,     the     16-dimensional     node     representations     are     commonly                                                                                                                                                         68.5    %   accuracy    scores   on    Cora   and    EMRNet   datasets,   which   signifi-
employed.  For  transductive  learning,  the  weights  of  terminal  nodes  in                                                                                                                                                                        cantly outperforms the strong baselines and state-of-the-art approaches.
Huffman   tree   are   allocated   to   neighbor   nodes   based   on   the   structural                                                                                                                                                              It demonstrates that the distillation mechanism can effectively improve
priori  knowledge  with  respect  to  the  central  node  vi be  W =   {ω                                                                                                                                         i1,ω        i2,                     GAT by fine-tuning their attention coefficients, even better than GAT*.
…                       ,ωiq}, where ω               iq is the number of common neighbors between vi and its                                                                                                                                          Compared   with   GAT*,   GTGAT   can   converge   faster   and   achieve   better
q-th neighbor. Here we emphasize on the importance of detailed graph                                                                                                                                                                                  results          on         dense         graphs         with         fewer         parameters         (see         Section          4.5).
structures   in  determining   node  similarities.   Using  these   structural   sta-                                                                                                                                                                 Although our approach does not achieve the best results on Citeseer, it is
tistics as the weights of the terminal nodes, we can iteratively construct a                                                                                                                                                                          consistently     superior     to     GAT.     Considering     that     our     method     adopts     a
Huffman tree and ensure that neighbors more similar to the central node                                                                                                                                                                               single-head   attention   in   encoding   layer,   the   optimization   on   GAT   can
are closer to the root node, providing direct effects on the representation                                                                                                                                                                           still validate the effectiveness of fine-tuning strategies.
of vi. For inductive learning, the terminal nodes of Huffman tree consist
of symptoms, test results, and all potential diseases of a specific patient.                                                                                                                                                                          4.3.1.                 Transductive
Schematic    of    unseen    entity         “        type    2    diabetes”                 is    shown    in    Fig.    3.    We
adopt  the  frequency  of  these  entities  in  an  external  EMR  repository  as
weights to construct Huffman tree. It implies that the rarer the clinical                                                                                                                                                                             4.3.2.                 Inductive
evidence,         the         more         important         it         is         to         the         diagnsis.         Following         the                                                                                                                For      the     inductive      tasks,     text     classification      models     take      the     given
experimental setup of [29], we feed the root representation of Huffman                                                                                                                                                                                symptom           and           test           entities           of           each           patient           as           inputs,           while           three
tree into a Softmax activation for graph classification. In the case of only                                                                                                                                                                          comparative graph models perform the graph classification based on the
one    neighbor   in   the   receptive   field,   we    duplicate   the   neighbor   repre-                                                                                                                                                           generated MKGs. More specifically, we are able to compare with GCN,
sentation       and       construct       a       Huffman       tree       with       a       depth       of       1.       In       our                                                                                                              GAT, and GAT* by modifying their architectures where all node repre-
experiment setup, we choose L2 regularization with λ  = 5 × 1e   4 and                                                                                                                                                                                sentations   are   fed   into   an   average   pooling   layer   for   outputing   graph-
the   dropout   with   p =    0.6   in   GAT   encoder.   Our   models   are   initialized                                                                                                                                                            level   class   label.   From   Table   3,   our   results   on   multi-relational   knowl-
using Glorot initialization and trained to minimize cross-entropy on the                                                                                                                                                                              edge graphs successfully match the state-of-the-art performance. GTGAT
training     nodes     and     graphs     using     the     Adam     optimizer     with     an     initial                                                                                                                                            improves upon HGT, GAT and GAT* by a margin of 5.2 %, 4.7 % and 1.1
learning rate of 0.01 for all datasets.

J. Jiang et al.
Table 4                                                                                                                          layer   can   accelerate   the   aggregation   of   valuable   information.   The   ac-
Compare GTGAT with the non-gated TGAT on four graph-structured test sets.                                                        curacy  of  GAT  fluctuates  through  iterations,  while  our  approaches  are
  Method                                              Cora                                                                                                                                       Citeseer                                                                                                      EMRNet                                                                                                    MKG more stable and converges to a higher accuracy. In addition, TGAT with
  TGAT                                                                84.8 ± 0.4 %                                                   67.7 ± 0.5 %                                                  68.0 ± 0.5 %                                                   71.0 ± 0.5 % lower loss and the same verification accuracy as GTGAT does not obtain
  GTGAT                                               85.1 ± 0.6 %                                              68.5 ± 0.8 %                                              68.6 ± 0.6 %                                              75.0 ± 0.6 %  higher      test      accuracy.      It      indicates      that      a      slight      overfitting      may      have
                                                                                                                                 occurred,    while    the    forgetting    function    of    gated    units    in    GTGAT    can
                                                                                                                                 effectively prevent it.
%, respectively, illustrating that the partial attention mechanism to node
representations may be beneficial, and that the gated tree-based RNN is
more suitable for aggregating multi-semantic contents. In addition, we                                                           4.5.                 Error  analysis
notice  that  our  model  improves  by  5.9  %  w.r.t.  the  best  text  classifier
(TextCNN).   It   successfully   justifies   the   importance   of   structural   infor-                                               By analyzing the cause of the misclassified nodes, we found that most
mation,     and     that     greater     reasoning     power     for     personalized     diagnosis                              of  misclassified nodes  have  only  a  small  number of  neighbors,  or  even
task can be achieved under the guidance of medical knowledge.                                                                    one   neighbor   in   the   receptive   field.   This   will   make   it   difficult   for   our
                                                                                                                                 gated    tree-based    RNN    to    purify    more    semantic    information    from    the
4.4.                 Ablation  study                                                                                             receptive fields of small-degree nodes, failing to reflect the advantages of
                                                                                                                                 the hierarchical aggregation and semantic compositionality. To further
     Our method has 2 variations, including TGAT with a non-gated RNN                                                            prove our  conjecture and explore the  effect of  graph properties on our
and     GTGAT.     For     both     cases,     we     use     the     same     experimental     setup     to                     model,    we    plot    the    degree    distribution    of    three    transductive    datasets
compare them on four graph-structured datasets. As shown in Table 4,                                                             (Cora, Citeseer, EMRNet) in a log-log space. From Fig. 4, we can observe
with the addition of two gated units, the effect of GTGAT is superior to                                                         that  three  distributions  approximately  follow  the  power  laws.  In  com-
that        of        TGAT        on        all        datasets.        Especially        in        multi-relational        graphs parison with the 1-th and 3-th regions of the three panels, the Citeseer
(MKG), the memory mechanism in the hierarchical aggregating process                                                              and EMRNet have more samlle-degree nodes than Cora, while there are
can effectively distill valuable information from neighbors with various                                                         more supernodes in EMRNet than Citeseer, as presented in 4-th region.
semantic associations.                                                                                                           Therefore, the denseness of the three graphs can be summarized as: Cora
                                                                                                                                 >           EMRNet >           Citeseer, which is consistent with the performance of our
4.4.1.                 Transductive  &                 inductive                                                                 model.     Synthesizing     the     above     analytical     results     of     transductive     and
     In    order    to    present    the    training    process    of    GAT    and    the    proposed                           inductive  reasoning,  our  approaches  have  a  range  of  desirable  proper-
methods  (TGAT and  GTGAT) on  our  dataset (EMRNet),  we plot  evolu-                                                           ties, such as being applicable with dense graphs, and capable of distilling
tion of the validation accuracy and loss function in Fig. 4. The objective                                                       valuable  information   from  multi-semantic   structure.   Superior  to   other
value   of   our   approaches   converges   faster   attributed   to   the   distillation                                        graph models, GTGAT provides a powerful platform to handle complex
                                      Fig.  4.            Convergence of  the validation accuracy  and loss  of the  proposed methods  and  GAT method on  EMRNet.
                                                         Fig. 5.            Degree distributions  of the  three graph-structured dataset  in log-log coordinates.

J. Jiang et al.
real-world data (Fig. 5).                                                                                                     [15]             P. Veliˇckovi´c G. Cucurull A. Casanova A. Romero P. Lio Y. Bengio , Graph attention
                                                                                                                                      networks, arXiv preprint arXiv:1710.10903.
                                                                                                                              [16]             Ma J, Tang W, Zhu J, Mei Q. A flexible generative framework for graph-based semi-
4.6.                 Conclusion                                                                                                       supervised learning. In: Advances in neural information processing systems; 2019.
                                                                                                                                      p. 3281–        90.
     We   propose   a   novel   graph   neural   network   model   and   information                                          [17]             Zhang C, Yao  H, Huang C, Jiang M, Li  Z, Chawla NV. Few-shot knowledge graph
distillation    approach    to    improve    the    performance    of    GAT.    The    graph                                         completion. In: Proceedings of the AAAI conference on artificial intelligence. Vol.
                                                                                                                                      34; 2020. p. 3041–        8.
model           combines           attention           mechanism           and           gated           recursive           neural [18]             Zhang Z, Zhuang F, Zhu H, Shi Z, Xiong H, He Q. Relational graph neural network
network to deal with transductive and inductive learning and achieves                                                                 with hierarchical attention for knowledge graph completion. In: Proceedings of the
better   reasoning   results   than   the   state-of-the-art   on   dense   graphs   and                                              AAAI conference on artificial intelligence. Vol. 34; 2020. p. 9612–        9. arXiv:
                                                                                                                                      1806.01261.
multi-semantic                     knowledge                     graphs.                     The                     information                     distillation [19]             Meilicke  C, Chekol MW, Ruffinelli D, Stuckenschmidt H. Anytime bottom-up rule
approach under the guidance of structural and statistical priori knowl-                                                               learning for knowledge graph completion. In: IJCAI; 2019. p. 3137–        43.
edge generates comprehensive node representations by focusing partial                                                         [20]             Sun Z, Wang C, Hu W, Chen M, Dai J, Zhang W, Qu Y. Knowledge graph alignment
                                                                                                                                      network with gated multi-hop neighborhood aggregation. In: Proceedings of the
semantic contents of neighborhoods and aggregating them into the root                                                                 AAAI conference on artificial intelligence. Vol. 34; 2020. p. 222–        9.
representation   of   a   Huffman   tree.   Experiments   on   open   datasets   have                                         [21]             Xu K, Song L, Feng  Y, Song Y, Yu D. Coordinated reasoning for  cross-lingual
shown          that          our          proposed          methods          have          successfully          surpassed          or knowledge graph alignment. In: Proceedings of the AAAI conference on artificial
                                                                                                                                      intelligence. Vol. 34; 2020. p. 9354–        61.
matched     state-of-the-art     performance     across     three     node     classification                                 [22]             Tang X, Zhang J, Chen B, Yang Y, Chen H, Li C. Bert-int: a bert-based interaction
benchmarks and one well-established graph classification task.                                                                        model for  knowledge graph  alignment. In: Proceedings of the twenty-ninth
                                                                                                                                      international conference on international joint conferences on artificial
Declaration  of competing  interest                                                                                                   intelligence; 2021. p. 3174–        80.
                                                                                                                              [23]             A. Bordes N. Usunier A. Garcia-Duran J. Weston O.  Yakhnenko , Translating
                                                                                                                                      embeddings for  modeling multi-relational data, Adv Neural  Inf Proces Syst 26.
     We declare that we have no financial and personal relationships with                                                     [24]             B. Yang W.-T. Yih X. He  J. Gao L. Deng , Embedding entities and relations for
other     people     or     organizations     that     can     inappropriately     influence     our                                  learning and inference in  knowledge bases, arXiv preprint  arXiv:1412.6575.
                                                                                                                              [25]             Z. Sun Z.-H. Deng J.-Y. Nie J. Tang , Rotate: knowledge graph embedding by
work. There is no professional or other personal interest of any nature or                                                            relational rotation in complex space,  arXiv preprint  arXiv:1902.10197.
kind     in     any     product,     service,     and     company.     The     manuscript     entitled                        [26]             Wan G, Pan S, Gong C, Zhou C, Haffari G. Reasoning like  human: Hierarchical
“        Gated         Tree-based         Graph         Attention         Network         (GTGAT)         for         Medical         reinforcement learning for knowledge graph reasoning. In: Proceedings of the
                                                                                                                                      twenty-ninth international conference on international joint conferences on
Knowledge Graph Reasoning.”                                                                                                           artificial intelligence; 2021. p. 1926–        32.
                                                                                                                              [27]             W. Xiong T. Hoang W. Y. Wang , Deeppath:  a reinforcement learning method for
Acknowledgment                                                                                                                        knowledge graph reasoning, arXiv preprint  arXiv:1707.06690.
                                                                                                                              [28]             Socher  R, Huval  B, Manning CD, Ng AY. Semantic compositionality through
                                                                                                                                      recursive matrix-vector spaces. In: Proceedings of  the 2012 joint conference on
     This  study  was  supported  in  part  by  a  grant  from  National  Natural                                                     empirical methods  in natural language  processing and computational natural
Science      of      China      [62006063]      and      the      Heilongjiang      Provincial      Post-                             language learning;  2012. p. 1201.
                                                                                                                              [29]             Socher  R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C. Recursive
doctoral Science Foundation (CN) [LBH-Z20015].                                                                                        deep models for  semantic compositionality over a sentiment  treebank. In:
                                                                                                                                      Proceedings of the 2013 conference on empirical methods in  natural language
References                                                                                                                            processing; 2013. p. 1631–        42.
                                                                                                                              [30]             Mitchell J, Lapata M. Composition in distributional models of semantics. Cognit Sci
 [1]             Olivier B. The unified medical language system (umls): integrating biomedical                                        2010;34(8):1388–        429.
       terminology. Nucleic Acids Res  2004;(suppl_1):267–        70.                                                         [31]             Huffman DA. A method for the construction of  minimum-redundancy codes. Proc
 [2]             Silva RRC, Leal BC, Brito FT, Vidal VMP, Machado JC. A  differentially private                                       IRE 1952;40(9):1098–        101.
       approach for querying rdf data of  social networks. In: The 21st international                                         [32]             K. Cho B. Van Merri¨enboer C. Gulcehre D.  Bahdanau F. Bougares H. Schwenk Y.
       database engineering  &              applications symposium; 2017.                                                             Bengio , Learning phrase representations using rnn encoder-decoder for statistical
 [3]             Callahan A, Cruz-Toledo J, Ansell P, Dumontier M. Bio2rdf release 2: improved                                        machine translation, arXiv preprint arXiv:1406.1078.
       coverage, interoperability and provenance of life science linked data. In: Extended                                    [33]             Sen P, Namata G, Bilgic M, Getoor  L, Galligher B, Eliassi-Rad T. Collective
       semantic web conference; 2013.                                                                                                 classification in network data. AI Mag 2008;29(3). 93-93.
 [4]             Scarselli F, Gori M, Tsoi AC, Hagenbuchner M, Monfardini G. The graph neural                                 [34]             J. Jiang C. Zhao Y. Guan Q. Yu , Learning and inference in knowledge-based
       network model. IEEE Trans Neural Netw 2009;20(1):61.                                                                           probabilistic model for medical diagnosis, Knowl-Based  Syst.
 [5]             P. W. Battaglia J. B. Hamrick V. Bapst A. Sanchez-Gonzalez V. Zambaldi M.                                    [35]             Yang Z, Cohen W, Salakhudinov R. Revisiting semi-supervised learning with graph
       Malinowski A. Tacchetti D. Raposo A. Santoro R. Faulkner , et al., Relational                                                  embeddings. In: International conference on machine learning, PMLR;  2016.
       inductive biases,  deep learning, and graph networks, arXiv preprint arXiv:                                                    p. 40–        8. arXiv:1707.06690.
       1806.01261.                                                                                                            [36]             Getoor L. Link-based classification. In: Advanced methods for knowledge discovery
 [6]             Ruffinelli D, Broscheit S, Gemulla R. You can teach an old dog  new tricks! on                                       from complex data.  Springer; 2005. p. 189–        207.
       training knowledge graph embeddings. In: International conference on learning                                          [37]             Monti F, Boscaini D, Masci J, Rodol`a E, Svoboda J, Bronstein MM. Geometric deep
       representations; 2019.                                                                                                         learning on graphs and manifolds using mixture model cnns. In: 2017 IEEE
 [7]             Y.  Zhang X. Chen Y. Yang A. Ramamurthy B. Li  Y. Qi L. Song , Efficient                                             conference on computer vision and pattern recognition (CVPR); 2017.
       probabilistic logic reasoning with graph neural networks.                                                              [38]             M. Schlichtkrull T.  N. Kipf P.  Bloem R. V. Berg M. Welling , Modeling relational
 [8]             Y. Li D. Tarlow M. Brockschmidt R. Zemel , Gated graph sequence neural networks,                                     data with graph convolutional networks, Springer, Cham.
       arXiv preprint arXiv:1511.05493.                                                                                       [39]             Hu Z, Dong Y, Wang K, Sun Y. Heterogeneous graph transformer. In: Proceedings of
 [9]             J. Bruna W. Zaremba A. Szlam Y. LeCun , Spectral networks and locally connected                                      the web conference 2020; 2020. p. 2704–        10.
       networks on graphs, arXiv preprint  arXiv:1312.6203.                                                                   [40]             J. Jiang H. Wang  J. Xie X. Guo Y. Guan Q. Yu , Medical knowledge embedding
[10]             T.  N. Kipf M. Welling , Semi-supervised classification with graph convolutional                                     based on recursive neural network for multi-disease diagnosis, Artif Intell Med 103.
       networks,  arXiv preprint  arXiv:1609.02907.                                                                           [41]             J. Devlin M.-W. Chang K. Lee K. Toutanova , Bert:  pre-training of  deep
[11]             M. Henaff J. Bruna Y. LeCun , Deep convolutional networks on graph-structured                                        bidirectional transformers for language understanding, arXiv preprint  arXiv:
       data,  arXiv preprint arXiv:1506.05163.                                                                                        1810.04805.
[12]             Defferrard M, Bresson X, Vandergheynst P. Convolutional neural networks on                                   [42]             Y. Kim , Convolutional neural networks for sentence classification, arXiv preprint
       graphs with fast localized spectral filtering.  In: Advances in neural information                                             arXiv:1408.5882.
       processing systems; 2016. p. 3844–        52.                                                                          [43]             Lai S, Xu L, Liu K, Zhao J. Recurrent convolutional neural networks for  text
[13]             Duvenaud DK, Maclaurin D, Iparraguirre J, Bombarell R, Hirzel T, Aspuru-Guzik A,                                     classification. In: Twenty-ninth AAAI conference on artificial intelligence; 2015.
       Adams RP. Convolutional networks on graphs for learning molecular fingerprints.                                        [44]             Johnson R, Zhang T. Deep pyramid  convolutional neural networks for text
       In: Advances in neural information processing systems; 2015.  p. 2224–        32.  arXiv:                                      categorization. In: Proceedings of the 55th annual meeting of the association for
       1312.6203.                                                                                                                     computational linguistics (volume 1: long papers); 2017. p. 562–        70.
[14]             Hamilton W, Ying Z, Leskovec J. Inductive representation learning on large graphs.
       In: Advances in neural information processing systems; 2017.  p. 1024–        34.

