IEEE         TRANSACTIONS         ON         NEURAL         NETWORKS         AND          LEARNING         SYSTEMS,         VOL.         33,          NO.          6,          JUNE         2022                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2543
                                                                        Deep                                  Graph                                  Learning                                  for                                  Anomalous
                                                                                                                                                                      Citation                                   Detection
                                                                                Jiaying              Liu                                   ,                Member,               IEEE ,FengXia                                                                             ,                Senior                Member,              IEEE ,XuFeng                                                                                       ,
                                                                                  Jing               Ren                           ,                Graduate               Student               Member,               IEEE ,andHuanLiu                                                                                                                                   ,                 Fellow,                 IEEE
         Abstract—     Anomaly         detection        is         one         of         the        most         active         research
areas                   in                  various                   critical                   domains,                   such                   as                   healthcare,                   ﬁntech,                                                              m  1                                                                                                                                                                                                                     Dimension             of              the             node             attribute             vectors.
and                 public               security.                 However,                  little                attention                has                 been                paid                 to                                                                m  2                                                                                                                                                                                                                     Dimension             of              the             edge             attribute             vectors.
scholarly              data,             that             is,             anomaly              detection              in              a             citation              network.
Citation                    is                    considered                    as                    one                    of                    the                    most                    crucial                     metrics                    to                d1                                                                                                                                                                                                                                Dimension             of              node            representation.
evaluate              the              impact              of              scientiﬁc              research,              which             may              be              gamed                                                                                          d2                                                                                                                                                                                                                                Dimension             of              edge             representation.
in               multiple              ways.               Therefore,               anomaly               detection               in               citation               net-                                                                                             r                                                                                                                                                                                                                                                    Learning             rate             for             the             autoencoder.
works             is             of              signiﬁcant             importance             to             identify            manipulation             and                                                                                                             X          ∈                Rn 1 ×m 1                                                                   Node             local             feature             matrix.
inﬂation               of               citations.               To               address               this               open               issue,               we               propose               a                                                                 Z         ∈                Rn 2 ×m 2                                                                       Relation             features             for             the              paper             pairs             (edge).
novel                    deep                   graph                   learning                   model,                   namely                   graph                    learning                   for                                                                A       ∈                Rn 1 ×n 1                                                                           Node             connection            information            (adjacency
anomaly                     detection                    (GLAD),                    to                     identify                    anomalies                     in                    citation
networks.              GLAD              incorporates              text              semantic              mining             to              network                                                                                                                                                                                     matrix).
representation            learning           by           adding           both            node            attributes           and            link                                                                                                                         H   n          ∈                Rn 1 ×d1                                                    Final             node             representations.
attributes         via          graph         neural         networks         (GNNs).         It         exploits         not         only                                                                                                                                  H   e          ∈                Rn 2 ×d2                                                      Final             edge             representations.
the             relevance              of              citation              contents,              but             also              hidden             relationships
between               papers.               Within              the               GLAD               framework,               we               propose               an
algorithm          called          Citation         PUrpose          (CPU)         to         discover          the         purpose                                                                                                                                                                                                                        I.                I NTRODUCTION
of            citation            based            on            citation            context.            The           performance            of            GLAD                                                                                                                           N                     OBJECTIVE                     and                     fair                      evaluation                     of                      the                     impact                     of
is                    validated                     through                    a                     simulated                     anomalous                     citation                     dataset.                                                                                     published                 research                 is                 essential                  for                 science                 itself.                 Since
Experimental             results             demonstrate             the            effectiveness             of             GLAD             on                                                                                                                     A
the             anomalous             citation             detection             task.                                                                                                                                                                               citations                are                important               for                assessing                 the                scholarly                impact                of
         Index             Terms—     Anomalous               citation,               deep               graph               learning,               net-                                                                                                            academic                       entities                       (e.g.,                       publica  tions,                      scholars,                       and                        institu-
work             representation,             scholarly             network             analysis             (SNA).                                                                                                                                                   tions),                    it                     becomes                    essential                    to                     avoid                     making                    citation-based
                                                                                                                                                                                                                                                                     bibliometrics                        strongly                        ﬂawed                         and                        defective.                        A                         branch                        of
                                                                                        N OMENCLATURE                                                                                                                                                                science                       of                        science                        (SciSci)                       [1]                        entitled                       scholarly                       network
      G                                                                                                                                                                       Academic             citation              network.                                    analysis              (SNA)              [2]              has              emerged              intending              to              discover              mean-
      V                                                                                                                                                                         Set             of             nodes             (represent            papers).      ingful                 insights                  to                  implement                  data-driven                 research                  decisions.
      E                                                                                                                                                                       Set             of             edges             (represent            citing-cited    Citation                        network                        analysis                        is                         one                        of                        the                        important                       points
                                                     relationship).                                                                                                                                                                                                  of                        interest                        in                        SNA,                        aiming                       to                        ﬁnd                       relationships                       between
     n 1     =| V    |                                                Number             of             nodes             in              the             citation             network                  G  .                                                         cited                         papers                         and                         citing                         papers.                         Expansion                        of                         large                         open
     n 2     =|   E  |                                                 Number             of             edges             in              the             collaboration                                                                                             databases                         such                          as                         Web                          of                         Science                          (WOS),                          Scopus,                         and
                                                     network                G  .                                                                                                                                                                                     Microsoft                Academic                Graph                 (MAG)                 has                 opened                up                 opportu-
     Y                                                                                                                                                                             Label             set             for             edges.                          nities            for            multi-perspective           and            systematic            analysis            of            citation
      y L                                                                                                                                                                Labeled             citations.                                                              networks            [3].
                                                                                                                                                                                                                                                                               Traditionally,                  the                   quantity                   of                   citations                   is                   one                  measure                   of
      Manuscript                                received                                December                                6,                              2020;                               revised                                June                               10,                               2021,the             quality             of             a              paper,            or             the             impact             of             academic             entities             [4].
September            26,               2021,                and               December                28,               2021;                accepted                 January                10,               2022.                                                 For           example,          existing           bibliometric          indicators          such           as           H-index,
Date           of           publication             February            10,           2022;           date           of           current           version            June           2,           2022.                                                             g-index,                  and                  journal                 impact                  factor                  (JIF)                  all                  focus                 on                  quan-
This          work           was           supported           in           part           by          the         National            Natural            Science            Foundation                                                                              titative                   evaluation                   aspect                   while                    ignoring                  qualitative                  aspects.
of            China           under            Grant           61872054.           (Corresponding             author:             Feng            Xia.)
      Jiaying                 Liu                 is                 with                 the                 School                 of                 Economics                 and                 Management,                  Dalian                            However,           with            the           explosive           growth           of            publications,           the            num-
University             of            Technology,            Dalian            116024,            China.                                                                                                                                                              ber                 of                 citations                increases                fast.                 As                 citation-based                metrics                 are
      Feng          Xia         and          Jing          Ren          are         with          the         School          of         Engineering,          IT         and          Physical                                                                      transformed              into                “academic               currency,”              the                credit               of                citations
Sciences,                      Federation                      University                      Australia,                      Ballarat,                      VIC                    3353,                     Australia
(e-mail:             f.xia@ieee.org).                                                                                                                                                                                                                                also            implies           economical          gains.           Suspicious           citations           are           one            of
      Xu           Feng           is           with           the           School           of           Software,            Dalian            University            of           Technology,                                                                      many          other          issues           in          the           context          of           broader         unethical          practices
Dalian            116620,            China.                                                                                                                                                                                                                          that         affect        science         academic         inte grity        and         fairness.         Speciﬁcally,
      Huan                 Liu                 is                 with                  the                  School                  of                 Computing,                  Informatics,                  and                  Decision
Systems            Engineering,            Arizona            State            University,             Tempe,           AZ           85281            USA.                                                                                                           some                citations                 are                solely                 intended                to                 boost                the                 unwarranted
      This                                                article                                                  has                                                 supplementary                                                  material                                                 provided                                                  by                                                theimpact                  of                  the                  publication,                  rather                  than                   disseminate                  scientiﬁc
authors                                and                                color                                 versions                                of                                one                                or                                more                                ﬁgures                                available                                 atadvancements.           Although            researchers           have            realized            that            not            all
https://doi.org/10.1109/TNNLS.2022.3145092.
      Digital            Object            Identiﬁer             10.1109/TNNLS.2022.3145092                                                                                                                                                                          citations                   are                    equal                   and                   tried                   to                   assign                    different                   weights                   to
                                                                          2162-237X            ©           2022           IEEE.          Personal           use           is           permitted,            but            republication/redistribution              requires            IEEE           permission.
                                                                                                                                  See            https://www.ieee.org/publications/rights/index.html               for            more           information.
             Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

2544                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               IEEE         TRANSACTIONS         ON         NEURAL         NETWORKS         AND         LEARNING         SYSTEMS,          VOL.         33,          NO.         6,          JUNE          2022
citations                 with                  different                types,                 most                 studies                 have                 focused                 on                                               detection                  task.                  Our                  major                  contributions                 can                   be                  summarized
distinguishing          and          weighting          citations          of          a          single          type.          In          fact,                                                                                         as              follows.
as                              Prabha              [5]              illustrated,              more              than               two-thirds              of              references                                                              1)                       We              formally             deﬁne             a             novel             class             of              relationships            intro-
in             a             paper            are             unnecessary,            which            also             conﬁrms            the            existence                                                                                              ducing                both                semantic                information               and                relational                infor-
of             dubious            citations.                                                                                                                                                                                                                     mation,             which             covers            universal             real-world             issues.
        These               misused               citations              can               cause               a               series               of               side               effects.                                                    2)                       We           provide          an           effective          algorithm          named          CPU           based           on
On                       the                        one                       hand,                       they                       affect                       citation-based                      measurement                                                transfer           learning           to            solve           the           problem           of           unmarked          cita-
indices            related             to             scholar             promotion            and            reward             in             bibliomet-                                                                                                       tion         purposes.         It          can         automatically         judge         the          relationship
rics.               On                the               other               hand,               they               may               mislead               readers              about               the                                                          between                the               cited                paper               and                the                citing                paper               based                on
paper                 correct                 value.                 Worst                  of                 all,                 a                  large                 number                 of                 mean-                                     the             citation              context.
ingless                       citations                      can                       negative ly                      impact                      academ ic                       integrity.                                                      3)                       We                propose               a                deep                graph               learning               framework,               namely
Therefore,         anomaly         detection          i  n           the          citation           network          is           critical                                                                                                                      GLAD,                     for                    anomalous                    citation                     detection.                    GLAD                     can
to                       identify                      manipulation                      and                       inﬂation                       of                       citations.                      To                       the                          model               rich                information              with                integrating               attributes               such
best                      of                      our                     knowledge,                     there                     exists                      no                     related                      method                     in                 as             text             content             and             author             relationship.
the               relevant               literature               that               is                directly                applicable               to                automatic                                                                 4)                       Extensive          experiments         have           been          conducted.          The           results
detection             of              anomalous            citations.                                                                                                                                                                                            verify                    the                     effectiveness                    and                     superiority                    of                     GLAD                     as
        Conventional                 anomaly                  detection                   techniques                  cannot                  tackle                                                                                                             compared            against             state-of-the-art            baselines.
this                  problem                  well                  because                  of                  th  e                  complexity                 of                  graph                  data                                 The         rest          of         this          article          proceed s         as          follows.         Section          II         brieﬂy
(e.g.,               irregular               structures               and               relational              dependencies).              Com-                                                                                           reviews               the                related               work                of               SNA                and               GNNs.                In                Section               III,
pared                      with                       the                      traditional                      conventional                     detection                      methods,                                                   we            introduce           mathematical           preliminaries           and            formally           formu-
anomaly                        detection                         methods                        based                         on                         graph                         learning                        can                 late               the               problem              of               anomalous              citation               detection.              Section               IV
preserve                       the                       node                       attributes                       and                       network                       structure                       at                        the illustrates                         our                        optimal                        solution,                        GLAD,                        and                        describes                        its
same             time             during             the             learning             process.             However,            most             of              the                                                                    architecture                   in                    detail.                    Experimental                  settings                    and                   results                    are
anomaly            methods            based             on             graph            learning            focus            on             detecting                                                                                      described                in                 Section                V.                Finally,                in                 Section                VI,                we                conclude
anomalous                   nodes                    in                    the                    network,                    such                    as                     GCNSI                     [6]                    and          the             article.
GCAN                   [7].                  To                  date,                  only                  few                  works                  have                  put                  their                  efforts                                                                                  II.                R   ELATED                  W ORK
into                  utilizing                 graph                 learning                 techniques                 for                 anomalous                edge
detection.                    At                     the                    same                     time,                    most                    of                    the                     anomalous                   edge                Although                    numerous                   techniques                    have                    been                     developed                   for
detection                   methods                  are                   task-dependent                  and                   are                   designed                   for                                                      anomalies         detection         in          different        areas         such         as          ﬁnancial         security
speciﬁc            tasks           such            as            spam           review           detection           and           rumor          detec-                                                                                   and                       image/video                      surveillance                       [11],                       limited                       work                       has                        paid
tion                   [8]–[10].                 These                   methods                  cannot                  be                   directly                  applied                  to                                       attention            to             anomalous            citatio n             detection            in             academia.            Hence,
anomalous            citation            detection.            Th  e             major             issue             of             direct             usage                                                                               in              this             part,             we             introduce             topics             that             are             most             relevant            to              our
of                graph               learning               in                anomalous               citation                detection               lies               in                the                                            research             topics,              including             SNA              and              GNNs.
fact                that                they                cannot                integrate                edge                feature                learning                process                                                      A.              Scholarly             Network               Analysis
and             semantic             analysis             such             as             citation             purpose,             which             cannot
be                  ignored                  in                   the                  learning                  process.                  It                   drives                  us                   to                  propose            It               is                well               known               that               citation                count               is                an                important               index
a            new            method            that            considers           both            anomalous           citation            features                                                                                         to                    measure                   academic                   entities                    such                   as                    papers,                   journals,                   and
and           the          structure          of           the          citation           network           to           identify          anomalous                                                                                      scholars             [12],             [13].             Indeed,             refer   ences             in              the              article              are              often
citations.                                                                                                                                                                                                                                 overlooked               and                there                is                 also                 randomness                to                 a                 certain                extent
        In             this            work,            as            a             ﬁrst            step,             we             deﬁne             a             novel           class             of             cita-                in                     citation                     behavior.                     Prabha                    [5]                     has                     pointed                    out                     that                     only
tions,         namely        anomalous       citations,         which         contains        two         aspects:                                                                                                                         less            than            one-third           of            references           in            each            paper            are            mandatory.
semantic              information             and              relational             information.             We              say               that                                                                                      Moustafa         [14]         introduces         the          idea         that          there         are         multiple         inher-
a                 citation                 is                 anomalous                if:                 1)                 the                 cited                 paper                and                 the                 citingent            biases             in             citation             practice,            which             will            make            citation-based
paper          are           disparate           from          the           perspective          of           contents,          and           there                                                                                      bibliometric                         measures                         strongly                         ﬂawed                          and                          defective.                         For
is            no           clear            purpose           in            the           scientiﬁc           citation            context           and           2)            the                                                        example,             Bartneck              and              Kokkelmans             [15]              make             the             point             that
citing                  paper                 contains                 excessive                 relational                 citations.                 We                  then                                                            authors              can              easily               inﬂate              their              h-indices             and               distort              scientiﬁc
propose               a                framework               based                on                deep                graph               learning,               namely                                                               knowledge                   toward                    more                    conformism                   by                    manipulating                   self-
graph               learning               for                anomaly               det  ection                (GLAD),                to                detect                the                                                          citations.                     Inappropriate                    coauthor                     self-citations                     and                     collabo-
anomalous                       paper                       pairs.                        GLAD                        incorporates                       text                        semantic                                              rative                       self-citations                       can                        also                        mislead                       and                        distort                       scientiﬁc
mining                     to                      network                     representation                    learning                     by                     adding                     both                                       literature,                  thereby                  challenging                  scientiﬁc                   fairness.                  Reciprocal
node               attributes               and               link                attributes               via               graph               neural               networks                                                             citations            indicate            that            some            authors            prefer            to            cite             publications
(GNNs).           In                       addition,          we           propose          an           algorithm          called           Citation                                                                                      of                   people                   who                   cite                    their                   own                   work                   rather                   than                   those                   who
PUrpose                       (CPU)                        to                        identify                        the                        purpose                       of                        citation                        baseddo                not               [16]               publications               of                people               who                cite                their               own                work
on                  citation                  context.                  To                  validate                  the                  performance                 of                  GLAD,                                           rather            than             those             who            do             not.            Moreover,           to             inﬂate             the             journal
we                         generate                        a                         simulated                        dataset                         for                        anomalous                       citations.                impact         factor,         some         editors         have           encouraged        authors         to          cite         the
We           compare         GLAD’s          performance           with          that          of          state-of-the-art                                                                                                                journal’s         own         articles,          which         will         lead          to          journal        citation          stack-
approaches          for           network           embedding.          GLAD           outperforms          base-                                                                                                                          ing.             Previous            studies             have             explored            the             relationships            between
line                       approaches                      by                      up                       to                       37%                       in                       the                       anomalous                      citationjournal            impact            factors            and             self-citations            [17].            Another            similar
            Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

LIU          et          al.:            DEEP         GRAPH         LEARNING         FOR          ANOMALOUS        CITATION         DETECTION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2545
kind               of              citations              is               called               coercive-induced             self-citation              [18],                                                                            ConvGNNs                are                 closely                 related                to                 RecGNNs.                 Different                from
which             may             occur             in              reviewers,             grants,             advisors,             and             so              on.                                                                 RecGNNs,                    ConvGNNs                   handle                   the                    cyclic                   mutual                   dependen-
        Under                      pressure                       to                       ensure                      that                       journals/authors                      maintain                                         cies                           architecturally                           by                           a dopting                          a                           ﬁxed                           number                          of                           lay-
high-value                 bibliometric                 indicators,                  that                  is,                  JIF                   and                  H-index,                                                      ers                     with                      different                     weights.                      ConvGNNs                     can                      be                      categorized
some          editors          have          artiﬁcially           generated          citations           for          their           jour-                                                                                             into             spectral-based            ConvGNNs            and             spatial-based            ConvGNNs.
nals/papers             by             forcing            authors             to             submit             papers             or             by             writing                                                                 Spectral-based             approaches            deﬁne             graph             convolution            based             on
reviews.                    “Citation                     cartel”                    refers                    to                     the                    behavior                    of                     journal                  graph             signal             processing             and              spatial-based             ConvGNNs             includ-
groups                   exchanging                  citations                   with                    excessive                   frequency                  [19].                                                                    ing                    DCNN                                          [25],                    GraphSAGE                    [26],                    FastGCN                     [27],                    graph
Increasing                      competition                      in                       the                         academic                      publishing                      market                                               isomorphism                  networks                   (GIN)                   [28],                   and                   SemanticGCN                   [29]
has                  led                  to                  such                  practice                 becoming                 more                 common                 in                  recent                             inherit                        ideas                         from                        RecGNNs                         by                        information                        propagation.
years.                       Many                       instances                       of                       cartels                       have                       been                       reported.                      For  Recently,        scholars         have        paid        attention        to         compute        mutual        infor-
instance,                        Clarivate                        Analytics,                        the                        publisher                       of                        the                        annual               mation              between               high-dimensional             input–output             pairs              of               deep
Journal                  Citation                   Report                   (JCR),                   has                   featured                  and                   suspended                                                    neural        networks        in         diverse        domains        such        as         images        and        speech.
journals                       which                       distort                       JIF                        caused                       by                       different                       anomalous                      Speciﬁcally,        in         deep        graph        learning,        Velickovic                et              al.             [30]         pro-
citation              patterns.1               The             Scholarly             Kitchen              also              reports             citation                                                                                 pose                deep                graph                infomax               (DGI)                for                learning               representations
cartel               cases.2                 Recipient              journals             can               enhance              their              JIF               by              up                                                  for          graph-structured         inputs          by          maximizing          mutual          information
to               94%              by              receiving             citations              from              donor              journals             in               the              JIF                                           between                   global                   representations                  and                   the                   local                   patches                   in                   an
years.          As          such          indicators         can          reﬂect          the          quality          and          prestige          of                                                                                unsupervised                manner.                Following                 on                 DGI,                 Ren                 and                 Liu                 [31]
journals,             it              is              essential              to              detect              such              behaviors             for              academic                                                       propose           heterogeneous          DGI            (HDGI)            to            learn           node           representa-
fairness.                                                                                                                                                                                                                                tions             by             maximizing             local–global            mutual            information.
        Unlike                     self-citations                     which                     are                     easy                      to                      detect,                     the                     issue               GAEs            learn            network            embeddings           and            graph            generative           dis-
of                 journal                citation                 cartels                 has                 not                 been                 widely                 documented.                                               tributions            by             encoding            nodes/graphs            into             the             latent             space             and
Few                     algorithms                     are                     designed                     for                     detecting                     citation                     cartels.                                  reconstructing          the           graph           from           encoded          information.          GAEs           are
Determining                  the                  boundaries                 of                   “excessive”                  has                   become                  the                                                         available             for              network             embedding             (learn             latent             representations
biggest             challenge.            It              seems             feasible             to             detect             such             anomalous                                                                            by             graph            structural            information            reconstruction)           [32],            [33]            and
pairs         based         on         network         community         approaches.        However,        such                                                                                                                         graph          generation          (generate          step           by           step           or           output          the           graph          all
approaches          may           lead           to           false           positives           because           publications          in                                                                                             at           once)           [34],          [35].          STGNNs           aim           to           capture          the           graph          dynam-
one             journal             tend              to              cite              publications             in              other              similar              journals                                                        ics                   while                   assuming                   the                   interdependence                 between                   connected
in               the                same               research               ﬁeld,               thus                forming               a               closely                connected                                             nodes.        The         dominant        merit         of            STGNNs         is         that         they        can         capture
community.                                                                                                                                                                                                                               spatial             and             temporal            dependencies            of             a             graph             simultaneously.
        Although                  the                  aforementioned                   literature                  explains                  the                   exis-                                                                STGNNs              are              divided              into              two               directions,             that              is,              RNN-based
tence          of          anomalous         citations          through         citation          network          analysis                                                                                                              methods       [36]        and        CNN-based        methods        [37].        RNN-based        meth-
and                 publishing                ethics                elaboration,               to                 the                best                 of                our                knowl-                                    ods                ﬁlter                the                input               and                use                state                passed                to                the                loop                unit                by
edge,                         automatic                        anomalies                        detection                         methods                        for                         citation                                    graph                convolution                to                 learn                the                spatial–temporal               dependence.
networks                       have                        not                        been                        well                        studied.                        On                        the                        one                        hand,CNN-based                  methods                 interleave                 1-D-CNN                 layers                 with                  graph
existing                  graph                 anomaly                  detection                  algorithms                 cannot                 capture                                                                            convolutional                    layers                     in                      a                      non-recursive                    manner.                     Compared
abnormal        subtle          signal         strengths         to          support         pairwise         anomalies                                                                                                                  with                 RNN-based                 methods,                CNN-based                 approaches               can                 real-
detection                   in                   citation                   networks.                  On                   the                  other                  hand,                  the                   lack                ize                        parallel                        computing,                       stable                        gradients,                       and                        low-memory
of                   a                    ground-truth                  dataset                   with                   considerable                  size                    challenges                                                requirements.
the                    application                   of                   supervised                   machine                   learning                   methods                   in                                                          In                            addition                            to                            typical                            GNNs                            mentioned                            above,                            many
anomalies             detection.                                                                                                                                                                                                         variations                             of                             GNNs                             have                             been                             developed                            recently,                            that
                                                                                                                                                                                                                                         is,                                GRCN                                [38],                                GAUGM                                [39],                               GNN-Guard                                [40],                               and
B.              Graph              Neural              Networks                                                                                                                                                                          AMGCN                  [41].                  It                  has                  been                  proved                 that                  GNN                  brings                  break-
        The                                          concept                                        of                                          GNNs                                          was                                          initially                                         outlined                                         bythrough                     improvements                    to                      fulﬁ  lling                      various                     tasks                      in                      differ-
Gori              et              al.              [20],                which               is               a                kind               of               neural               network               specially                   ent                  areas                  such                  as                   e-commerce,                 ch  emistry,                  and                   biomedicine.
designed                     for                     processing                     highly                     irregular                     graph                    data.                     GNN                                      Speciﬁc                             applications                            of                             GNNs                             include                             computer                            vision,
models              can               be              categorized              into               recurrent             GNNs               (RecGNNs),                                                                                    natural                   language                   processing,                   recommender                  systems,                    and                   so
convolutional                                       GNNs                                         (ConvGNNs),                                        graph                                        autoencoders                            on             [42],            [43].
(GAEs),             and              spatial–temporal            GNNs              (STGNNs)             [21].                                                                                                                                     Graph              anomaly             detection              with               graph             learning              has              received
        RecGNNs               rely              on               recurrent              neural              architectures              and               apply                                                                           growing              attention               recently.              Exi sting               work              can               be               categorized
the               same               parameter               set                recurrently               over               nodes               to                learn                node                                             into                      anomalous                     nodes                      detection                      methods,                      anomalous                     edge
representations.                    They                    assume                     a                     node                    constantly                     exchanges                                                            detection             methods,            and             anomalous            subgraphs            detection             meth-
information/message                    with                     its                     neighbors                    in                     a                     graph                     until                     a                  ods                according                to                 the                anomalous               graph                objects                that                they                can
stable                    equilibrium                   is                    reached.                    Early                    research                    on                    RecGNNs                                             detect.                     The                     detection                     of                     anomalous                     nodes                     is                      the                     focus                     of
includes                  GNN                  [22],                  GraphESN                 [23],                  and                  gated                  GNN                  [24].                                             scholars.                     Scholars                     focus                     on                     using                     graph                    attention                     network
                                                                                                                                                                                                                                         (GAT),         graph         convolutional        network         (GCN),         and         GAE         to         iden-
     1  http://help.prod-incites.com/incitesLiveJCR/JCRGroup/titleSuppressions                                                                                                                                                           tify            malicious            users,            fake            news,            and            ﬁnancial            fraud.            Only            few
     2  https://scholarlykitchen.sspnet.org/2012/04/10/emergence-of-a-citation-                                                                                                                                                          works           have           put           their           efforts           into            the           anomalous          edge           and           sub-
cartel/                                                                                                                                                                                                                                  graph                detection                based                on                graph                learning.               Ouyang                  et              al.              [44]
            Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

2546                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               IEEE         TRANSACTIONS         ON         NEURAL         NETWORKS         AND         LEARNING         SYSTEMS,          VOL.         33,          NO.         6,          JUNE          2022
models                 the                 distribution                 of                  edges                 based                 on                 a                 neural                 network                                  training         processes.         That          is,          given         a          paper         pair         (i ,              j   )          and          related
and                identify                existing               edges                that               are                least                likely                to                appear               as                            feature         information,        GLAD          can          output         the          citation         relationship
anomalies.                    AddGraph                   [45]                   is                     a                    GCN-based                    framework                   for                                                     between              i                  and                      j    .
anomalous                 edge                 detection                  in                   dynamic                 networks.                 There                 is                  a                                                                                                                  IV.                 GLAD        F    RAMEWORK
huge        gap        between        the         existing        anomalous       edge/subgraph/graph
detection                      techniques                      and                      the                      emerging                     demands                      for                      more                                              In                   this                   section,                  we                   describe                  the                   proposed                  framework                  for
advanced            solutions             in              various            applications.                                                                                                                                                   identifying                       anomalous                        citations                        in                         detail.                         Speciﬁcally,                        the
                                                                                                                                                                                                                                             overall                 framework                 contains                 three                  main                  components,                that                  is,
                                      III.                P    RELIMINARIES        AND                D EFINITIONS                                                                                                                           node                feature               learning,                edge                feature                learning,               and                anomalous
         Before                        we                        give                        details                        about                       the                        proposed                       framework,                 citation                       identiﬁcation.                       Fig.                        1                        gives                       a                        graphic                       example                       of
we                   ﬁrst                   delve                  into                   the                   concept                  of                   anomalous                  citations                   and                     GLAD.             GNNs             can             effectively            learn            the             complex            structures            of
mathematical            preliminaries             used             throughout           the             article.                                                                                                                             graph-structured            data.              In              the              academic              citation              network,             both
                                                                                                                                                                                                                                             node         and         edge         attributes         can         reﬂect         the         network         properties.        For
A.               Deﬁnition               of              Anomalous              Citation                                                                                                                                                     example,              from               the               perspective              of               node              attributes               such               as               the
         The                        aforementioned                       literature                        explains                        the                        existence                        of                                    textual            information,            the             research             topic             and             content            of             a             paper
different               abnormal              citation                patterns               through               citation                network                                                                                           are          closely         related         to          the          abstract          of         a          paper,         which         helps         judge
analysis        and         publishing        ethics        elaboration.        Unfortunately,       there                                                                                                                                   the            similarity             of            two             papers            in             topic            and            content.            At            the            same
is                no               clear               deﬁnition               of               anomalous              citation,               as                well               as                auto-                                  time,            edge            attributes            such            as             citation            purpose           and            self-citation
matic                     anomalous                    citation                     detection                     methods.                    This                     research                                                              can                   reﬂect                   the                   conﬂict                   of                   interest                   between                   the                   two                   papers.
aims                       to                       solve                       the                       problem                      of                       anomalies                      detection                       in                       theHence,          we          incorporate         attributes          for          both          edges          and          nodes         into
citation               network.               To                this               end,               as               a               ﬁrst               step,               we               refer               to                the     GNN           models           and            formalize           the            task            as            modeling           the            relations
above-mentioned            aberration             citations              to              give             the              deﬁnition              of                                                                                         between                  vertices.                   In                   order                  to                   preserve                  the                   global                  and                   local
anomalous            citations.                                                                                                                                                                                                              network                  structures                  in                   the                  joint                  space                  composed                  of                  network
         Deﬁnition              1               (Anomalous             Citations):                   In         this         article,         the         cita-                                                                              structures,                     node                     attributes,                      and                      edge                      attributes,                      we                      exploit
tion              relationship              between              paper              pairs              that              satisﬁes               the              condi-                                                                      the               nodes’              and               edges’               proximity              jointly               by               using               the               GLAD
tions             described             below             is             regarded            as             anomalous.                                                                                                                       framework.             We              hope              that              the              process              of              feature              learning              can
         1)                        The             cited             paper             and             the              citing             paper             are             disparate             from                                      retain            the            feature           information           of            both           nodes           and           edges,            and            at
                     the          perspective         of          contents,         and          there          is          no          clear          purpose                                                                               the                   same                   time,                   integrate                   the                   learned                   feature                  representations
                     in              the             scientiﬁc              citation              context.                                                                                                                                   during                     the                      classiﬁcation                      process.                     The                      individual                     node                     and
         2)                        The             citing              paper              contains             excessive             relational             citations.                                                                       edge                  feature                  learning                  processes                  are                  unsupervised.                 For                  node
                                                                                                                                                                                                                                             feature             learning,             we              select              DGI              as              the              workhorse             method             for
B.               Mathematical              Preliminaries                                                                                                                                                                                     our             node            representation            module            because             it             can             integrate             node
         In               this               part,              we               introduce             the              notations              we              will               use              in               the                      attributes             and              network             structure             by              maximizing             mutual             infor-
remainder          of          the          article.          An           academic          citation          network               G             con-                                                                                      mation                between                patch                representations               and                the                corresponding
sists          of         a          set          of         nodes           V       ={v 1,v 2 ,...,v      | V   | }          (represent         papers)                                                                                     graphical            high-level           summary            based             on             GCN             in             a             completely
and                    a                    set                     of                    edges                         E       ={e1 ,       e2 ,...,              e|  E | },                    which                    describe           unsupervised                   manner.                   The                   edge                   feature                   learning                   process                    is
the                  interaction                  in                  terms                  of                  citing-cited                  relationship                  between                                                         established             based             on             the             autoencoder            because             it             can             learn             the
nodes.           The           connection          between           nodes          can           be           described          by           the                                                                                           edge                feature                representations                in                 an                 unsupervised                manner                and
adjacency               matrix                   A.                  Aij                    denotes                the                element                in                the                i    th                row                 the            learning           results            can            be            explained           by            reconstruction.           In            the
and             the                       j    th             column             of                A.             If              there             is             a              link              between             nodes          v i   following,               we                elaborate                on                the                speciﬁc                 process               of                each                part
and          v      j    ,                Aij           =           1;             otherwise,                Aij           =           0.                                                                                                    and              the             integrations             of              these              parts.
         In               this               research,               the               anomalous              citation               detection               problem                                                                         A.              Node              Feature             Learning
is              formulated             as              a               binary              classiﬁ cation              problem.              Speciﬁcally,                                                                                             Features          of           nodes          can           reﬂect          their           inherent          attributes.           In          our
the          citation          between          each          paper          pair          is           either          a           non-anomalous                                                                                            framework,              node              features              can               be               regarded              as               the               local               prop-
citation                or                an                anomalous               one.               Matrix                Y               ∈                    R| V  |×| V  |                  contains                                   erties                 of                 papers,                 such                 as                 research                 topics,                 and                 importance                in
labels                   for                   each                   paper                  pair,                   where                  the                   value                  of                   the                  element   the              citation              network.             These              features             reﬂect              the              importance             of
located                   in                   its                    i    th                   row,                           j    th                  column                      yij                 =                    1                   means                  that                  thepapers           and            topic            similarity            between            paper            pairs.            For            example,
citation                      from                      paper                      i                          to                     paper                               j                         is                      anomalous,                    otherwisecited                    papers                    and                    citing                    papers                    should                    be                     similar/related                    in
yij           =           0.              Nomenclature             provides             detailed              description             of              major                                                                                  research                topics                or                 contents.                N  ode                feature                learning                is                carried
notations            we             use             in              the             remaining             part.                                                                                                                              out          from          two           aspects:          textual          feature          learning          and          local–global
C.                Problem              Description                                                                                                                                                                                           information             fusion.
                                                                                                                                                                                                                                                      1)              Textual            Feature           Learning:                 Textual          information          including
         According              to               the               above              symbols              and               deﬁnitions,              the              input                                                                 titles,               keywords,              abstracts,               and               main               texts              is               a               major              compo-
of             the             anomalous           citation            detection            task             includes            a             paper            pair                                                                         nent               of                scientiﬁc               publications.               In               order               to                preserve               the               local
(i ,              j   )                 associated                  with                  their                 local                  features                   X   ,                  relation                 (edge)                     information                     as                      well                      as                      proximity                      of                      each                      paper                      from                      the
features                        Z                         for                       the                       paper                      pair,                       citation                       context                      between                           iperspective           of            textual           feature,           motivated           by            Doc2vec           [46],           the
and                           j    ,                 and                  partially                  labeled                  citations                     y L    .                  We                  aim                  to                   obtainprocess              of              vectorizing             text              into              a              numeric             space              is              illustrated
labels                for                remaining               unmarked               citations                by                the                learning               and                                                             as              follows.
            Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

LIU          et          al.:            DEEP         GRAPH         LEARNING         FOR          ANOMALOUS        CITATION         DETECTION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2547
Fig.           1.                                  Overall            framework            of           GLAD.
         We                         ﬁrst                        build                        up                        a                         corpus                       of                        text                       information                       (paperwhere                       U                           and                        b                       are                      softmax                      parameters.                      a                       is                       constructed
abstracts)                 as                 a                 set                     T       ={t1 ,       t2 ,...,              t| V   | },where                   ti                      is                  the                  list                     by                  averaging                 word                  vector                  extracted                  from                   W                       and                  document
of                   paper                    i    ’s                   abstract.                   In                   the                   training                   process,                   each                   abstract                            vector                extracted                from                   D .                The                paper                abstracts                are                vectorized
is                       represented                       by                       a                        unique                       vector                       in                       terms                       of                       a                        columnin          the          latent          space          by          using          the          Distributed          Memory         version          of
of                   matrix                    D ∈                      R| V  |×   p,where             | V    |                   represents                  the                  number                 of                                                    Paragraph             Vector             (PV-DM)              model             [46],             where             similar              papers
the                paragraphs               (abstracts)                and                each                abstract                is                mapped                to                            p                                                   will              be             close              to              each             other.
dimensions.             Each              word              of              the              abstract              is              also              represented             by                                                                                           2)              Local-Global                 Information                 Fusion:                        Through                the                 textual
a                unique              vector               in                terms               of               a               column               in                matrix               W  ∈                  R  Nw  ×q   ,                                information                       learning                       process                        illustrated                        in                        Section                        IV-A1,
where                Nw                is            the            number           of            words            in            the            vocabulary            and            each                                                                      we                have                obtained               the                textual                rep resentations               for                each                paper.
word                  is                   mapped                  to                     q                      dimensions.                  The                  goal                  is                   to                  predict                  a    However,                we                 hope                 that                 node                 representations                can                  reﬂect                 not
word                  with                  the                  highest                  probability                 in                  the                  current                 context                 by                                               only                     textual                     information,                    but                     also                      node                     importance                     in                     the
giving               a                speciﬁc                paragraph               vector               and                several               word                vectors.                                                                                 citation          network.         To          tackle         this          problem,         inspired         by          DGI          [30],
Speciﬁcally,         the          given         sequence         of          training         words         will          be          ﬁxed                                                                                                                      the             procedure             for             the             node             representations            generation            is              fully
through                a                 preset                 window                win.                 win                 represents                the                maximum                                                                             summarized             as             follows.
distance        between        the         given        words         and        the         predicted        word        in         the                                                                                                                                  For         nodes         in            G  ,         we          ﬁrst         connect         their         textual         representations
sentence.               The               window               will                traverse               the               entire               corpus.               When                                                                                     and                artiﬁcially                deﬁned               features               illustrated                in                Table                I.               Hence,
sliding             to             position             t   ,             the             abstract            vector               di                 of             the             abstract            and                                                    a                   set                   of                   node                  features                     X       ={ x1 ,       x2,...,             xn 1  }                    is                   generated,
the             word             vectors            of          |win|             words             in              the             front            and           |win|               words                                                                    where                      n 1      =| V    |.                      A                 ∈                         Rn 1 ×n 1                         is                     also                     provided,                   where                      A
in                the                back                of                the                word           wt       areusedtopredicttheword                                                wt                                                                 represents               node               connection               information.               The               ﬁnal                high-level
that          will           appear          at           this          position           with           the          maximum          probability.                                                                                                            node            feature           representations           are            learning           through           the            encoder,
The             task              is             to              maximize             the             average             log-likelihood                                                                                                                        ξ              :        Rn 1 ×m 1         ×      Rn 1 ×n 1           →                             Rn 1 ×d1    .
                      1                            M −|             win|                                                                                                                                                                                                 The                  encoder                 learning                  process                  relies                  on                   maximizing                  local
  M         −                           2|win|                                 log            pwt −|             win| ,...,                                                                                                                                    mutual                  information                  rather                   than                   minimizing                  the                   construction
                                                     t =|win|                                                                                                                                                                                                   error.         Negative         examples         are          gen erated         by         using          the          corruption
                                                                                                               wt −            1 ,wt +1 ,...,w       t +|win| ,       di                                                (1)                                    function            χ                      :            (  X  ,         A)           =         χ(           X  ,          A).            χ                               modiﬁes                the                network               to
where              M              is          the           number          of          all          training          words          (the          length          of           the                                                                            obtain                  negative                 samples.                 Here,                  we                  perturb                  the                  node                 feature
sequence)            and               di                is             the            document           representation            vector            of            the                                                                                         matrix            by            adding            row-wise           Gaussian            noise,            while            keeping           the
abstract         list         containing        the         context        word         in          the         current        window.                                                                                                                          graph          structure           unchanged.          The           patch           representations          for           each
The             prediction             task              is             performed            by             hierarchical             softmax                                                                                                                    node                    can                    be                    learned                    through                   GCN                 ε      ,                     which                    can                     integrate
 pwt   |wt −|             win| ,...,w       t −            1 ,wt +1 ,...,w              t +|win| ,       di                                                                                                                                                   the             information             of             neighbors             for             the             target             node.             Information
                                                                                                                                                                                                                                                               integration            process             can              be             formulated            as
                                                                                                                                               =                                                                         exp   Prwt  Nw                                                  (2)                                                                           
                                                                                                                                                                                                                                                                                                                    ε    (  X  ,          A)        =        σ                D −                 12         A   D −                 12         H   nl              W    nl                             (4)
                                                                                                                                                                      j  =1        exp   Prw   j
where                         Pr                          is                          the                         log                         probability                        of                         the                         output,                        which                         iswhere               A         =                A       +            I   .               D                 is               the               degree              matrix               of               A,and                H   n                   is
computed            as                                                                                                                                                                                                                                          the              learning             feature             of              each              layer.               W    n
Pr   =    U     awt −|             win| ,...,w       t −            1 ,wt +1 ,...,w              t +|win| ,       di   ;       W  ,         D  +  b                                                                                                           ter                    matrix                   in                    the                    l  th                   layer                   of                   node                   representation                  learning.l                             is              a              learnable             parame-
                                                                                                                                                                                                                                         (3)                    For                   the                   input                   layer,                      H   n0                       =                      X   .               σ                                 is                     a                    nonlinear                   activation
             Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

2548                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               IEEE         TRANSACTIONS         ON         NEURAL         NETWORKS         AND         LEARNING         SYSTEMS,          VOL.         33,          NO.         6,          JUNE          2022
                                                                                                    TA    B   L    E                     I                                                                                                                                                                                                        TA    B   L    E                     I    I
                       D ESCRIPTION   OF                A RTIFICIALLY         D EFINED          N ODE          F EATURES                                                                                                                                                                A NNOTATION             S CHEME     FOR               C ITATION          P URPOSE
function.                   We                    can                    get                    the                    patch                   representation                     hni                              for                    each
node                 in                 positive                 samples                 set                 through                 (4).                Similarly,                 we                 can
also          generate          patch          representations       hni                    through         (4)          for          negative
samples.                Global                summaries                 s                 are                obtained                by                passing                patch
representations            through            the             readout            function          R
                                                                                                                                     N  p
                                                              s        =      R   H   n          =                               1N   p           hni       .                                                                                                                                                                                                                                                                 (5)
                                                                                                                                      i =1                                                                                                     parts:                 feature                 representation                 learning                 process                 and                  ﬁnetuning
Note                that                     N   p                   is                 the                number               of                positive                samples.                A                discrim-                    process.
inator           D                    is                   employed                  to                   discriminate                  positive                  and                   negative                                                        To             capture            semantic             information            and             bidirectional            relations
patch-summary            pairs                                                                                                                                                                                                                 in            sentences            of            the           citation            context,           the           most            important           step            is
                                                                                                                                 T      W    n    s                                                                                           to           learn           a           good          feature          representation          for          each           word.          As                       there
                                                                D hni       ,       s        =        σ               hni                                                                                              (6)                   are                    few                    labeled                   samples                    for                    citation                   contexts,                   CPU                    adopts
where             W    n                is             a             learnable            scoring             matrix             for             the             discriminator.                                                                SCIBERT                   [48]                  for                  pre-training,                 which                   is                   a                   pre-trained                  lan-
The             ﬁnal             objective             function             is                                                                                                                                                                 guage          model          established           on           BERT           for           scientiﬁc           text.           The           core
                                                          ⎛                                                                                                                                                                                    idea           of          pre-training         is           to           learn           good          feature          representations         for
                                                                 N  p                                                                                                                                                                         words                 by                  running                 a                  self-supervised                 learning                 method                 on                  the
 L   n          =                                                                                 1N   p        +             Nn⎝E( X ,  A) log   D hni      ,       s                                                                     basis           of           a            massive           scientiﬁc            corpus.           Thus,           we            can            directly           use
                                                                  i =1                                                                                                                                                                         the              feature              representations             as              the              word              embedding             feature              of
                                                                                           Nn                                                                             ˜                   	 ⎞                                             the             classiﬁcation              task.
                                                                               +E    ˜                                    	  1      −                           log   D    hn                         ⎠                                                                (7)The                 overall                framework                for                 feature                 representation                learning
                                                                                                                X ,     ˜A                                                          j   ,       s                                              is            established            on            a            multi-layer           transformer           architecture,           which
                                                                                          j  =1
                                                                                                                                                                                                                                               is                composed               of                several               encoders               and                decoders.               The                encoder
where                     Nn                    is                the                number                of                negative               samples.                Thus,               we                can                          consists          of          multi-head         attentio n          and         a          full          connection,         which          is
generate            the             representation            for             each             node             by             integrating            local                                                                                    used               to               convert             input              corpora             into               feature              vectors.              The              input
information                  and                   global                   structural                   information                  into                   the                   ﬁnal                                                        of                the                 decoder                is                 the                 output                of                the                 encoder.                It                 is                 composed
feature             representations.                                                                                                                                                                                                           of                  masked                  multi-head                  attention,                  multi-head                  attention,                  and                   a
                                                                                                                                                                                                                                               full                connection                to                 output                the                conditional                probability                of                 ﬁnal
B.               Citation               Purpose              Classiﬁcation                                                                                                                                                                     results.            The           input           of           pre-training                 E  (i   )            is            the           unit           sum            of           three
         Citation          context        can         provide         important        semantic         information                                                                                                                            embedding          features,           including           wordpiece           embedding,          position
about          the           relation          between          the          paper          and          its           references,          that          is,                                                                                  embedding,            and             segment            embedding.            The            architecture            for             the
the                authors’                 intention.                The                 deﬁnition                of                 anomalous                citations                                                                       pre-training            process             includes             two             self-supervised             tasks.
brings             the              problem             of             identifying             the             purpose             of             the             citation.                                                                             1)                       Masked                language               model:               During                training,               some                words
That           is,            given           a            target           paper             i                and            its            citation            in            a            given            paper                                                   are              masked              from              the              input              corpus.              Then               the              words              are
 j    ,                 we                  need                 to                  identify                 the                 intention                 behind                 selecting                          j                     and                      predicted                  by                     T    (i   ),                  the                  output                  of                  the                  last                   Transformer
citing              it              by              the              author             of              i    .                                                                                                                                                       at                      position                      i    .                     In                     the                     experiment,                    15%                     of                     WordPiece
         Previous                  studies                   have                   been                  devoted                  to                   studying                  different                                                                          Token          [49]          will           be           randomly          masked.          When           training          the
purposes                of                 citing                 other                 publica  tions,                 including                comparison,                                                                                                         model,            a             sentence            will             be             input            into            the             model            multiple
use,         criticizing,         and          so          on.         Researchers         also          have         come         up          with                                                                                                                  times             for             parameter             learning.
different                   schemes                   composed                   of                    citation                    purpose                   categories.                                                                                2)                        This             task              is              to              determine             whether             two              sentences             follow
In                this                article,                we                adopt               a                scheme               that                contains                six                purpose                                                     each                  other                  by                  using                  the                   classiﬁcation                   token                     C                     of                  the
categories           (as           shown           in           Table            II)           after            studying           the            previously                                                                                                         sentence.                  The                  training                  data                  is                   generated                  by                   randomly
used             citation             taxonomies            [47].                                                                                                                                                                                                    extracting                two                 consecutive                sentences                 from                the                 parallel
         We                  propose                 an                 algorithm                 to                 identify                 the                 citation                 purpose                                                                   corpus.                50%                of                 the                 consecutive                sentences                are                 retained
based                     on                      the                      citation                     context,                     which                     we                      refer                     to                      as                      CPU as          positive          samples          (IsNext).          The          second          sentence          of          the
algorithm.              Different               from               previous              work,               we               incorporate              the                                                                                                           remaining              50%              is               randomly              extracted              from              the              corpus.
semantic           recognition           process           into           the            citation           purpose           classiﬁ-                                                                                                                               The             relationship            between             th ese             sentences             is             “NotNext.”
cation           task,            which            is            to            train            the           classiﬁcation            model           based           on                                                                               In                     the                     ﬁnetuning                    process,                     we                     feed                     ﬁnal                     vectors                     for                     the
feature             representations           obtaine  d            from             a             set             of             labeled             citation                                                                                 token                         into                         a                         linear                         classiﬁcation                         layer.                         For                         this                         purpose,
contexts.                 As                  shown                 in                  Fig.                  2,                 CPU                  mainly                 consists                  of                 two                  we                        perform                       support                       vector                        machines                       (SVMs)                        on                        citation
            Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

LIU          et          al.:            DEEP         GRAPH         LEARNING         FOR          ANOMALOUS        CITATION         DETECTION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2549
Fig.           2.                                   Architecture             of            CPU.
context                    representations.                    Since                     the                     problem                    is                     a                     multi-class                                                                                where                 k                   is                the                kernel               function,                  k ( ci   ,       cnew )           =         φ(          ci   )T   φ(          cnew ).
classiﬁcation              problem,              we              need              to              train                    Nc                  classiﬁers,               where                                                                                                     For            cnew,thereare                Nc                decision            functions           and                   Nc                outputs.           The
thereareatotalof                                                                               Nc                           classes                       of                       training                       data.                       The                          i    th  classiﬁcation              function                       f                       is
training         sample          is       ( ci   ,      l i   ),where         ci               is          the          textual         representation                                                                                                                                                                                                                                                                 l i   α                 j
of                 the                  citation                  context                 learned                  by                  CPU.                  The                  category                 label                                                                                          f      ( cnew )        =           arg                                                      maxj  ∈      {1,2,..., Nc  }                i            k ( ci   ,       cnew )      +         b      j    .                                    (11)
l i      ∈{            1,      2,...,                 Nc  }.Thereare                    N                    samples.               For               each                category                                                                                                  By                 solving                the                dual                problem,               the                optimal                Lagrangian               multi-
presented                   in                    Table                    II,                    we                    need                    to                    train                    a                    binary                   classiﬁer.                             plier                vector                can                 be                obtained.               Then,               we                can                get                the                decision
Speciﬁcally,                      for                      each                      category,                     we                       take                       it                      as                       the                       positive                          function,            which             is              used             to              classify             new             data.
class.                    All                    the                    samples                    of                     the                    remaining                        Nc            −                                1                    categories
are                 considered                 as                 the                 negative                class.                  Hence,                 we                 construct                a                                                                          C.               Edge              Feature             Learning
binary             SVM             to             separate             the                       j    th             category            from             the             rest             of             the                                                                                 Edge               attributes               can               depict               relationship               intensity.               Based                on
Nc           −                              1                   category.                 It                  can                  be                  regarded                 as                  solving                  a                  quadratic                           the                deﬁnition                of               anomalous               citations,               edge                features               are                used
programming            problem                                                                                                                                                                                                                                                      to                   help                   us                   determine                  whether                  there                   is                   a                   clear                   purpose                  and
                                                                      1w      j   T    w      j          +             R                N                                                                                                                                        excessive                       relations                       exist                       between                       the                       cited                       paper                       and                       the
                                             min                                                                                                          ε           ji                                                                                                            citing                   paper.                   To                   preserve                  the                    edge                   proximity,                  motivated                   by
                                       w    j   ,b    j   ,ε        j 2                                                                    i =1
                                                                                                                                                                                                                                                                                    Tu              et              al.              [50],                  a                 deep                  autoencoder                is                  used                  to                  obtain                 feature
                                             s.t.          w      j   T    φ        ( ci   )      +         b      j           ≥                            1      −                   ε           ji       ,                                      if              l i           =                   jrepresentations                      for                      edges                      in                       the                       network.                      An                       autoencoder
                                                            w      j   T    φ        ( ci   )      +         b      j      ≤−                                   1      +      ε           j                                                                                       consists            of            two            parts,            that            is,             an            encoder           and            a            decoder.           Thus,
                                                                                                                                                                                 i       ,                                      if              l i           =                   jhidden                    layers                     of                     the                     autoencoder                   can                     continuously                    encode
                                                            ε           ji                ≥0(8)                                                                                                                                                                                     and            decode            information           obtained            from            the             input            data,            thereby
where            the            subscript              i                 represents            the             index            of             the            sample.            The                                                                                                automatically                 capturing                  the                  characteristics                 of                  the                  input                  data
superscript                             j       ∈{            1,      2,...,                 Nc  }.             w      j                        is                   a                    normal                  vector                   that                                     and             keeping             them             unchanged.
determines             the              direction             of              the              hyperplane.           φ                           is               used              to               map                                                                                      For                   each                   edge                  connecting                  the                   citing                   paper                   and                   the                   cited
the             input            samples            to             the             high-dime  nsional           space.             Speciﬁcally,                                                                                                                                     paper                        in                         the                         citation                         network,                        we                         consider                        the                         attributes
we                   map                   the                   sample                   ci                        to                   the                    feature                   space                   and                   get            φ(          ci   ).          presented              in               Table              III.              Thus,              we              can              get              the              edge              feature              set
ε           ji                           and                     R                  represent               the                slack                 variable                and                the                regularization                                                   represented            as                Z    ={  z 1 ,       z 2,...,                    z n 2  },where              n 2     =|   E  |.
coefﬁcient,             respectively.            The              Lagrangian             duality             of              (8)             is                                                                                                                                               CP                         is                        decided                        by                        classiﬁcation                         results                        of                        CPU.                         If                        the
                                                  N                                                   N                                                                                                                                                                             classiﬁcation                        results                       belong                       to                        one                       of                        the                       ﬁrst                        ﬁve                       cate-
                          max                                α                 j                                 α                 j                                                                                                                                              gories           (criticizing,           comparison,          use,           substantiating,           and           basis),
                                                                    i              −                                12   i        α                 jn    l i    l n  φ        ( ci   ),φ        ( cn  )                                                                          we             treat             them             as             citations             with             a              clear             purpose.            JF              represents
                                               i =1                                            i,n =1                                                                                                                                                                               the         proportion       of         the         journal        to         which         the         cited         paper        published
                                                                                                                                                                                        N                                                                                          in             the             reference            list             of             the             cited             paper.             It             can             be             calculated             as
                          s.t.              0        ≤                    α                 ji                ≤                                 R ,                                         i           =           1,      2,...,                 N  ;α                 ji          l i           = 0(9)follows:
                                                                                                                                                                                         i =1
where         α                         denotes             the             Lagrangian             multiplier             of             inequality             con-                                                                                                                                                                                                                          JF        =           |   Pcj ||   Re |                                                                                                                                                                                                                                                                                                                                                                                     (12)
straints.                For              cnew ,               the               decision               function               of               type                          j                   adopts               the                                                          where           |   Pcj |               is              the              number              of               papers              in               the              reference              list               that
following             function:
                                                                                                                                                                                                                                                                                    publish             in              the             same             journal            as             the             cited             paper             and            |   Re |              is              the
       d        jnew         =        w      j   T    φ        ( cnew )      +         b      j           =              l i   α                 ji            k ( ci   ,       cnew )      +          b      j                                                     (10)         number            of              references             of             the              citing             paper.
              Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

2550                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               IEEE         TRANSACTIONS         ON         NEURAL         NETWORKS         AND         LEARNING         SYSTEMS,          VOL.         33,          NO.         6,          JUNE          2022
                                                                                                         TABLE           III                                                                                                                                     Algorithm             1              Anomalous             Citation              Detection              With              GLAD
                                                                 D ESCRIPTION   OF                E DGE          F EATURES                                                                                                                                       Input:                 A,             X   ,              Z ,         α,     β                      ,             r    ,                L  ,             and             the            convergence           condition        	  .
                                                                                                                                                                                                                                                                 Output:               l             for             each               e         =        (i ,              j   )
                                                                                                                                                                                                                                                                    1:               Initiate:           randomly          initiate         θ    1     ←{                            W    ei        ,         B ei       }mi =1  ,        θ    2     ←{                            W    nl          },
                                                                                                                                                                                                                                                                              and          θ    3     ←{                            W   l  ,         B l  }.
                                                                                                                                                                                                                                                                    2:               Pre-train:set           r    ,           conduct         edge           feature          learning          process          and
                                                                                                                                                                                                                                                                              node             feature             learning             process             independently.
                                                                                                                                                                                                                                                                    3:                      Tr    a    i    n:
                                                                                                                                                                                                                                                                    4:               while          	                is              true              do
                                                                                                                                                                                                                                                                    5:                                                              randomly               generate               a                batch                of                data                from                edge                features
                                                                                                                                                                                                                                                                              and             local             node             features
                                                                                                                                                                                                                                                                    6:                                                               calculate                 L   n                 based             on             Eq.(7),             get             reconstructed                H   n
                                                                                                                                                                                                                                                                    7:                                                               calculate                 L   e                based             on             Eq.(14),            get             reconstructed                 H   e
                                                                                                                                                                                                                                                                    8:                                                               concatenate               H     ←[                              H   n  ,         H   e   ]
         The                autoencoder               adopts                   Z                  as                 input.                In                 each                 hidden                layer                                                      9:                                                               calculate                 L  sum                 based             on             Eq.(16)
of                      the                       autoencoder,                     the                      transformation                     mechanism                      can                       be                                                       10:                                                              update          θ    1,θ    2,θ    3
formulated            as                                                                                                                                                                                                                                         11:               end              while
                                              he                                                                        
                                                    1           =                        f       W    e1       z       +         be1,                                        i           =           2,...,              k                                                                                                                               (13)
                                               he                                                                                                                                                                                                                The             joint             loss             function                  L  sum                is             computed             as
                                                    i               =                        f       W    ei            hei −            1        +         bei                                                                                                                                                                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
where                                    f                                  is                         the                         activation                         function,                        that                         is,                         the                         SigmoidL  sum         =              L   n        +            L   e        +      β               L  lr        +      α | W    ei |2F         +         |   B ei           |2F
function.              W    ei                           and              bei                       represent              the               transformation             matrix              and                                                                                                                                                                                               i
the              bias               vector              of              the               i    th              layer,              respectively.               k                 represents              the
number                of                 hidden                layers.                 The                 output                of                 the               (i           −                              1)th                  layer                                                                                                                                                                                 +    | W    nl                             |2F                         (16)
isfedintothe                                   i    th                hidden               layer.                For                any                arbitrary               real                value                                                                                                                                                                                                                                     l
vector,            we             use            the             Sigmoid            function            to             map             it             to             range        [0,      1].                                                                   where         α                       and         β                        are            two            hyperparameters          to            adjust            the            weights
         The          ultimate          goal          of          edge         feature          learning          is          minimizing          the                                                                                                            of             each             part.             Our             goal             is             to              minimize                   L  sum  .             For             this             purpose,
reconstruction                 error                  between                  th  e                 ﬁnal                  feature                 representation                                                                                                we                use                the                back                propagation               (BP)                algorithm               with                stochastic
and             input.             The             reconstruction            loss             is             computed             as                                                                                                                             gradient                descent                to                 train                the                 overall               framework.               The                overall
                                                                                  L   e          =          |   Z       −                           H   e   |2                                                                                                   architecture             of             GLAD              is             summarized             in             Algorithm             1.
                                                                                                                                                     F   .                                                                                                                                                                                                                                                                                                         (14)
By                 minimizing                 the                distances                 between                 the                 reconstructed                rep-                                                                                         F.              Model              Complexity              Analysis
resentations             and              the             original             input,              we              can              get              the              ﬁnal              edge                                                                               The             overall             framework            contains             three              critical             parts,              that             is,
feature             representations.                                                                                                                                                                                                                             node         feature         learning,         edge          feature         learning,         and          classiﬁcation.
                                                                                                                                                                                                                                                                 The                      node                      representation                        H   n                           is                       generating                      by                       maximizing
D.               Anomalous             Citation               Identiﬁcation                                                                                                                                                                                      mutual         information         in          an          unsupervised         manner,         whose          training
                                                                                                                                                                                                                                                                 complexity            is                 O  (cn   n n      In    dn  (   N   p       +           Nn  )),where                N   p                and                 Nn                rep-
         Now                         we                         have                         obtained                        node                        representations                        and                         edge                                 resent              the               number              of              positive              samples               and               negative              samples,
representations                             through                              the                               process                              of                               feature                              learning.                          respectively.             cn                is             the             sample            size             (the            number            of             nodes          | V    |).
As                mentioned               previously,               we                formulate               the                task                as                a                binary                                                                    In                 represents             the              iteration              times.              n n                 is             the             dimension             of             node
classiﬁcation            task.            Accordingl y,           we            need            to            train            a            supervised                                                                                                           features              and                 dn                  is               the               layer              dimension.              The              edge              embedding
classiﬁer             to             predict             the             label             for             each             paper            pair.             Speciﬁcally,                                                                                       H   e                is             generated            through            a             deep             autoencoder.            The             complexity
for                each                 unlabeled                paper                pair                    e             =            (     pi  ,            p     j   ),                 the                ﬁnal                output                       of                   generating                   the                    above                   embedding                   matrix                   is                            O  (ce   n e  de     Ie ),
should           be            its            label           (0            or            1).           We            adopt           logistic            regression           (LR)                                                                              where          ce            is         the         size          of         samples         (the         number        of         edges        |   E  |),          n e            is
as              the              classiﬁer.              The             loss              function                L  lr                of             LR              is                                                                                        the           dimension           of           edge           features,              de              is            the           maximum           dimension
                                                              L  lr         =            L        −                    σ             H         ·        W   l         +           B l                                                                                                                                                                                                                           (15)of                     the                     hidden                    layer,                     and                           Ie                         represents                      the                      iteration                      times.
                                                                                                                                                                                                                                                                 The                 classiﬁcation                 process                 adopts                 logistic                 regression                 and                 the
where                H                 connects             node             representations             and              edge             representa-                                                                                                           training          complexity          is              O  (n    ∗            k ),where           n            represents          the           sample
tions.              W   l                  and                B l                  refer              to               the              weight              matrix              and              bias               matrix,                                      size                 and                  k                    is                 the                 feature                dimension,                which                 is                  related                 to                 the
respectively.       σ                       is          the          Sigmoid          activation         function.         For          the          ﬁrst-                                                                                                       embedding            dimensions.             Thus,             the             overall             training             complexity
order             gradient-based            optimization,             we             use             the             Adam             method.                                                                                                                    of           GLAD           is               O  (cn   n n      In    dn  (   N   p     +        Nn  )   +    ce   n e  de     Ie     +        Il    n    ∗            k )            which
                                                                                                                                                                                                                                                                 is             approximate            to                   O  (   In  | V    |dn        +            Ie |   E  |de        +            Il    k |   E  |).
E.              Overall              Reconstruction                                                                                                                                                                                                                                                                                                    V.                                       E XPERIMENTS
         To           achieve          better           performance,         we           jointly           train          node          feature                                                                                                                           In                    this                     section,                    we                    will                    introduce                   the                    datasets                    including
learning,                 edge                 feature                 learning,                    and                  classiﬁcation                  processes.                                                                                               citation           purpose           annotation,          ground-truth          anomalous          citations,
             Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

LIU          et          al.:            DEEP         GRAPH         LEARNING         FOR          ANOMALOUS        CITATION         DETECTION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2551
Fig.           3.                                   Experimental             procedure.
and                 the                 constructed                citation                 network.                We                 validate                 the                 effec-                                                                                                                                                                    TA    B   L    E                     I    V
tiveness           of           GLAD           by            conducting          a            series            of            experiments          with                                                                                                                                   P ROP ORTI ON     OF         C ITATION          P URPOSE    IN             A NNOTATION
discussions                   of                   the                   experimental                  results.                   Fig.                   3                   presents                   the
overall            experiment            procedure.
A.               Datasets
         1)              Citation          Purpose          Annotation:               The          annotation         process         for
citation                  context                 is                   conducted                 by                  postgraduate                 students                  with
a                  good                  background                in                  natural                 language                 processing.                 We                  ask
them           to           determine         the           purpose         of           citing           the          target          reference         by                                                                                             for              neurological             disorders”              published              in                    The                Scientiﬁc               World
choosing         from         the          six          purpose         categories         described         in          Table          II.                                                                                                             Journal                          has                        cited                         124                        papers                        in                         total,                         96                         of                         which                         are
To           estimate           inter-annotator          agreement,          each           citation           context           is                                                                                                                     published                 in                     Cell                    Transplantation .                 Two                  of                  the                  four                  authors
assigned               to               three              different             annotators.              Kappa              coefﬁcient              [51]                                                                                               of              that              paper              are              editors             of                 Cell               Transplantation .             The              impact
is             used              to             measure             the              agreement                                                                                                                                                          factor                of                  Cell                 Transplantation                 has                dropped               by                removing               the
                                                                             K            =                     P  (    A)      −                                P  (   E  )                                                                            reference                    of                     that                     paper.                     We                      also                     crawl                     the                     citation                     cartels
                                                                                                          1      −                                P  (   E  )                                                                                                                                                                                                                                                                                                                                        (17)detection         results          from         previous         research          [52].         Taking         the          paper
where              P  (    A)          represents         the          relative         observed        agreement         among                                                                                                                         “Crystal                   structure                  reﬁnement                  with                    SHELXL?”                  published                  in
three             annotators            and                    P  (   E  )              represents            the             hypothetical            proba-                                                                                            Acta                         Crystallographica                        C                           as                        an                        example.                       The                       paper                       has
bility              of              agreement.                                                                                                                                                                                                          received            594             citations            from            the             donor            journals                 IUCrData              and
         In                         our                         task,                          the                          agreement                         among                         three                          annotators                        isActa              Crystallographica              Section              E ,             accounting             for             94%             of             the
K  12              =                0.606,                     K  13              =                 0.477,                and                     K  23              =                0.677,                respectively,                               total                   citations                  in                   2017.                  Removing                  the                   citations                   from                  donor
which          indicates         moderate         and          subs  tantial          agreements.          Besides,                                                                                                                                     journals                  decreases                   the                   JIF                   of                       Acta                    Crystallographica                    C                       by
uncertain            labeled            results            are            discarded.           The            distribution            of            the                                                                                                 22%.            Details             of             the             anomalous           citations            dataset             are             described
purpose            categories            is              shown             in              Table             IV.                                                                                                                                        in             Table             V             in              the             Supplementary            Material.
         2)              Anomalous                       Citation                        Dataset:                              For                       anomalous                      citation                                                                  3)              Microsoft                 Academic                 Search                 Dataset:                        The                 citation                 rela-
detection            in            academic            networks,           ground-truth          anomalous           cita-                                                                                                                              tionships        of         anomalous        papers        are         obtained         from        a         widely         used
tions                     are                     not                     clearly                     deﬁned.                     H  ence,                     we                      artiﬁcially                      create                          bibliographic            dataset,              Micro  soft             Academic             Graph             (MAG). 3               It
a                    dataset                   motivated                   by                   previous                  research                   [52].                   The                   dataset                                              contains            abundant             informatio  n            for             each             paper             such             as             journal
generation             process              is              described             as              follows             in              detail.                                                                                                           name,           publication           year,           authors,           abstract,           citation           relationship,
         We           ﬁrst           collect           the           paper           titles           which           are          reported           as           papers                                                                               and                so                 on.                In                 particular,               it                provides                citation                context,                that                is,
causing                   “journal                  cartels”                  in                   [53].                  Most                   of                  these                   papers                  are                                the                  raw                   text                  where                  the                  citation                   is                   mentioned                  in                   the                   article.
published                  in                   donor                  journals,                  including                  the                        Medical                    Science                                                              We               use              citation              contexts              to               conduct             citation              purpose              classiﬁ-
Monitor                and                The                Scientiﬁc               World                Journal .              These              papers              cite                                                                            cation                process                as                mentioned                in                 Section                IV-B.                We                 match                the
papers        published        in            Cell         Transplantation          at         an         excessively        high
rate.                    For                    example,                    the                    paper                    titled                    “Regenerative                    medicine                                                               3  https://www.openacademic.ai/oag/
            Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

2552                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               IEEE         TRANSACTIONS         ON         NEURAL         NETWORKS         AND         LEARNING         SYSTEMS,          VOL.         33,          NO.         6,          JUNE          2022
                                                                                                                                                                                                                   TA    B   L    E                     V
                                                              P ERFORMANCE                  C OMPARISON   OF             D IFFERENT        A NOMALY              D ETECTION                M ETHODS                BASED     ON            G RAPH          L EARNING
                                                                                                                                                  ON      THE             TASK     OF         A NOMALOUS                  C ITATION          D ETECTION
anomalous         papers          mentioned          in           Table           V           in           the           Supplementary                                                                                                 Furthermore,          we           also            utilize           ROC-AUC           (AUC)           to           evaluate           the
Material            and             construct            the            citation             network            centered            on            them.                                                                                overall             performance            of              all              methods.             AUC              is              calculated              based
Edges             for              anomalous             citations             are              labeled             as              anomalous,             and                                                                         on                   the                   relative                  ranking                   of                   the                   predicted                   probabilities                   of                   all
the            rest            are            normal.            The            features            of            papers           (such            as            abstracts)                                                           instances.
and                 citation                 relationships                (such                 as                 citation                 context)                can                 also
be               obtained              and               calculated               in               this               process.               It               is               important              to                               C.               Baselines
note              that              the               dataset              does              not              contain              some              retracted               papers.                                                            Due             to              the             lack              of             any             existing             baseline             for             the             given             task,
We                      searched                    for                     the                     corresponding                   papers                    on                     the                     Internet                  we            compare           GLAD           with            the            following           state-of-the-art          models
and              then              added              them               back              manually.             Finally,              the              constructed                                                                    of                     GNNs                     because                    our                     feature                    learning                     process                    is                     built                     on
network                     contains                     4718                    citing–cite  d                     relationships,                    of                     which                                                     GNNs.
300             citations             are             anomalous.                                                                                                                                                                                1)                         DeepWalk             [54]:             DeepWalk            uses            the             co-occurrence           rela-
B.               Evaluation              Metrics                                                                                                                                                                                                            tionship                   between                   nodes                  in                   the                   network                  to                   learn                   node
                                                                                                                                                                                                                                                            representations.          The            latent            representations          are           generated
        In          this         article,          the         task          of          anomalous        citation          identiﬁcation         is                                                                                                        by             employing            skip-gram.
formulated           as            a            binary           classiﬁ cation            problem,           that            is,            for           each                                                                                 2)                         Node2vec                               [55]:                               Node2vec                             obtains                             node                             sequences
paper          pair        (     pi  ,            p     j   ),whether       (     pi  ,            p     j   )           is          anomalous         (yes          or          no).                                                                       through               random               walk                in                terms                of                both                DFS                and                BFS.
Therefore,        we          adopt         four         widely          used         metrics         for          classiﬁcation                                                                                                                            Hence,              the              low-dimensional             feature              representations             can
tasks,                   including                  accuracy,                 precision,                  recall,                   and                  F1-score                  to                                                                       comprehensively       preserve        the         similarity         of         DFS         and        BFS
validate           the           performance         of           the           m  odel.           Speciﬁcally,          before          the                                                                                                                neighborhoods.
task         of         anomalous       citation         identiﬁcation,        we         divide        the         training                                                                                                                    3)                         LINE                   [56]:                  LINE                  is                   also                   a                  graph                  embedding                 method
set             into              two             categories.                                                                                                                                                                                               based                     on                     the                     assumption                     of                     neighborhood                   similarity.
        1)                         Positive              sample:             The             positive             sample             set                  P                 contains            all                                                         LINE         uses         BFS         to          construct         neighborhoods.       In                   addition,
                     real             anomalous            paper             pairs             (labeled             as             1).                                                                                                                      LINE             can             also              be             applied             in              weighted             graphs.
        2)                         Negative          sample:           The         negative         sample          set               N              contains         all                                                                       4)                         GraphSAGE                 [26]:                  GraphSAGE                is                 an                 inductive                frame-
                     sample           pairs          that          are           not          anomalous          paper          pairs          (labeled                                                                                                     work,                 which                 can                  use                  vertex                 feature                 information                 (such
                     as             0).                                                                                                                                                                                                                     as              text              attributes)              to               efﬁciently              generate              embedding             for
After         the          classiﬁcation         process,         we          assume          that         set         TP         includes                                                                                                                  nodes.
all                        pairs                        in                              P                            that                        are                        correctly                        classiﬁed                         as                        anomalous5)                         DGI                      [30]:                    DGI                    learns                    node                    representations                   by                    max-
citations,                  and                  FN                  are                  misclassiﬁed                   pairs                  in                       P .Inthesame                                                                       imizing                         mutual                        information                        between                         the                         graph-level
way,                 the                 set                 FP                 involves                the                 misjudged                pairs                 in                        N                      and                 TN          summary                 representation                 and                 the                 local                 patches.                 DGI                  is
is               the              correctly              judged              set               of                     N   .               Apparently,                  N             =             FP       +         TN.                                   an             unsupervised            network            representation            learning            frame-
Hence,             the             indicators             can             be              calculated             as             follows.                                                                                                                    work.
        1)                       Accuracy        =           |TP|+|TN|                                                                                                                                                                          6)                         Multi-layer                perceptron                 (MLP):                MLP               neural               network               is
                                                                       |   P |+|   N  |                         .                                                                                                                                           a                  common                 artiﬁcial                  neural                  network                 (ANN)                  algorithm,
        2)                       Precision        =                                                       |TP|                                                                                                                                              which         consists         of         an          input        layer,         an         output        layer,         and         one
                                                                  |TP|+|FP|       .                                                                                                                                                                         or               more               hidden              layers.               Different              layers              of               MLP              neural
        3)                       Recall        =                                                          |TP|                                                                                                                                              network             are             fully             connected.
                                                        |TP|+|         FN|       .                                                                                                                                                              The                  comparison                  results                   are                  presented                  in                   Fig.                  4.                  We                   also
        4)                       F1-score        =               2       ∗                Precision      ∗                Recall                                                                                                       compare                   GLAD                    with                    state-of-the                   art                    methods                   for                   anomaly
                                                                      Precision      +        Recall                              .                                                                                                    detection             based             on             graph             learning.
            Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

LIU          et          al.:            DEEP         GRAPH         LEARNING         FOR          ANOMALOUS        CITATION         DETECTION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2553
                                                                                                                                                                                                                              TA    B   L    E                     V    I
                                                                                                                                                                          S UMMARY    OF                 VARIANTS    FOR                 C OMPARISON
         1)                         GDN                 [57]:                GDN                is               a                new               GNN                architecture               designed
                      for                        network                        anomaly                        detection                        with                         limited                        labeled
                      data.               Speciﬁcally,              the               authors              also               propose              meta-GDN,
                      which                integrates               cross-network               meta-learning               to                detect
                      anomalies             with             few              labeled             instances.
         2)                         OCGNN                          [58]:                        OCGNN                        is                        a                        one-class                        classiﬁcation
                      framework          that           contains          a           series           of           algorithms          for           graph
                      anomaly         detection.         OCGNN           co mbines         the          powerful         rep-
                      resentation          ability          of          GNNs          along          with          the          classical          one-
                      class                objective.               In               this                work,               we                compare               GLAD                with
                      OC-GCN,              OC-GAT,              and              OC-SAGE.
         3)                         CARE                          [59]:                         CARE                         enhances                        the                        GNN                         aggregation
                      process            with             three             unique            modules            against            camouﬂages                                                                                                       Fig.                   4.                                                           Performance                    comparison                    of                    different                     methods                    on                    the                   task                    of
                      including            a            label-aware           similarity            measure,           a            similarity-                                                                                                      anomalous            citation             detection.
                      aware                     neighbor                     selector,                     and                     a                      relation-aware                     neigh-                                                  including:             1)              comparison             against             baselines;              2)              comparison             of
                      bor               aggregator.              In               this                work,               we                compare              GLAD                with                                                            GLAD             variants;             and              3)              parameter             sensitivity.
                      various               forms               of                CARE,                including               CARE-Att,                CARE-                                                                                                 1)              Comparison         Against         Baselines:                The         results         of         comparing
                      Weight,              CARE-Mean,             and             CARE-GNN.                                                                                                                                                          GLAD           with           all           baselines          mentioned          in            Section           V-C           are           shown
         Table                VI                 lists                 the                results                of                 comparing               GLAD                with                 base-                                           in               Fig.               4.               From               the               results,               we               can               observe              that               our               method
lines.                   For                   the                    sake                   of                   fair                    comparisons                  with                    GLAD                   which                          GLAD                   outperforms                 all                   the                   baselines                   in                   terms                   of                   Accuracy,
considers                  both                  node                  attributes                  and                  edge                 attributes,                  we                  con-                                                   Precision,                     and                      F1-score,                     which                      shows                      the                     effectiveness                     of
catenate                      the                      raw                      edge                      attribute                      matrix                        Z                       to                      learned                     nodeGLAD.                  Although                 the                  results                 of                  the                  baselines                 on                  Recall                  are
embeddings                  of                    the                   methods                   that                   ignore                   the                    edge                   attributes                                           higher               than               the               proposed              method,               their               Precisions               are               lower.
such              as             Deepwalk,             LINE,              Node2vec,            and             DGI,              that             is,                  H          ←                                                                  Higher                   Precisions                   indicate                   that                   when                   we                   use                   the                   model                   to
[   H   n  ;         Z ].           Furthermore,           we            also            compare           GLAD            with            different                                                                                                 determine               that                a                citation                is                 an                anomalous               citation,                then                it                 is
variants             to              examine             the              learning             efﬁcacy             of              each             module             and                                                                           highly               likely                to                be                anomalous.               We                do                not                expect               to                ﬁnd                all
eliminate                  the                   effect                  of                   each                   module.                  Table                  VI                   presents                  the                              anomalous                  citations                   but                   hope                  that                   the                   identiﬁed                   anomalous
details                    of                    each                    variant.                    First,                     we                     only                    utilize                    node                    feature            citations           are            as            accurate           as            possible.            In            addition,           the            proposed
learning                process                mentioned               in                 Section                IV-A                 (GLAD-N)                and                                                                                    framework          has           achieved          the           best           results           on           the           comprehensive
edge          feature          learning         mentioned         in          Section          IV-C         (GLAD-E)          to                                                                                                                     indicator               F1-score.               In                all                the                comparison               methods,               DGI                and
detect         anomalous        citations.         To         examine        whether         node        and         edge                                                                                                                            GraphSAGE               have                better                performances,               which                illustrates                that
features          are           useful,          we           use           random          vectors          to           replace           the           node                                                                                       learning                     attribute                     features                     is                     more                     effective                     in                     the                     task                      of
features              in               the              process              of               node              feature              learning              (GLAD-RN)                                                                                 anomalous           citation            detection.           The            comparison           between            GNNs
and              the              edge              features              in              the               process             of              edge              feature              learning                                                      and                   MLP                  shows                   that                   feature                  learning                  based                   on                   neural                  net-
(GLAD-RE).                   We                    then                   remove                  the                   loss                   function                   of                   training                                              works             can              improve            the              perform ance            of              the             model.             Compared
nodes        features             L   n            (GLAD-EL)        and         the         loss         function        of         training                                                                                                         with                      directly                      inputting                      features,                      learning                      features                      ﬁrst                       can
edge                   features                        L   e                      in                    the                   ﬁnal                   loss                   function                   (GLAD-NL)                   to                effectively                    improve                    the                     perfo rmance                   of                     anomalous                    citation
observe                       the                       model                       performance                         without                       auxiliary                       training                                                       detection.
objective.                        Finally,                        we                         also                         use                         the                        edge                        feature                        learning          The                      results                      of                      comparing                     G LAD                       with                       all                       baselines                      for
module            (based            on            the             autoendoder)           to             learn            the             node            features                                                                                    anomaly                  detection                   mentioned                  in                   Section                   V-C                   are                   presented
and             connect             the             embeddings            with              edge             representations            as              the                                                                                          in                Table               V.                Among               all               GNN                baselines               in                Table                V,               in                terms
ﬁnal             input            (GLAD-EE).                                                                                                                                                                                                         of                  Accuracy,                 Precision,                  and                  F1-score,                  our                  approach                  outper-
D.               Results              and             Analysis                                                                                                                                                                                       forms               all                the                other                compared               methods               by                a                signiﬁcant                mar-
                                                                                                                                                                                                                                                     gin.                  It                  demonstrates                 that                  GLAD                 can                  better                  detect                 abnormal
         The                        experimental                       results                        are                        presented                       together                       with                        a                        edges                    than                    other                    methods.                    GLAD                    is                    capable                    of                    extracting
number            of            case             studies            from            the             following            three            perspectives,                                                                                              comprehensive            knowledge             across             the             citation             network,             which
            Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

2554                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               IEEE         TRANSACTIONS         ON         NEURAL         NETWORKS         AND         LEARNING         SYSTEMS,          VOL.         33,          NO.         6,          JUNE          2022
                                                                                                                                                                                                                          TA    B   L    E                     V    I    I
                                                                                                  C ITATION         P URPOSE          C LASSIFICATION               R ESULTS     OF            CPU     U SING          D IFFERENT        C LASSIFIERS
                                                                                                                                                                                                                         TABLE           VIII
                                                                                                                                                  P ERFORMANCE    OF                GLAD     W   ITH        D IFFERENT        B AT   C   H         S IZE
                                                                                                                                                                                                                           TA    B   L    E                     I    X
                                                                                                                              P ERFORMANCE    OF                GLAD     W   ITH        D IFFERENT        E MBEDDING                D IMENSION
                                                                                                                                                                                                                            TA    B   L    E                     X
                                                                                                                                           P ERFORMANCE    OF                GLAD     W   ITH        D IFFERENT        L EARNING             R  AT   E
further                        enhances                        the                         detectio  n                        performance                       on                        the                         target                                                                                                                        TA    B   L    E                     X    I
network.                  Besides,                  experimental                 results                   have                  shown                   that                  the                                                                            N UMBER     OF              H IDDEN           U NITS     IN           E ACH        L  AY   E   R            F    O   R        AUTOENCODER
proposed                  model                   performs                  better                   than                   the                   OC-based                   model.
One-class             classiﬁcation-based             methods             attempt             to              model            nor-
mal                    examples                   and                   classify                    new                    examples                   as                    either                    normal
or                      abnormal.                     In                      this                       article,                      we                       aim                      to                      detect                      anomalous
citations               through              analyzing               the               contexts               and                give               an               explicit
deﬁnition            of            anomalous            citation            shown            in            Section            III.            In            real-
world             scenarios,             there             are             different            kinds            of             citation             purposes
between        two         papers.        In         other        words,         it         is         impossible        to         learn         all
features        of         normal        citations        due        to         the         complexity        and         diversity                                                                                                                       2)              Comparison                      of                       GLAD                      Variants:                           A                      key                     ingredient                     of
of                 normal               samples.                Therefore,               the                one-class                classiﬁer                 is                 not                                                            GLAD           is           the           feature           learning          of           both           node          attributes          and           edge
suitable             for             detecting             anomalous            citations.                                                                                                                                                       attributes.                    To                    validate                    the                    effectiveness                   of                    this                    mechanism,
            Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

LIU          et          al.:            DEEP         GRAPH         LEARNING         FOR          ANOMALOUS        CITATION         DETECTION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2555
                                                                                                                                                                                                                       TA    B   L    E                     X    I    I
                                                          P ERFORMANCE    OF                GLAD     W   ITH        D IFFERENT        H IDDEN           L AYERS     AND              H IDDEN            U NITS     IN          E DGE          F EATURE             L EARNING
                                                                                                                                                                                                                                              with           respect           to           the           following           parameter          settings:           1)           batch           size;
                                                                                                                                                                                                                                              2)         embedding        dimension;       3)         learning        rate;         4)         number        of         hidden
                                                                                                                                                                                                                                              layers         and         units         in          the         autoencoder.        For         each         parameter         setting,
                                                                                                                                                                                                                                              we                     ﬁx                    other                    parameters                    and                    try                    ten                    different                    splits                     of                    the
                                                                                                                                                                                                                                              training                and                 testing.                The                 results                 are                 reported                as                the                 average
                                                                                                                                                                                                                                              performance             of              the              ten               experiments.              The              results              also               include
                                                                                                                                                                                                                                              conﬁdence            intervals             of              each             index.
                                                                                                                                                                                                                                                       First                  of                   all,                   Table                  VII                   presents                  the                  citation                  purpose                 clas-
                                                                                                                                                                                                                                              siﬁcation              results              of              CPU              using              different             classiﬁers,              including
                                                                                                                                                                                                                                              SVM,           logistic           regression,          and           Naive           Bayes.           By           comparing          the
                                                                                                                                                                                                                                              results                 of                 experiments,                we                 can                  ﬁnd                 that                 the                 SVM                 performs
                                                                                                                                                                                                                                              better               in               the              task               of               citation               purposes              classiﬁcation.               Besides,
Fig.        5.                      Performance        comparison        of        different         variants         on        the       task        of       anomalous                                                                      one           of           the           edge           features           CP           refers           to           that           whether           the           purpose
citation             detection.                                                                                                                                                                                                               of           the           citation           is           clear           or           not.           In           this           article,           we           also           try           binary
                                                                                                                                                                                                                                              classiﬁcation.                     The                    classiﬁcation                     precision                    can                     be                    improved
here               we               examine              the               performance              of               GLAD               with               only               node                                                            to             78.4%.
feature                  learning                  process                  or                  with                   only                   edge                  feature                  learning                                                           a)               Batch            size:                 Table            VIII           lists            the           experimental          results           of
process.                     From                     Fig.                     5,                     we                     can                     clearly                     observe                    that                     GLAD     GLAD’s               performance              when               hyperparameter             batch               size               is                set
is                     36.2%                     and                     44.4%                     higher                     than                     GLAD-N                     and                     GLAD-E,                             doubled          increasing           from           1            to           64.           We            can           observe           that           the           per-
respectively,            on            Precision.            GLAD            is             21.7%            and            26.3%            higher                                                                                           formance           of            GLAD            is            relatively            stable            within            a            large           range           of
than         GLAD-N         and         GLAD-E         in         terms         of         F1-score,        respectively.                                                                                                                     batch           sizes.            In           addition,           GLAD            achieves          the            best           performance
In            general,           the            performance          of            GLAD            is            better           than            GLAD-N                                                                                      in               Accuracy,              Precision,               F1-score,               and               Recall               when               the               batch
and                   GLAD-E,                   which                   suggests                   that                   utilizing                   both                   node                  and                                        size          is          set           to          4.          This          observation         demonstrates         that          larger         batch
edge             attributes             in              feature             learning             processes            is              necessary,            and                                                                               sizes             don’t             necessarily             lead             to              better             performance.
GLAD            can            facilitate            them            well            to            help            detect            anomalous           cita-                                                                                                  b)              Embedding                    dimension:                           We                    empirically                   evaluate                   the
tions.              Besides,               if               we              use              random              vectors             for              feature              learning                                                           impact                     of                     edge                     embedding                    dimension.                    Table                     IX                     illustrates
(GLAD-RN              and              GLAD-RE),             the              performances            will              be              better                                                                                                the            performance          of            GLAD            when            altering            the            edge            embedding
than                  that                  of                  GLAD-N                  and                  GLAD-E                  on                  F1-score,                  but                  there                                dimension                   from                  32                   to                   512.                  We                    can                   conclude                  that                   the                   per-
is                    still                   a                    certain                   gap                   compared                  with                    GLAD.                    The                   proposed                  formance           of             GLAD            slightly             ﬂuctuates            within             a             large            range            of
features            are             more            effective            than            random            vectors.            If             we             do             not                                                               edge          embedding         dimensions,          and          the           performance         drops          when
minimize                 the                 loss                  function                 of                 feature                 learning                 (but                 still                  use                               the               edge               embedding               dimension               is                too                small.               The               model               can
these             features),            the             performan  ce           of            the             models            will             be            further                                                                        reach             a             relatively             better             state             when             the             embedding            dimension
improved,              but               will                not               exceed               GLAD.                Without               the                auxiliary                                                                   is             set              to             512.
training              objectives,              the              model              performs              worse.               Finally,              using                                                                                                       c)              Learning          rate:               Learning         rate          controls         the          update         speed
the               autoencoder             to               learn               the               node              features              and               edge              features                                                         of           GLAD.           Table           X           shows           how           different          values           of           the           learning
at                 the                 same                  time                 (GLAD-EE),                the                 best                 results                 are                 obtained                                     rate                 inﬂuence                 the                  performan ce                of                  GLAD.                  The                  results                  show
in                  all                 variants.                 This                 shows                 the                 necessity                  of                 feature                 learning                               that                 the                 performance                 drops                 wh  en                  the                  learning                  rate                  is                  either
for             both            edges            and             nodes.            However,            in             the             task             of             anomalous                                                               too                large               or                too                small.                We                can                see                that                the                proposed               model
citation              detection,              the              performance             of              node             learning             with               the                                                                           performs             best             in              Accuracy,             Precision,              and              F1-score             when             the
autoencoder                  is                   not                   as                   good                   as                   that                   of                   DGI.                   In                   conclusion,  learning                 rate                  is                  set                  to                  0.01.                 Besides,                  the                  value                  of                  Recall                  is
although           some            indicators           of            som  e            variants            exceed            GLAD,            the                                                                                            relatively            stable             despite             the             learning            rate             varies            from             0.001            to
overall        performance       of          GLAD         is         better         than         that         of         all         variants,                                                                                                0.1.
which             indicates            that             the             proposed           features            are             effective            and             it                                                                                          d)              Number                  of                  hidden                  layers:                         To                  investigate                 the                  impact
is             necessary             to              conduct            feature             learning             processes.                                                                                                                   of                 different                sizes                 of                 neural                networks                on                 the                performance               of
         3)               Parameter                      Sensitivity:                         We                     conduct                    a                     series                     of                    exper-                 GLAD,         we         set         eight        different        experimental       groups        by        adjusting
iments                       to                       examine                      how                      different                     parameters                      inﬂuence                      the                                   the         numbers        of         hidden        layers        and         hidden        units         in         the         process        of
performance            of             GLAD.             We             report            the             performance           of             GLAD                                                                                            edge          feature          learning.          The          detailed          settings          of          the          eight          groups
            Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

2556                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               IEEE         TRANSACTIONS         ON         NEURAL         NETWORKS         AND         LEARNING         SYSTEMS,          VOL.         33,          NO.         6,          JUNE          2022
can                   be                   seen                    in                   Table                    XI.                   According                  to                    Table                   XII,                   we                    can    [12]                     F.              Xia,               W.              Wang,               T.              M.               Bekele,                and                H.              Liu,               “Big                scholarly                data:                A
conclude                   that                    GLAD                    performs                   better                    when                    the                    number                   of                                                                            survey,”            IEEE            Trans.             Big            Data,            vol.           3,           no.            1,           pp.           18–35,           Mar.            2017.
hidden         layers        is         set          to         2          (Group        5).         Furthermore,        increasing         the                                                                                                                     [13]                     X.          Kong,          J.          Zhang,          D.          Zhang,          Y.          Bu,          Y.          Ding,          and          F.          Xia,          “The          gene          of
                                                                                                                                                                                                                                                                                      scientiﬁc             success,”          ACM            Trans.            Knowl.             Discovery             Data,            vol.           14,           no.           4,
number          of           hidden          layers           does          not          always           improve          the           metrics.                                                                                                                                     pp.           1–19,           2020.
                                                                                                                                                                                                                                                                    [14]                     K.            Moustafa,             “Aberration             of            the            citation,”          Accountability               Res.,           vol.            23,
                                                                                      VI.                C   ONCLUSION                                                                                                                                                                no.           4,           pp.            230–244,           Jul.           2016.
         In                    this                    work,                    we                    aim                    to                    solve                    the                    problem                   of                    anomaly          [15]                     C.                      Bartneck                      and                      S.                      Kokkelmans,                      “Detecting                       h-index                      manipulation
                                                                                                                                                                                                                                                                                      through          self-citation           analysis,”       Scientometrics,           vol.         87,         no.         1,         pp.           85–98,
detection                     in                      academic                     citation                      networks.                     For                      this                      purpose,                                                                            Apr.           2011.
we                develop               a                novel               deep                graph               learning                framework,               namely                                                                                        [16]                     Z.                      Corbyn,                       “An                       easy                       way                       to                       boost                       a                       paper’s                       citations,”                     Nature,
GLAD,         to         model        and         identify        anomalous       citations.         We         propose                                                                                                                                                               Aug.           2010,            doi:          10.1038/news.2010.406.
                                                                                                                                                                                                                                                                    [17]                     M.            Mimouni,             M.            Ratmansky,             Y.            Sacher,             S.            Aharoni,             and            A.            Mimouni-
to                     incorporate                    the                     node                    feature                    learning                    process                     and                    edge                                                                  Bloch,              “Self-citation               rate              and             impact              factor              in             pediatrics,”        Scientomet-
feature           learning          process           into           the           framework.          Thus,           GLAD           can                                                                                                                                             rics,           vol.            108,           no.            3,           pp.           1455–1460 ,           Sep.           2016.
make          full          use          of          information         from          node          attributes          and          network                                                                                                                       [18]                     J.        P.        A.        Ioannidis,         “A         generalized          view         of         self-citation:           Direct,         co-author,
                                                                                                                                                                                                                                                                                      collaborative,           and         coercive          induced         self-citation,”      J.          Psychosomatic          Res.,
structures.               Particularly,               we               propose               an                effective               algorithm               to                                                                                                                     vol.           78,            no.           1,           pp.           7–11,           2015.
capture           semantic           information          of            citation           contexts.           We            conduct                                                                                                                                [19]                     G.                            Franck,                            “Scientiﬁc                             communication—A                              vanity                             fair?”                         Science,
comprehensive                    experiments                    to                     validate                     the                     performance                    of                                                                                                         vol.           286,            no.           5437,            pp.           53–55,           Oct.            1999.
                                                                                                                                                                                                                                                                    [20]                     M.              Gori,              G.             Monfardini,               and              F.              Scarselli,               “A              new              model               for              learning
GLAD.             Experimental            results             demonstrate            the             effectiveness            of                                                                                                                                                      in           graph           domains,”            in          Proc.           IEEE          Int.           Joint            Conf.           Neural           Netw.,vol.2,
the             proposed            anomalous            citation             detection             framework            as             well                                                                                                                                          Jul.           2005,            pp.           729–734 .
as             the             proposed            citation             purpose             classiﬁcation              algorithm.                                                                                                                                   [21]                     F.               Xia                 et                 al.,                “Graph                learning:                 A                survey,”               IEEE                 Trans.                 Artif.                  Intell.,
                                                                                                                                                                                                                                                                                      vol.           2,           no.            2,           pp.           109–127,           Apr.            2021.
         However,             we              only              consider             node             attributes              and              connection                                                                                                           [22]                     F.         Scarselli,           M.          Gori,          A.         C.          Tsoi,         M.          Hagenbuchner,           and          G.         Monfardini,
information                 (adjacency                 matr  ix)                 in                  the                  node                 feature                 learning                                                                                                       “The           graph            neural            network            model,”          IEEE           Trans.            Neural            Netw.,           vol.           20,
process.                  More                  network                   structural                  information                  such                   as                   pref-                                                                                                  no.           1,           pp.            61–80,           Dec.            2009.
                                                                                                                                                                                                                                                                    [23]                     C.          Gallicchio           and          A.         Micheli,           “Graph          echo          state          networks,”          in    Proc.          Int.
erential                 attachment                 and                 Admai c-Adar                index                 could                 be                 incor-                                                                                                             Joint             Conf.             Neural             Netw.             (IJCNN),            Jul.            2010,           pp.            1–8.
porated.              Besides,               the              construction              of               anomalous             datasets              and                                                                                                            [24]                     Y.                    Li,                    D.                    Tarlow,                    M.                    Brockschmidt,                     and                    R.                    Zemel,                     “Gated                     graph
the            labeling             of             citation             purpose           are             time-consuming.           It            would                                                                                                                               sequence                neural                networks,”                in              Proc.                4th                Int.                 Conf.                Learn.                Represent.
                                                                                                                                                                                                                                                                                      (ICLR),           2016,            pp.           1–20.
be             an              interesting             topic             to              generate             such              ground-truth            data             for                                                                                        [25]                     J.            Atwood             and            D.            Towsley,            “Diffusion-convolutional                neural             networks,”
further             research.                                                                                                                                                                                                                                                         in            Proc.            Adv.            Neural             Inf.            Process.             Syst.,           2016,            pp.           1993–2001 .
                                                                                                                                                                                                                                                                    [26]                     W.               L.              Hamilton,                R.               Ying,               and                J.              Leskovec,                “Inductive                 representation
                                                                                 ACKNOWLEDGMENT                                                                                                                                                                                       learning          on         large          graphs,”          in       Proc.          Adv.          Neural          Inf.          Process.          Syst.,          Annu.
                                                                                                                                                                                                                                                                                      Conf.             Neural             Inf.            Process.             Syst.,           vol.           30,            2017,           pp.            1024–1034.
         The                 authors                 are                 grateful                to                  master                 students                 at                 the                 Dalian                                                  [27]                     J.                 Chen,                  T.                  Ma,                  and                  C.                  Xiao,                  “FastGCN:                  Fast                  learning                  with                  graph
University            of            Technology           who            have            helped            with            experimental                                                                                                                                                convolutional                 networks                via                importance                sampling,”                in           Proc.                Int.                Conf.
dataset             preparation.                                                                                                                                                                                                                                                      Learn.            Represent.,            2018,            pp.           1–15.
                                                                                                                                                                                                                                                                    [28]                     K.                     Xu,                     W.                     Hu,                     J.                     Leskovec,                      and                     S.                     Jegelka,                      “How                     powerful                      are
                                                                                                R EFERENCES                                                                                                                                                                           graph               neural                networks?”                in               Proc.                Int.                Conf.                Learn.                Represent.,               2019,
                                                                                                                                                                                                                                                                                      pp.           1–17.
   [1]                     S.             Fortunato              et               al.,              “Science               of              science,”             Science,               vol.             359,             no.              6379,                    [29]                     L.            Wu            et              al.,             “Learning             the             implicit             semantic             representation              on             graph-
                  2018,            Art.           no.           eaao0185.                                                                                                                                                                                                             structured                  data,”                  in                Proc.                  26th                  Int.                  Conf.                   Database                  Syst.                   Adv.                  Appl.
   [2]                     X.           Kong,           Y.           Shi,           S.           Yu,           J.           Liu,           and           F.           Xia,           “Academic            social            networks:                                                 (DASFAA),           vol.           12681,            2021,            pp.           3–19.
                  Modeling,              analysis,              mining             and              applications,”          J.              Netw.              Comput.              Appl.,                                                                          [30]                     P.                   Velickovic,                      W.                   Fedus,                    W.                    L.                   Hamilton,                     P.                    Liò,                    Y.                    Bengio,                     and
                  vol.            132,           pp.           86–103 ,           Apr.            2019.                                                                                                                                                                               R.            D.            Hjelm,                 “Deep                 graph                 InfoMax,”                 in               Proc.                  7th                  Int.                  Conf.                   Learn.
   [3]                     W.            Wang              et              al.,             “Attributed              collaboration               network              embedding              for             aca-                                                                     Represent.,            2019,           pp.           1–17.
                  demic            relationship             mining,”         ACM            Trans.            Web,            vol.           15,           no.           1,           pp.            1–20,                                                          [31]                     Y.                 Ren                 and                 B.                 Liu,                 “Heterogeneous                   deep                 graph                 InfoMax,”                  in              Proc.
                  Feb.            2021.                                                                                                                                                                                                                                               Workshop          Deep          Learn.          Graphs,          Methodol.           Appl.          Co-Located           34th          AAAI
   [4]                     X.          Bai,          H.          Pan,          J.          Hou,          T.          Guo,          I.          Lee,          and          F.          Xia,          “Quantifying           success                                                    Conf.             Artif.             Intell.,            2020,            pp.           1–9.
                  in                 science:                  An                 overview,”                 IEEE                 Access,                 vol.                8,                 pp.           123200–123214 ,                                      [32]                     K.          Tu,          P.         Cui,          X.          Wang,          P.          S.         Yu,          and          W.          Zhu,          “Deep           recursive           network
                  2020.                                                                                                                                                                                                                                                               embedding            with            regular            equivalence,”             in        Proc.            24th            ACM            SIGKDD            Int.
   [5]                     C.          G.          Prabha,           “Some          aspects           of          citation           behavior:           A          pilot           study          in          busi-                                                                  Conf.             Knowl.             Discovery             Data            Mining,            Jul.           2018,            pp.           2357–2366 .
                  ness          administration,”          J.           Amer.          Soc.           Inf.           Sci.,          vol.         34,          no.         3,          pp.           202–206 ,                                                        [33]                     W.          Yu          et           al.,          “Learning           deep           network           representations            with           adversarially
                  May            1983.                                                                                                                                                                                                                                                regularized                      autoencoders,”                      in                  Proc.                     24th                      ACM                     SIGKDD                      Int.                     Conf.
   [6]                     M.          Dong,          B.          Zheng,           N.          Q.          V.          Hung,          H.          Su,          and           G.          Li,          “Multiple            rumor                                                      Knowl.             Discovery             Data            Mining,            Jul.           2018,            pp.           2663–2671 .
                  source         detection          with         graph         convolutional           networks,”          in    Proc.         28th          ACM                                                                                                    [34]                     T.             Ma,             J.             Chen,              and             C.             Xiao,             “Constrained               generation              of             semantically
                  Int.             Conf.             Inf.            Knowl.             Manage.,            Nov.           2019,           pp.           569–578 .                                                                                                                    valid               graphs              via               regularizing                variational                autoencoders,”                in            Proc.               32nd
   [7]                     Y.-J.            Lu            and             C.-T.            Li,            “GCAN:            Graph-aware              co-attention              networks             for                                                                               Int.             Conf.             Neural            Inf.             Process.            Syst.,            2018,           pp.            7113–7124.
                  explainable              fake             news             detection              on             social             media,”             in        Proc.             58th             Annu.                                                        [35]                    A.             Bojchevski,               O.             Shchur,              D.             Zügner,              and              S.             Günnemann,              “NetGAN:
                  Meeting              Assoc.            Comput.             Linguistics,            2020,            pp.           505–514 .                                                                                                                                         Generating           graphs          via          random          walks,”          in       Proc.           Int.           Conf.           Mach.           Learn.,
   [8]                     T.               Bian                 et                 al.,                “Rumor                detection                 on               social                media                with                bi-directional                                2018,            pp.           610–619.
                  graph            convolutional             networks,”            in        Proc.           AAAI          Conf.           Artif.           Intell.,            2020,                                                                               [36]                     J.       Zhang,        X.       Shi,        J.        Xie,        H.        Ma,        I.       King,        and        D.-Y.       Yeung,        “GaAN:        Gated
                  vol.            34,           no.           1,           pp.           549–556 .                                                                                                                                                                                    attention             networks            for            learning             on            large            and            spatiotemporal             graphs,”            in
   [9]                     A.          Li,           Z.          Qin,           R.           Liu,          Y.           Yang,           and          D.           Li,          “Spam           review           detection            with                                             Proc.            Conf.             Uncertainty             Artif.             Intell.,            2018,           pp.            1–11.
                  graph         convolutional           networks,”         in       Proc.         28th          ACM         Int.          Conf.          Inf.          Knowl.                                                                                       [37]                     S.         Guo,         Y.         Lin,         N.         Feng,         C.         Song,         and          H.         Wan,          “Attention          based          spatial-
                  Manage.,            Nov.           2019,            pp.           2703–2711 .                                                                                                                                                                                       temporal              graph              convolutional               networks              for             trafﬁc              ﬂow             forecasting,”              in
[10]                     S.        Yu,        F.        Xia,         Y.        Sun,        T.        Tang,        X.        Yan,         and         I.        Lee,        “Detecting          outlier          pat-                                                                  Proc.           AAAI           Conf.            Artif.            Intell.,            2019,            vol.           33,            no.           1,           pp.           922–929 .
                  terns         with         query-based          artiﬁcially          generated          searching          conditions,”  IEEE                                                                                                                     [38]                     D.                    Yu,                    R.                    Zhang,                    Z.                    Jiang,                    Y.                    Wu,                    and                    Y.                    Yang,                    “Graph-revised
                  Trans.            Computat.             Social             Syst.,           vol.           8,           no.            1,           pp.           134–147 ,           Feb.            2021.                                                                         convolutional             network,”             in          Proc.            Joint             Eur.            Conf.             Mach.             Learn.             Knowl.
[11]                     F.          Ugo,          D.          S.          Alfredo,           P.          Francesca,           Z.          Paolo,           and           P.          Francesco,           “Using                                                                     Discovery             Databases,            vol.           12459,            2020,            pp.           378–393 .
                  generative                 adversarial                networks                for               improving                classiﬁcation                 effective-                                                                                 [39]                     T.          Zhao,           Y.           Liu,           L.           Neves,           O.          Woodford,           M.           Jiang,           and           N.           Shah,           “Data
                  ness               in                credit                card                fraud                detection,”              Inf.                Sci.,               vol.               479,               pp.           448–455 ,                                  augmentation          for        graph        neural         networks,”         in     Proc.       35th        AAAI       Conf.        Artif.
                  Apr.            2019.                                                                                                                                                                                                                                               Intell.,            2021,           pp.            1–15.
             Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

LIU          et          al.:            DEEP         GRAPH         LEARNING         FOR          ANOMALOUS        CITATION         DETECTION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2557
[40]                    X.                    Zhang                     and                    M.                    Zitnik,                     “GNNGUARD:                     Defending                     graph                     neural                                                                                                                     Feng                          Xia                       (Senior                    Member,                    IEEE)                   received                       the
                  networks           against           adversarial            attacks,”            in        Proc.           Adv.           Neural           Inf.           Process.                                                                                                                                                                               B.Sc.             and              Ph.D.             degrees              from             Zhejiang              University,
                  Syst.,            vol.           33,           2020,            pp.           1–13.                                                                                                                                                                                                                                                              Hangzhou,            China,            in            2001            and            2006,            respectively.
[41]                     X.                  Wang,                  M.                  Zhu,                  D.                  Bo,                  P.                 Cui,                  C.                  Shi,                  and                  J.                 Pei,                  “AM-GCN:                                                         He                           was                           a                           Full                           Professor                           and                           an                           Associate
                  Adaptive                 multi-channel                 graph                convolutional                  networks,”                 in            Proc.                26th                                                                                                                                                                    Dean        (Research)         with        the        School        of        Software,        Dalian
                  ACM              SIGKDD               Int.              Conf.               Knowl.               Discovery               Data              Mining,              Aug.             2020,                                                                                                                                                           University                       of                     Technology,                       Dalian,                       China.                       He                      is
                  pp.           1243–1253 .                                                                                                                                                                                                                                                                                                                        an                     Associate                      Professor                      and                      a                     former                      Discipline
[42]                     J.             Liu              et               al.,             “Shifu2:              A             network               representation               learning               based              model                                                                                                                                  Leader           (IT)           with           the           School            of           Engineering,            IT           and
                  for        advisor-advisee          relationship          mining,”     IEEE        Trans.        Knowl.          Data         Eng.,                                                                                                                                                                                                              Physical               Sciences,               Federation               University               Australia,
                  vol.            33,           no.           4,           pp.           1763–1777 ,           Apr.            2021.                                                                                                                                                                                                                               Ballarat,         VIC,        Australia.        He       has        published         two        books
[43]                     L.            Wang,             J.            Ren,             B.            Xu,             J.            Li,            W.            Luo,            and             F.            Xia,             “MODEL:            Motif-                                                                                                          and            over            300            scientiﬁc             papers             in            international              jour-
                  based           deep            feature            learning            for           link           prediction,”        IEEE           Trans.            Computa.                                                                                           nals                and                conferences.                 His                research                interests                 include                data                science,                 artiﬁcial
                  Social             Syst.,            vol.           7,           no.            2,           pp.           503–516,           Apr.            2020.                                                                                                         intelligence,            graph          learning,           and          systems          engineering.            He         is          a          Senior          Member
[44]                     L.             Ouyang,              Y.             Zhang,              and              Y.             Wang,             “Uniﬁed              graph              embedding-based                                                                     of           the            Association             for           Computing             Machinery            (ACM).
                  anomalous                    edge                    detection,”                     in                 Proc.                     Int.                     Joint                     Conf.                     Neural                     Netw.
                  (IJCNN),            Jul.           2020,            pp.           1–8.
[45]                     L.        Zheng,         Z.        Li,        J.         Li,        Z.        Li,         and         J.        Gao,         “AddGraph:         Anomaly          detection
                  in          dynamic          graph          using          attention-based            temporal          GCN,”         in     Proc.         IJCAI,
                  Aug.           2019,            pp.           4419–4425.
[46]                     Q.               Le               and                T.               Mikolov,                “Distributed                 representations                  of               sentences                 and
                  documents,”             in           Proc.            Int.            Conf.             Mach.             Learn.,           2014,            pp.           1188–1196 .
[47]                     A.           Abu-Jbara,           J.          Ezra,           and           D.          Radev,            “Purpose           and           polarity            of           citation:
                  Towards          NLP-based          bibliometrics,”            in        Proc.           Conf.           North           Amer.          Chapter                                                                                                                                                                                                  Xu                    Feng                      received                     the                    B.Sc.                   degree                    in                    software
                  Assoc.            Comput.             Linguistics,              Hum.            Lang.            Technol.,            2013,            pp.           596–606 .                                                                                                                                                                                   engineering                 from                Heilongjiang                  University,                 Harbin,
[48]                     I.                Beltagy,                 K.                Lo,                and                A.                Cohan,              “SciBERT:                 A                pretrained                 language                                                                                                                   China,         in         2019.        He        is        currently          pursuing         the         master’s
                  model                for               scientiﬁc                 text,”                in              Proc.                Conf.                Empirical                 Methods                 Natural                                                                                                                                       degree            with            the            School            of           Software,            Dalian            Univer-
                  Lang.                Process.                9th                Int.                Joint                 Conf.                Natural                Lang.                Process.                (EMNLP-                                                                                                                                       sity            of            Technology,            Dalian,            China.
                  IJCNLP),            2019,            pp.           3606–3611 .                                                                                                                                                                                                                                                                                         His                research                interests                 include                 big                scholarly                data
[49]                     M.                Schuster                 and                 K.                Nakajima,                 “Japanese                 and                Korean                 voice                 search,”                                                                                                                             and            data            science.
                  in                  Proc.                  IEEE                Int.                  Conf.                  Acoust.,                  Speech                  Signal                  Process.                  (ICASSP),
                  Mar.            2012,            pp.           5149–5152 .
[50]                     C.                Tu,                Z.                Zhang,                Z.               Liu,                and                M.                Sun,                “TransNet:                 Translation-based
                  network             representation              learning             for            social             relation             extraction,”              in        Proc.
                  26th            Int.             Joint             Conf.             Artif.            Intell.,            Aug.           2017,            pp.           2864–2870 .
[51]                     J.                 Cohen,                  “Weighted                   kappa:                  Nominal                  scale                  agreement                   provision                  for
                  scaled                 disagreement                 or                partial                 credit,”              Psychol.                  Bull.,               vol.                70,                no.               4,
                  p.           213,           1968.
[52]                     S.               Kojaku,                G.               Livan,                and                N.               Masuda,                “Detecting                 anomalous                citation
                  groups            in            journal            networks,”            2020,          arXiv:2009.09097.
[53]                     P.                    Davis,                    “The                    emergence                     of                    a                    citation                     cartel,”                 Scholarly                      Kitchen,                                                                                            Jing                              Ren                               (Graduate                            Student                           Member,                           IEEE)
                  vol.                    10,                    pp.            15–17,                    Apr.                    2012.                     Accessed:                     Dec.                    6,                    2020.                     [Online].                                                                                        received                the               bachelor’s                degree                from               Huaqiao                Uni-
                  Available:                   https://scholarlykitchen.sspnet.org/2012/04/10/emergence-of-                                                                                                                                                                                                                                                        versity,               Xiamen,                China,                in               2018,               and                the                master’s
                  a-citation-cartel/                                                                                                                                                                                                                                                                                                                               degree                from               the               Dalian                 University                 of               Technology,
[54]                     B.            Perozzi,            R.            Al-Rfou,            and            S.           Skiena,             “DeepWalk:             Online            learning             of                                                                                                                                                      Dalian           China,          in          2020.          She         is          currently           pursuing          the
                  social             representations,”              in          Proc.             20th             ACM             SIGKDD              Int.             Conf.             Knowl.                                                                                                                                                                   Ph.D.        degree         with         the         School         of         Engineering,          IT        and
                  Discovery             Data             Mining,            Aug.           2014,            pp.           701–710.                                                                                                                                                                                                                                 Physical               Sciences,               Federation               University               Australia,
[55]                     A.               Grover                and                J.               Leskovec,                “node2vec:                 Scalable                 feature                learning                 for                                                                                                                               Ballarat,             VIC,            Australia.
                  networks,”               in             Proc.               22nd               ACM              SIGKDD               Int.               Conf.               Knowl.                Discovery                                                                                                                                                            Her            research             interests             include             data            science,              graph
                  Data            Mining,            Aug.           2016,            pp.           855–864 .                                                                                                                                                                                                                                                       learning,             anomaly            detection,             and            social            computing.
[56]                     J.         Tang,          M.         Qu,          M.         Wang,          M.         Zhang,          J.         Yan,          and          Q.          Mei,          “Line:          Large-
                  scale              information              network              embedding,”              in          Proc.             24th              Int.              Conf.              World
                  Wi  d   e                     We  b,            2015,            pp.           1067–1077 .
[57]                     K.                Ding,                Q.                Zhou,                H.                Tong,                and                H.                Liu,                “Few-shot                 network                anom-
                  aly                  detection                   via                  cross-network                   meta-learning,”                   in             Proc.                   Web                   Conf.,
                  Apr.            2021,           pp.            2448–2456.
[58]                     X.           Wang,           B.           Jin,           Y.           Du,           P.           Cui,           Y.           Tan,           and           Y.           Yang,            “One-class            graph
                  neural              networks              for             anomaly              detection              in             attributed              networks,”        Neural
                  Comput.             Appl.,           vol.            33,           pp.           12073–12085 ,           Mar.            2021.                                                                                                                                                                                                                   Huan                       Liu                        (Fellow,                      IEEE)                    received                        the                      B.Eng.
[59]                     Y.             Dou,             Z.             Liu,             L.             Sun,             Y.             Deng,              H.             Peng,             and              P.             S.             Yu,             “Enhancing
                  graph            neural            network-based             fraud            detectors             against            camouﬂaged             fraud-                                                                                                                                                                                             degree             in            computer             science             and            electrical              engineer-
                  sters,”             in              Proc.              29th               ACM              Int.               Conf.               Inf.               Knowl.               Manage.,              Oct.              2020,                                                                                                                          ing                 from                 Shanghai                  Jiaotong                 University,                  Shanghai,
                  pp.           315–324 .                                                                                                                                                                                                                                                                                                                          China,              in              1983,              and               the              Ph.D.             degree               in              computer
                                                                                                                                                                                                                                                                                                                                                                   science             from            the            University             of            Southern             California,
                                                                                                                                                                                                                                                                                                                                                                   Los           Angeles,            CA,           USA,           in            1989.
                                                                                    Jiaying                    Liu                   (Member,                 IEEE)                 received                    the                  B.Sc.                                                                                                                               He               is               currently                a               Professor               of               computer                science
                                                                                    and           Ph.D.           degrees            in           software           engineering             from           the                                                                                                                                                    and          engineering           at          Arizona          State           University,           Tempe,
                                                                                    Dalian             University              of            Technology,              Dalian,             China,             in                                                                                                                                                    AZ,       USA.       His        research        interests         include        data        mining,
                                                                                    2016            and            2021,            respectively.                                                                                                                                                                                                                  machine                  learning,                   social                   computing,                  and                  artiﬁcial
                                                                                          She               is               currently                an               Assistant               Professor               with               the                                                                                                                      intelligence.
                                                                                    School          of         Economics          and          Management,           Dalian          Uni-                                                                                           Dr.              Liu             is              a              fellow              of             the              Association               for             Computing              Machinery               (ACM),
                                                                                    versity          of         Technology.          Her         research          interests          include                                                                                 the               Association                for               the               Advance               of               Artiﬁcial                Intelligence                 (AAAI),              and               the
                                                                                    data            science,             big            scholarly             data,            and            social            network                                                       American              Association              for             the              Advancement              of             Science              (AAAS).             He             serves
                                                                                    analysis.                                                                                                                                                                                 on               journal                editorial                 boards               and               numerous               conference                 program               committees.
                                                                                                                                                                                                                                                                              He           is           a           Founding            Organizer            of           the           International             Conference            Series            on           Social
                                                                                                                                                                                                                                                                              Computing,            Behavioral             Cultural             Modeling,            and            Prediction.
              Authorized licensed use limited to: Necmettin Erbakan Universitesi. Downloaded on January 29,2025 at 05:06:02 UTC from IEEE Xplore.  Restrictions apply.

