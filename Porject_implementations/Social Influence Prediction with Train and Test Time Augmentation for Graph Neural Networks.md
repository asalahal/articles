Bo, H., Mcconville, R., Hong, J., & Liu, W. (2021). Social Influence
Prediction with Train and Test Time Augmentation for Graph Neural
Networks. In                International Joint Conference on Neural Networks 2021
(IJCNN 2021) Institute of Electrical and Electronics Engineers (IEEE).
https://doi.org/10.1109/IJCNN52387.2021.9533437
Peer reviewed version
Link to published version (if available):
10.1109/IJCNN52387.2021.9533437
Link to publication record on the Bristol Research Portal
PDF-document
This is the author accepted manuscript (AAM). The final published version (version of record) is available online
via Institution of Electrical and Electronics Engineers at https://ieeexplore.ieee.org/document/9533437 . Please
refer to any applicable terms of use of the publisher.
University of Bristol – Bristol Research Portal
General rights
This document is made available in accordance with publisher policies. Please cite only the
published version using the reference above. Full terms of use are available:
http://www.bristol.ac.uk/red/research-policy/pure/user-guides/brp-terms/

       Social Inﬂuence Prediction with Train and Test
     Time Augmentation for Graph Neural Networks
                                Hongbo Bo                                                    Ryan McConville
                   Department of Computer Science                              Department of Engineering Mathematics
                           University of Bristol                                            University of Bristol
                                 Bristol, UK                                                     Bristol, UK
                         hongbo.bo@bristol.ac.uk                                      ryan.mcconville@bristol.ac.uk
                                 Jun Hong                                                         Weiru Liu
                   Department of Computer Science                               Department of Engineering Mathematics
                        and Creative Technologies                                            University of Bristol
                   University of the West of England                                              Bristol, UK
                                 Bristol, UK                                               weiru.liu@bristol.ac.uk
                           Jun.Hong@uwe.ac.uk
  Abstract—Data augmentation has been widely used in machine                and natural language processing (NLP) [8], the combination
learning for natural language  processing and computer vision               of data augmentation methods and deep neural networks have
tasks  to  improve  model  performance.  However,  little  research         been shown to be effective. By performing data augmentation,
has  studied  data  augmentation  on  graph  neural  networks,              model performance can be improved as it facilitates the neural
particularly  using  augmentation  at  both  train-  and  test-time.        network  to  learn  generalizable  features  related  to  the  task.
Inspired by the success of augmentation in other domains, we
have  designed  a  method  for  social  inﬂuence  prediction  using         While  GNNs  have  become  a  popular  research  ﬁeld,  little
graph neural networks with train- and test-time augmentation,               research has focused on using data augmentation for GNNs,
which can effectively generate multiple augmented graphs for                especially in terms of using augmentation at both train- and
social  networks  by  utilising  a  variational  graph  autoencoder         test-time. Motivated by the success of data augmentation in
in both scenarios. We have evaluated the performance of our                 CV and NLP, we study whether data augmentation at not only
method on predicting user inﬂuence on multiple social network
datasets.  Our  experimental  results  show  that  our  end-to-end          train-time, but also test-time, can improve the performance of
approach, which jointly trains a graph autoencoder and social               GNNs, particularly on the task of social inﬂuence prediction.
inﬂuence behaviour classiﬁcation network, can outperform state-                Extending the work of DeepInf [2], we have developed a
of-the-art approaches, demonstrating the effectiveness of train-            method,AugInf,forsocialinﬂuencepredictionwithbothtrain-
and test-time augmentation on graph neural networks for social              and  test-time  augmentation  for  GNNs.  In  this  method,  the
inﬂuence prediction. We observe that this is particularly effective
on smaller graphs.                                                          augmented graphs are ﬁrst generated by utilising a variational
  Index Terms—graph neural networks, social network analysis,               graph autoencoder (VGAE) [9] and then social inﬂuence is
social inﬂuence analysis, augmentation                                      predicted  by  joint  training  of  both  a  Graph  Auto-Encoder
                         I.  INTRODUCTION                                   (GAE) [9] and a GNN prediction module based on either a
                                                                            Graph Convolutional Network (GCN) [10] or a Graph Atten-
  Graph  neural  networks  (GNNs)  [1]  have  been  shown  to               tion Network (GAT) [11]. We have compared the performance
be effective in various graph machine learning tasks, such as               of AugInf with several state-of-the-art GNN approaches by
link prediction and node classiﬁcation. The rapid growth of                 experimentingonnumeroussocialnetworks.Ourexperimental
online social networks has led to the development of numerous               results show that AugInf can improve prediction performance
methods for studying social behaviour online. However, many                 on several of these social networks.
learning  tasks  on  social  networks  have  relied  heavily  on               In  summary,  our  contributions  are  as  follows.  First,  we
manual feature extraction. GNNs have provided an alternative                propose a joint training approach consisting of Graph Auto-
to this with their ability to automatically learn representations           Encoder (GAE)  [9] and GNN prediction module. The GNN
end-to-end. One such task of interest, which has been shown                 prediction module is implemented as either a Graph Atten-
to be enhanced using GNNs, is social inﬂuence prediction [2].               tion Network (GAT) [11] or a Graph Convolutional Network
  Data  augmentation  [3],  which  increases  the  amount  of               (GCN) [10]. The joint training approach optimizes for two
data available by creating informative variations of existing               tasks  simultaneously.  The  ﬁrst  is  to  obtain  more  effective
data, can improve the performance of machine learning mod-                  latent representations of input graphs, while the second is to
els  and  has  been  widely  used  in  many  machine  learning              utilize the resulting representations to improve social inﬂuence
tasks [4, 5, 6]. In the ﬁelds of computer vision (CV) [7, 3]                predictive performance. Additionally, we use the augmentation

approach at both train- and test-time to further improve the                 the most general sense, a social network can be represented
joint training model performance, which we demonstrate with                  by a graphG, whereV  represents users andE  is the set of
an ablation study. To the best of our knowledge, we are the                  directed edges representing how the users are connected. The
ﬁrst to explore test-time augmentation on GNNs, and therefore                adjacency matrixA is a (0,1) matrix with 0s on its diagonal
the ﬁrst to combine both train-time and test-time augmentation               (i.e. no self-connection) which can represent graphG, where
on GNNs. Finally, we conduct an experimental evaluation on                   Aij= 0    indicates nodesiandjare not connected. IfG  is
multiple  different  social  networks  comparing  our  proposed              undirected,Aij=1    indicates nodesiandjare connected and
method with two state-of-the-art methods.                                    A is symmetric. IfG is directed,Aij=1    indicates there is a
                        II.  RELATED WORK                                    link from nodeito nodej. A graph neural network model is
     a) Graph  Neural  Networks:   Graph  Neural  Networks                   deﬁned asf(X,A), whereX  is the node feature matrix ofA.
(GNNs) [12, 13] have rapidly grown to become a popular                       A. Graph Convolutional Network
research  area,  providing  a  highly  competitive  approach  for               A  Graph  Convolutional  Network  (GCN)  [10]  is  a  semi-
tasks  involving  graph  data.  One  line  of  research  focuses             supervised learning algorithm for graph data, typically used
on unsupervised models, e.g. VGAE [9] and Graphite [14].                     for node and graph classiﬁcation, as well as link prediction.
These unsupervised variational models typically aim to use                   A GCN model is typically formed by stacking multiple GCN
generative modelling of graphs for graph reconstruction, link                layers, and for each GCN layer, the inputs are the adjacency
prediction  and  clustering.  Additionally,  supervised  models              matrixA and the features matrix,H∈Rn×F, wheren is the
have  attracted  signiﬁcant  attention,  such  as  SCNN  [15],               number of vertices, andF is the number of features. For each
ChebyNet [16], GAT [11] and GCN [10], which are widely                       GCN layer:
used in tasks where labelled data is available.                                             H (l+1)   = σ(˜D−1/2 ˜A˜D 1/2H (l)W  (l)),             (1)
     b) Data  Augmentation:   Data  augmentation  has  been
shown to be an effective approach in machine learning which                  where  ˜A   is   the   adjacency   matrix A   with   added   self-
expands a dataset by producing transformed copies of data,                   connections (the diagonal elements of the matrix are 1), ˜D
thereby making the model invariant to these transformations.                 is the diagonal degree matrix where ˜Dii= ∑             j˜Aij, andσ(·)
Data augmentation has been widely used to improve gener-                     denotes an activation function. The input layerH (0)   = X .
alizability  of  machine  learning  models  in  natural  language            We will experiment with the use of a GCN model for social
processing(NLP)andcomputervision(CV).Mostofthework                           inﬂuence prediction.
on data augmentation has focused on improving augmenta-
tion at the training phase, e.g., batch augmentation [5] and                 B. Graph Attention Network
UDA [6]. There are also studies that focus on augmentation                      A Graph Attention Network (GAT) [11] is an attention-
during the testing phase [17]. However, data augmentation for                basedversionofGCN,whichincorporatesself-attentionmech-
graph neural networks has only been recently studied, such as                anisms. The GAT layer performs the self-attention mechanism
SUBG-CON [18] and NodeAug [19]. Particularly, there is no                    for each node by introducing attention coefﬁcients. The self-
research on test-time augmentation for GNNs.                                 attention mechanism is an attention functionattn:
     c) Social     Inﬂuence:      The     Independent     Cascade                                       RF′×RF′→R,                              (2)
Model  [20]  and  Linear  Threshold  Model  [21]  are  classic
social  network  inﬂuence  propagation  models.  Measuring                   whereF′represents the number of output features of each
social inﬂuence can be broadly divided into two categories                   node in a GAT layer and the attention coefﬁcient between
based on the methods used. The ﬁrst category typically utilizes              each pair of nodes is calculated as:
ranking  algorithms  such  as  TwitterRank  [22],  Truetop  [23]                                   eij= attn(Whi,Whj),                        (3)
and  EIRank  [24],  to  quantify  each  user’s  inﬂuence.  This
category  of  methods  provide  a  coarse  inﬂuence  value  for              wherehi,hj∈RF  andW∈RF′×RF, and the atten-
each user at a speciﬁc time-point and do not model changes                   tion functionattnis instantiated with a dot product and a
to  inﬂuence,  nor  the  direct  effect  of  this  inﬂuence  on              LeakyReLU [27] non-linearity. The attention coefﬁcienteijis
others  on  the  network.  The  second  category  of  methods                considered as the importance of nodejto nodei. A softmax
are based on predictive models to estimate social inﬂuence                   function is adopted to the normalized attention coefﬁcient to
change. For example, on the global-level patterns of social                  make  it  easier  to  calculate  coefﬁcients  and  compare  them
inﬂuence, the DeepCas model [25] can predict the information                 among nodes:
cascade by using recurrent neural networks. DeepInf [2] and                              αij= softmaxj(eij)=    Exp(eij )∑
NNMLInf  [26]  consider  a  social  inﬂuence  prediction  task                                                            k∈N iExp(eik ),          (4)
as  a  label  classiﬁcation  task  and  directly  predict  the  user         whereNi is the set of neighbour nodes of nodeiandαij
behaviour with respect to inﬂuence.                                          is the coefﬁcient for aggregating the calculation of the output
                        III.  PRELIMINARIES                                  featurehi′which has incorporated neighborhood information:
  LetG = (V,E) be an input graph which consists of a set                                            hi′= σ(∑        αijWhj),                         (5)
of nodesV  and a set of edgesE, whereE∈V×V. In                                                                j∈Ni

whereσ is a non-linear function. In order to make the self-                  and the decoder of VGAE is also an inner-product. The loss
attention  learning  process  more  stable,  it  is  known  to  be           function of VGAE is:
effective to use multi-head attention to expand the attention                                         L=LCE +LKLD,                         (12)
mechanism. By usingK  independent attention mechanisms
to perform transformations with Equation 5, and then con-                    WhereLCE  is  same  as  Equation  9  andLKLD   is  the  KL
catenating their features together, the following output can be              divergence which is  1      ∑ d
obtained:                                                                    dis the dimension ofZ. We use VGAE as part of the graph2i=1  (σ2i+ µ2i−log(σ2i)−1) , where
                    h′i=∥Kk=1 σ(∑        αkijWhkj),                     (6)  augmentation process.
                                   j∈Ni
where∥denotes the vector concatenation operation and the                                                  IV.  METHOD
dimension ofh′iisKF′. We will experiment with the use of                        We propose a method for social inﬂuence prediction which
a GAT model for social inﬂuence prediction.                                  consists of a joint training of the GAE and the GNN pre-
C. Graph Auto-Encoder                                                        diction module with both train- and test-time augmentation.
  A  Graph  Auto-Encoder  (GAE)  [9]  can  utilize  the  GCN                 For  clarity,  as  different  strategies  are  used  for  train-  and
layers to obtain the latent representations of the nodes in the              test-time augmentation, we demonstrate the entire method in
graph through an encoder-decoder structure to learn represen-                Figure1withsharedboxesshowingtrainingandtestingstages,
tations for downstream tasks, such as link prediction and node               respectively.
classiﬁcation. The encoder process can be expressed as:
                         Z = GCN (X,A),                            (7)       A. Data Augmentation
                                                                                Before the joint training, we ﬁrst obtain augmentations of
whereZ∈Rn×F  is the latent representation, which is also                     our data by adapting a similar idea to  [4]. The approach for
referred to as an embedding. In this paper, we use a two layer               augmentation consists of two steps: (1) using the variational
GCN autoencoder which can be deﬁned as:                                      graph auto-encoder (VGAE) [9] to obtain edge probabilities
                 GCN (X,A)=  ˜Aσ(˜AXW   0)W 1,                 (8)           for all possible and existing edges in graphG, (2) using the
                                                                             predicted edge probabilities, with a threshold set to stochas-
whereW 0  andW 1  are the parameters to be learned andσ                      tically add new edges, creating a modiﬁed graphGm, which
is the ReLU activation function. GAE uses the inner-product                  is used as input to the joint training process. The key idea of
as a decoder to reconstruct the original graph. In the training              this augmentation approach is to use information inherent in
process of GAEs, cross entropy is used as the loss function:                 the graph to predict which non-existent edges are likely to be
                   N∑                                                        added to the augmented graph to improve generalization. As
        L=   1N        −(yilog ˆyi+(1 −yi)log(1 −ˆyi))        (9)            a VGAE is a generative model, we utilize VGAE as part of
                  i=1                                                        our graph augmentation process.
In the above equation,yirepresents the value of an element                      The VGAE consists of a two-layer GCN encoder and an
in the adjacency matrixA  (0 or 1), andN   is the size of                    inner-product decoder:
A, ˆyi represents the value of the corresponding element in                                              M  = σ(ZZT),                             (13)
the reconstructed adjacency matrix ˆA (between 0 and 1). We
use  GAE  in  the  joint  training  model  to  obtain  the  latent           whereZ  is the latent representation,σ is an element-wise
representation.                                                              sigmoid function andM   is the predicted (symmetric) edge
D. Variational Graph Auto-Encoder                                            probability matrix produced by the inner-product decoder. We
  A Variational Graph Auto-Encoder (VGAE) [9] uses latent                    can add edges in graphG  to obtain the augmentation graph
variables for the model to learn the distributions, and then                 Gm  according to the probabilities inM . Corresponding to
samples from these distributions to get latent representations.              G =(V,E),Gm  can be expressed asGm =(V,Em), and for
In VGAE,Z is no longer obtained by a certain function (such                  each nodev∈V, there are different edge setseandem  inG
as eq. (7)), but by sampling from a Gaussian distribution. This              andGm  respectively.
mechanism makes VGAE suitable for graph generation tasks.                       For efﬁciency, the input graph we will use is not an entire
VGAE uses a 2-layer GCN model (same as Equation. (8)) to                     graphG, but a set of subgraphs sampled by random walks
calculate the meanµ and varianceσ respectively to obtain a                   onG, resulting inN  subgraphs, following the approach of
Gaussian distribution:                                                       DeepInf [2]. We will perform augmentation on each of the
                                                                             subgraphs separately, generating in totalQ augmentations for
                         µ= GCNµ(X,A),                          (10)         eachoftheN subgraphs.FortheprobabilitymatrixMiofeach
                       logσ = GCNσ(X,A),                       (11)          subgraph, we will set a threshold, and augment the subgraph
                                                                             by adding number of edges that have probabilities higher than
W 0   is  the  same  in  bothGCNµ  andGCNσ,  butW 1   is                     this threshold. This threshold, as a hyperparameter of AugInf,
different.Z  can  be  calculated  by  reparameterization  [28]               will be discussed in the experimental section of this paper.

Fig. 1.  The AugInf method. AugInf will ﬁrst obtain the predicted edge probability matrixM    by using the VGAE [9]. A threshold hyperparameter will control
the amount of edges added during the augmentation. We extend the work of [4] to perform multiple augmentations, randomly sampling a subset of edges to
add to each original subgraph. (a) For train-time augmentation, the method will then integrate all augmentations together for the joint training process. The
joint training model has two steps: the GAE step will learn the latent representations of the input data for the prediction stage; the prediction module (GAT
or GCN) will produce the social inﬂuence predictions. The losses of these two stages (decoder loss and prediction loss) will be combined and propagated
backwards to jointly update the model. (b) For test-time augmentation, AugInf will generate several augmentations of each test (validation) example, learning
a representation for each before producing the inﬂuence prediction. AugInf will calculate the average of the predictions to produce the ﬁnal social inﬂuence
prediction.
B. Representation Learning                                                                  a) Jointly trained model with train- and test-time aug-
   AugInf will primarily use a GAE to learn a representation                          mentation:  We  utilize  joint  training  of  both  the  GAE  and
of the graph via a transformation of the graph structure into                         the  GNN  prediction  module  to  achieve  two  objectives:  (1)
a low-dimensional latent space. Additionally, consistent with                         obtain  a  more  effective  latent  feature  representation  of  the
DeepInf,  AugInf  will  also  use  DeepWalk  [29]  to  generate                       input graphs, (2) using these representations with the GNN
features of both the original graphG andQ augmented graphs                            prediction module to more accurately predict social inﬂuence.
as extra features for the social inﬂuence prediction.                                 We experiment with the GNN prediction module implemented
                                                                                      as either a GAT or a GCN for the social inﬂuence prediction.
C. Neural Network Model                                                                  The train-time augmentation process is shown in Figure 1
   WehaveimplementedtwovariantsofAugInfbasedonGCN                                     (a). The inputs are the subgraphs sampled from graphG, along
and GAT, denoted by AugInf-GCN and AugInf-GAT. In our                                 with their augmentations. We utilize the learned representation
joint training model, AugInf-GCN uses GCN layers to obtain                            Z as one of the GNN prediction module features, along with
the output, while AugInf-GAT uses multi-head GAT layers.                              inﬂuence features and pretrained DeepWalk embeddings. The

           inﬂuence features are provided by the authors of DeepInf [2],                                                    TABLE I
           which record the user behaviour status (i.e. have they taken an                     THE STATISTICS OF THE DATASETS. |V   |  AND |E   |  ARE THE TOTAL
           action) and whether the user is the ego [2] user. The output of                       NUMBERS OF NODES AND EDGES OF THE ORIGINAL DATASET
                                                                                                 RESPECTIVELY ANDN    IS THE NUMBER OF SUBGRAPHS AFTER
           the GNN prediction module is a 2-dimension representation                                                    PREPROCESSING.
           for each user (each node), corresponding to the negative log-
           likelihood. The test-time augmentation process is shown in
           Figure 1 (b). For each test graph, we generate a number of
           augmented versions of the graph, learning representations and
           producing inﬂuence predictions for each, with the average of
           these predictions forming the ﬁnal social inﬂuence prediction.
                 b) Loss: The loss function of the joint model consists of                B. Evaluation Metrics
           two parts:                                                                        We  analyze  several  hyperparameters  in  our  model  and
                                      L = LD + LGAE,                           (14)       study  how  different  hyperparameters  may  affect  prediction
           whereLD  is the loss of the ﬁnal prediction and the ground                     performance. The performance is evaluated in terms of Area
           truth, which is calculated by a negative log-likelihood, and                   Under  Curve  (AUC)  andF1   score  (F1 ).  We  compare  our
           LGAE   is the loss of GAE which is calculated by the cross                     model with the state-of-the-art DeepInf [2] and PSCN [30]
           entropy of the decoded reconstructed graphs and the input                      algorithms.
           graphs.                                                                        C. Experiment Setup
                                      V.  EXPERIMENTS                                        In our experiments we apply three augmentations to each
                                                                                          graph with the augmentation hyperparameter threshold value
           A. Datasets                                                                    set  to  0.8  and  train  for  500  epochs.  We  will  discuss  the
              We  evaluate  using  four  datasets  across  different  social              performance of varying these parameters in a later section. For
           network  domains,  namely  OAG1  (Open  Academic  Graph),                      the GAE component, each of the two hidden layers contain
           Digg2, Twitter3 and Weibo4.                                                    64 hidden units for Digg and Twitter, and 32 for OAG and
              •The OAG graph consists of academic graphs representing                     Weibo. They are trained with the Adagrad optimizer, using a
                 the co-author network in which the citation behaviours,                  0.2learningrateforOAGandWeibo,0.05forDiggand0.1for
                 which  we  are  predicting,  are  deﬁned  as  the  inﬂuence              Twitter. Weight decay is set to 0.0005 all datasets except Digg,
                 action behaviour.                                                        where it is 0.001. Additionally, we use dropout rate of 0.2. For
              •The Digg dataset contains the timestamped voting be-                       the GNN prediction module, the ﬁrst and second layers each
                 haviours of users on stories on a social news aggregation                contain 128 hidden units and the third layer, as the output
                 website.TheedgesofDigggrapharedeﬁnedasfollowing                          layer, has two hidden units. There are eight attention heads in
                 relationships  and  the  inﬂuence  actions,  which  we  are              each GAT layer, which means each head needs to process 16
                 predicting, are voting behaviours.                                       hidden units for Digg and Twitter, with four attention heads
              •The Twitter dataset has been built by collecting Twitter                   for OAG and Weibo, which means each head needs to process
                 data corresponding to tweets collected before, during and                32 hidden units. The nonlinear activation function we use for
                 after the announcement of the discovery of the Higgs                     both augmentation and prediction (σ in Eq. 1 and 5) is the
                 boson  in  2012.  The  graph  is  deﬁned  as  a friendship               exponential linear unit (ELU) [31].
                 network, and the social action, which we are predicting,                 D. Experimental Results
                 is deﬁned as whether a userretweets Higgs boson tweets.                     We  report  the  performance  of  our  method  (AugInf-GAT
              •The Weibo graph was built from 100 randomly selected                       and  AugInf-GCN)  over  ten  runs.  The  mean  and  standard
                 users and their followers and followees. The social net-                 deviations are shown in Table II. For clarity, the input features
                 work is deﬁned as afriendship network, and the social                    of our models are inﬂuence features and pretrained DeepWalk
                 action, which we are predicting, is deﬁned asretweeting                  embeddings,  consistent  with  the  end-to-end  method  Deep-
                 behaviors in the Weibo social network.                                   Inf [2], but without any hand-crafted vertex features. As an
              These datasets were used previously by Qiu et al. [2]. Qiu                  experimental comparison, we used the state-of-the-art GNN
           et al. [2] sampled the entire social network into sub-networks                 methods DeepInf and PSCN [30] as the baselines.
           with 50 nodes in each sub-network by using a random walk                          Our proposed method AugInf-GAT achieves better perfor-
           with restart, extracted features for each node and provided a                  mance over most of the baselines in terms of AUC. However,
           ground-truth for the dataset. The statistics of the three datasets             consistent  with  all  models,  the  GCN-based  approaches  do
           are shown in Table I.                                                          not perform well, as GCN uses the unweighted aggregations
              1OAG dataset details: www.openacademic.ai/                                  over the neighbours’ representations when calculating a node’s
              2Digg dataset details: www.isi.edu/ lerman/downloads/digg2009.html/         representations, a mechanism that appears not to be suitable
              3Twitter dataset details: snap.stanford.edu/data/higgs-twitter.html         for tasks like social inﬂuence prediction, which beneﬁts from
              4Weibo dataset details: www.aminer.cn/inﬂuencelocality                      considering the importance of neighbouring nodes.
        OAG            Digg            Twitter          Weibo
|V   |  953,675        279,630         456,626          1,776,950
|E   |  4,151,463      1,548,126       12,508,413       308,489,739
N       499,848        24,428          362,888          779,164

                                            TABLE II
           THE PERFORMANCE OF TWO AUGINF MODELS ON DIFFERENT DATASETS,
            ALONG WITH THE PERFORMANCE OF THE BASELINES WITHOUT VERTEX
                                            FEATURES.
                                                                                           Fig. 2.  The performance of AugInf-GAT on Digg as we vary the number of
                                                                                           augmentations. The value of threshold is set to 0.8.
              Referring to the statistics of the datasets in Table I, we
           can see that the performance improvement of AugInf-GAT
           is particularly clear on the datasets with much fewer edges
           (Digg) but limited on datasets with more edges (Twitter and
           Weibo).  We  believe  that  this  is  because  the  Twitter  and
           Weibo datasets contain enough edges to learn a sufﬁciently
           comprehensive  representation,  hence  less  beneﬁt  is  gained
           from the augmentation. We will further investigate the effect
           of removing edges from graphs as part of data augmentation
           in future work. Nonetheless, particularly for smaller graphs,
           we  believe  our  proposed  approach  of  train-  and  test-time
           augmentation can provide additional performance.
              1) Hyperparameter Analysis: We conduct an hyperparame-
           teranalysisontheDiggdatasetwiththesamehyperparameters                           Fig. 3.  The performance of AugInf-GAT on Digg as we vary the parameter
           values mentioned previously, unless stated otherwise.                           that controls the minimum quality of edges that are added for augmentation.
                                                                                           The number of augmentations is set to three.
                 a) NumberofAugmentations: Forthedataaugmentation,
           we analyze the effect of the number of augmentations on the                     E. Ablation Study
           performance of AugInf. We successively apply one to eight
           augmentations while leaving the other parameters constant.                         We have further evaluated AugInf-GAT and AugInf-GCN
           The results of this are shown in Figure 2. When we apply                        on the individual components of our approach, to determine
           fouraugmentations, thehighest performanceis achieved.After                      thecontributionofeachcomponenttotheoverallperformance.
           that, as the number of augmentations increase, the AUC score                       There  are  three  main  components  in  our  approach:  (1)
           stabilises. Interestingly, theF1  score does not appear to be                   train-time  augmentation  (2)  test-time  augmentation  and  (3)
           affected  by  the  number  of  augmented  graphs  and  remains                  the jointly trained model. We have evaluated the following
           stable between 0.73 and 0.74.                                                   combinations on the Digg dataset:
                 b) The Threshold for Augmentation:  Another parameter                        •Ablation #1   :  The  method  contains  only  a  GNN  pre-
           we analyze is the threshold that determines which edges may                           diction  module.  We  use  this  set  of  experiments  as  a
           be added. The results of this are shown in Figure 3. When the                         baseline to compare with other combinations when other
           threshold is set to 0.8 for Digg dataset, our method achieves                         components are added.
Dataset    thehighestperformance,whileonaveragethenumberofedgesModelAUCF 1                    •Ablation #2   :  The  method  contains  the  jointly  trained
           per dataset increases by 2.7%   . As we increase the number ofDeepInf-GAT0.8882(±0.011)0.7052(±0.010)model with no augmentation. The purpose of this exper-
           added edges, the performance of our method decreases.DeepInf-GCN0.8372(±0.007)0.6404(±0.006)iment is to measure the effect of joint training without
Digg        PSCN                0.8499(±0.013)        0.6636(±0.011)
            AugInf-GCN          0.8580(±0.013)        0.6711(±0.024)
            AugInf-GAT          0.9067(±0.011)        0.7385(±0.017)
            DeepInf-GAT         0.7843(±0.002)        0.5484(±0.002)
            DeepInf-GCN         0.7615(±0.004)        0.5271(±0.004)
Twitter     PSCN                0.7664(±0.004)        0.5319(±0.006)
            AugInf-GCN          0.7694(±0.003)        0.5360(±0.006)
            AugInf-GAT          0.7861(±0.004)        0.5486(±0.007)
            DeepInf-GAT         0.6814(±0.006)        0.4544(±0.004)
            DeepInf-GCN         0.6281(±0.002)        0.4231(±0.003)
OAG         PSCN                0.6556(±0.006)        0.4372(±0.029)
            AugInf-GCN          0.6334(±0.003)        0.4229(±0.011)
            AugInf-GAT          0.6889(±0.012)        0.4602(±0.004)
            DeepInf-GAT         0.8212(±0.003)        0.5770(±0.003)
            DeepInf-GCN         0.7706(±0.002)        0.5312(±0.004)
Weibo       PSCN                0.8012(±0.004)        0.5625(±0.002)
            AugInf-GCN          0.7531(±0.006)        0.5147(±0.004)
            AugInf-GAT          0.8124(±0.005)        0.5785(±0.007)

      augmentation.                                                            between 0.83 and 0.86. We can see that all components have
   •Ablation#3   : The method uses a GNN prediction module                     contributed to some extent to the performance of the GCN
      with train-time augmentation only. The purpose of this                   prediction module, but overall, AugInf when using a GCN for
      experiment is to measure the effect of train-time aug-                   social inﬂuence prediction achieves lower performance than
      mentation.                                                               AugInf when using a GAT for social inﬂuence prediction.
   •Ablation#4   : The method uses a GNN prediction module
      with  test-time  augmentation  only.  The  purpose of  this
      experiment is to measure the effect of test-time augmen-
      tation.
   •Ablation#5   : The method uses a GNN prediction module
      with  both  train-time  and  test-time  augmentation.  The
      purpose of this experiment is to measure the effect of
      both forms of augmentation together.
   •Ablation  #6   :  The  method  contains  the  joint  training
      model with train-time augmentation only. The purpose
      of  this  experiment  is  to  evaluate  the  extent  of  which
      using train-time augmentation improves the joint training
      model.
   •Ablation #7   : The method contains the joint train-model
      with  test-time  augmentation  only.  The  purpose of  this
      experiment is to evaluate the extent of which using test-
      time augmentation improves the joint training model.                     Fig. 5.  The performance of using different combinations of components with
   •Complete  #8   :  The  method  is  our  complete  method,                  GCN, where #8    is our complete model consisting of all components, AugInf.
      AugInf, consisting of all components.
   The results with the GAT prediction module are shown in                                               VI.  CONCLUSIONS
Figure 4. Through the comparison with #1    and #2   , we can                     In  this  paper  we  proposed  a  new  social  inﬂuence  pre-
see the jointly trained model contributes to the performance                   diction method, AugInf, which incorporates train- and test-
improvement. Through the comparison among the ablations                        time augmentation with a jointly trained graph neural network
#1   , #3   , #4    and #5   , we observe that the augmentations can           approach.  During  training,  this  method  takes  into  account
improve the GAT performance, but there is little difference                    the  losses  of  both  the  graph  representation  learning  and
between using train-time augmentation and test-time augmen-                    downstream social inﬂuence prediction task. We improve per-
tation separately or together, without the jointly trained model.              formance by applying numerous augmentations to the graphs
The result of version #8   , the complete AugInf method, shows                 using variational graph auto-encoders at both train- and test-
using train- and test-time augmentation, along with the jointly                time. Via an ablation study we show that the jointly trained
trained model, has the highest AUC score andF1  score.                         model obtains more effective latent feature representations by
                                                                               using the joint loss along with both the train- and test-time
                                                                               augmentations. We compare our proposed end-to-end method
                                                                               with the state-of-the-art on several social network datasets.
                                                                               The  experimental  results  show  that  our  proposed  method,
                                                                               AugInf-GAT,canimprovetheperformanceofpredictingsocial
                                                                               inﬂuence on a number of social networks, and in particular,
                                                                               on the smallest of the social network graphs.
                                                                                                             REFERENCES
                                                                                 [1]F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and
                                                                                      G. Monfardini, “The graph neural network model,” IEEE
                                                                                      Transactions on Neural Networks, vol. 20, no. 1, pp. 61–
                                                                                      80, 2008.
                                                                                 [2]J. Qiu, J. Tang, H. Ma, Y. Dong, K. Wang, and J. Tang,
                                                                                      “Deepinf: Social inﬂuence prediction with deep learn-
                                                                                      ing,” in Proceedings of the 24th ACM SIGKDD Inter-
Fig. 4.  The performance of the ablation study using GAT where #8    is our           national Conference on Knowledge Discovery & Data
complete method consisting of all components, AugInf.                                 Mining, 2018, pp. 2110–2119.
                                                                                 [3]C. Shorten and T. M. Khoshgoftaar, “A survey on image
   The  results  with  the  GCN  prediction  module  are  shown                       data  augmentation  for  deep  learning,”  Journal  of  Big
in Figure 5. The AUC scores of this set of experiments are                            Data, vol. 6, no. 1, p. 60, 2019.

 [4]T. Zhao, Y. Liu, L. Neves, O. Woodford, M. Jiang, and                            Data Mining, 2020, pp. 207–217.
      N. Shah, “Data augmentation for graph neural networks,”                 [20]K. Saito, R. Nakano, and M. Kimura, “Prediction of in-
      arXiv preprint arXiv:2006.06830, 2020.                                         formation diffusion probabilities for independent cascade
 [5]E. Hoffer, T. Ben-Nun, I. Hubara, N. Giladi, T. Hoeﬂer,                          model,” in International conference on knowledge-based
      and D. Soudry, “Augment your batch: better training with                       and  intelligent  information  and  engineering  systems.
      larger batches,” arXiv preprint arXiv:1901.09335, 2019.                        Springer, 2008, pp. 67–75.
 [6]Q.  Xie,  Z.  Dai,  E.  Hovy,  M.-T.  Luong,  and  Q.  V.                 [21]W. Chen, Y. Yuan, and L. Zhang, “Scalable inﬂuence
      Le,  “Unsupervised  data  augmentation  for  consistency                       maximization in social networks under the linear thresh-
      training,” arXiv preprint arXiv:1904.12848, 2019.                              old model,” in 2010 IEEE international conference on
 [7]A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, and                           data mining.   IEEE, 2010, pp. 88–97.
      A. V. Dalca, “Data augmentation using learned transfor-                 [22]J. Weng, E.-P. Lim, J. Jiang, and Q. He, “Twitterrank:
      mations for one-shot medical image segmentation,” in                           ﬁnding topic-sensitive inﬂuential twitterers,” in Proceed-
      Proceedings of the IEEE conference on computer vision                          ings of the third ACM international conference on Web
      and pattern recognition, 2019, pp. 8543–8553.                                  search and data mining, 2010, pp. 261–270.
 [8]K. M. Yoo, Y. Shin, and S.-g. Lee, “Data augmentation                     [23]J. Zhang, R. Zhang, J. Sun, Y. Zhang, and C. Zhang,
      for spoken language understanding via joint variational                        “Truetop:  A  sybil-resilient  system  for  user  inﬂuence
      generation,” in Proceedings of the AAAI conference on                          measurement  on  twitter,”  IEEE/ACM  Transactions  on
      artiﬁcial intelligence, vol. 33, 2019, pp. 7402–7409.                          Networking, vol. 24, no. 5, pp. 2834–2846, 2015.
 [9]T.  N.  Kipf  and  M.  Welling,  “Variational  graph  auto-               [24]H.  Bo,  R.  McConville,  J.  Hong,  and  W.  Liu,  “So-
      encoders,” arXiv preprint arXiv:1611.07308, 2016.                              cial network inﬂuence ranking via embedding network
[10]——,  “Semi-supervised  classiﬁcation  with  graph  con-                          interactions  for  user  recommendation,”  in  Companion
      volutional  networks,”  in  International  Conference  on                      Proceedings of the Web Conference 2020, 2020, pp. 379–
      Learning Representations (ICLR), 2017.                                         384.
[11]P.  Veli      ˇckovi´c,  G.  Cucurull,  A.  Casanova,  A.  Romero,        [25]C. Li, J. Ma, X. Guo, and Q. Mei, “Deepcas: An end-to-
      P. Lio, and Y. Bengio, “Graph attention networks,” arXiv                       end predictor of information cascades,” in Proceedings
      preprint arXiv:1710.10903, 2017.                                               of the 26th international conference on World Wide Web,
[12]Z. Zhang, P. Cui, and W. Zhu, “Deep learning on graphs:                          2017, pp. 577–586.
      A survey,” IEEE Transactions on Knowledge and Data                      [26]X.  Wang,  Z.  Guo,  X.  Wang,  S.  Liu,  W.  Jing,  and
      Engineering, 2020.                                                             Y. Liu, “Nnmlinf: social inﬂuence prediction with neural
[13]Z.  Wu,  S.  Pan,  F.  Chen,  G.  Long,  C.  Zhang,  and                         network multi-label classiﬁcation,” inProceedings of the
      S. Y. Philip, “A comprehensive survey on graph neural                          ACM Turing Celebration Conference-China, 2019, pp.
      networks,” IEEE Transactions on Neural Networks and                            1–5.
      Learning Systems, 2020.                                                 [27]B. Xu, N. Wang, T. Chen, and M. Li, “Empirical eval-
[14]A. Grover, A. Zweig, and S. Ermon, “Graphite: Iterative                          uation of rectiﬁed activations in convolutional network,”
      generative modeling of graphs,” in International Confer-                       arXiv preprint arXiv:1505.00853, 2015.
      ence on Machine Learning, 2019, pp. 2434–2444.                          [28]D. P. Kingma and M. Welling, “Auto-encoding varia-
[15]J.Bruna,W.Zaremba,A.Szlam,andY.LeCun,“Spectral                                   tional bayes,” arXiv preprint arXiv:1312.6114, 2013.
      networks  and  locally  connected  networks  on  graphs,”               [29]B.  Perozzi,  R.  Al-Rfou,  and  S.  Skiena,  “Deepwalk:
      arXiv preprint arXiv:1312.6203, 2013.                                          Onlinelearningofsocialrepresentations,”inProceedings
[16]M. Defferrard, X. Bresson, and P. Vandergheynst, “Con-                           of the 20th ACM SIGKDD international conference on
      volutional neural networks on graphs with fast localized                       Knowledge discovery and data mining, 2014, pp. 701–
      spectral  ﬁltering,”  in Advances  in  neural  information                     710.
      processing systems, 2016, pp. 3844–3852.                                [30]M. Niepert, M. Ahmed, and K. Kutzkov, “Learning con-
[17]K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep                               volutional neural networks for graphs,” in International
      into rectiﬁers: Surpassing human-level performance on                          conference on machine learning, 2016, pp. 2014–2023.
      imagenet  classiﬁcation,”  in Proceedings  of  the  IEEE                [31]D.-A. Clevert, T. Unterthiner, and S. Hochreiter, “Fast
      international conference on computer vision, 2015, pp.                         and accurate deep network learning by exponential linear
      1026–1034.                                                                     units (elus),” arXiv preprint arXiv:1511.07289, 2015.
[18]Y.  Jiao,  Y.  Xiong,  J.  Zhang,  Y.  Zhang,  T.  Zhang,
      and  Y.  Zhu,  “Sub-graph  contrast  for  scalable  self-
      supervised graph representation learning,” arXiv preprint
      arXiv:2009.10273, 2020.
[19]Y. Wang, W. Wang, Y. Liang, Y. Cai, J. Liu, and B. Hooi,
      “Nodeaug: Semi-supervised node classiﬁcation with data
      augmentation,”inProceedingsofthe26thACMSIGKDD
      International  Conference  on  Knowledge  Discovery  &

