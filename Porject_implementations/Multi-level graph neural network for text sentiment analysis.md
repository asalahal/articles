                                                        Contents lists available at ScienceDirect
                                        Computers and Electrical Engineering
                                          journal           homep               age:      www.            elsevier.com/                       locate/comp                        eleceng
Multi-level  graph  neural  network  for  text  sentiment  analysis
Wenxiong  Liao          a,  Bi  Zeng     a,  Jianqi  Liu      b,*,  Pengfei  Wei        a,  Xiaochun  Cheng            c,*,
Weiwen  Zhanga
aSchool of Computer, Guangdong University of Technology,  Guangzhou, Guangdong, China
bSchool of Automation, Guangdong University of Technology,  Guangzhou, Guangdong, China
cDepartment  of Computer Science, Middlesex University, London, UK
ARTICLE                          INFO                 ABSTRACT
Keywords:                                             Text sentiment analysis is a fundamental task in the field of natural language processing (NLP).
text sentiment analysis                               Recently, graph neural networks (GNNs) have achieved excellent performance in various NLP
graph neural network                                  tasks. However, a GNN only considers the adjacent words when updating the node representa-
attention mechanism                                   tions of the graph, and thus the model can only focus on the local features while ignoring global
deep learning                                         features. In this paper, we propose a novel multi-level graph neural network (MLGNN) for text
                                                      sentiment analysis. To consider both local features and global features, we apply node connection
                                                      windows with different sizes at different levels. Particularly, we integrate a scaled dot-product
                                                      attention mechanism as a message passing mechanism into our method for fusing the features
                                                      of each word node in the graph. The experimental results demonstrated that the proposed model
                                                      outperformed other models in text sentiment analysis tasks.
1. Introduction
    With the development of social networks, people surf the Internet and express their personal opinions on websites with increasing
frequency [1]. The masses of text, including private emotional disposition, are a high-quality data source for data mining, especially by
using sentiment analysis of text. According to the sentiment analysis results, different people often have different views and emotional
tendencies toward the same thing, and by understanding this phenomenon, powerful functionality for competitive analysis and
marketing analysis can be provided. Nowadays, sentiment analysis has been widely used in many fields, such as e-commerce, movie
recommendations, and public opinion analysis [2]. For example, the text sentiment analysis of users’ evaluation of commodities on an
e-commerce platform can provide valuable information in establishing a model of the user-commodity relationship. The e-commerce
platform can provide personalized recommendations for users, and merchants can update their commodities according to the
user-commodity relationship model in a timely manner. Text sentiment analysis of book reviews can help recommend books according
to the reader’s personal preferences. Publishers can better understand what books are popular based on the sentiment analysis result of
book reviews, and then publish a related book that meets the public interest.
    Text sentiment analysis, also known as opinion mining, is essentially a text classification task. Text sentiment analysis aims to
classify text into a predefined sentiment polarity according to its content. Text sentiment analysis methods can mainly be classified into
one of the following three groups: lexicon-based methods, shallow learning methods, and deep learning methods.
    The lexicon-based methods mainly evaluate the texts with the emotional disposition by matching the sentiment words in the
 *Corresponding authors.
    E-mail addresses: liujianqi@ieee.org (J. Liu), x.cheng@mdx.ac.uk (X. Cheng).
https://doi.org/10.1016/j.compeleceng.2021.107096
Received 31 July 2020; Received in revised form 24 October 2020; Accepted 10 March 2021

W. Liao et al.
sentiment   lexicon.   The   shallow   learning   methods   focus   on   feature   engineering,   and   they   usually   represent   text   with   hand-crafted
features,  such  as  sparse  lexical  features.  The  lexical  features  mainly  include  bag-of-words  (BOW)  and  n-grams.  Classifiers  are  used
in   shallow   learning   methods.   However,   due   to   their   limited   expressive   ability,   it   is   difficult   to   improve   the   performance   of   these
methods, and their generalization ability is also poor.
    Deep  learning methods  have  been  widely applied  in  text  sentiment analysis  because word2vec  [3] provides  an  effective  way  to
learn distributed representations of words. The most popular deep neural networks used for text sentiment analysis are convolutional
neural network (CNN) and recurrent neural network (RNN). In a CNN [4],                                      the text features representation is constructed by acquiring
the local information in the filter region, which makes it difficult for a CNN to learn the dependencies between distant positions. In
comparison, RNN and its variants (long short-term memory (LSTM) [5] and gated recurrent unit (GRU) [6] can connect contextual
memory and store more long-term global information, thus achieving the sentiment analysis of text. These methods mainly focus on
consecutive word sequences, but they do not explicitly use word co-occurrence information, and the complex model structure is like a
black-box.
    In recent years, a new neural network, named graph neural network (GNN) [7],                                     has aroused wide interest. In a GNN, a node collects
information from other nodes and updates its representations. Prior work [8] has proposed using a graph convolutional network (GCN)
[9] for text classification (Text_GCN). The Text_GCN model builds a graph for the whole corpus, which does not support online testing.
Another study [10] proposed a text-level GNN for text classification, which can avoid graph construction for the entire corpus, and
graphs are constructed for each input text. Like other GNN-based methods, this method [10] also only considers the adjacent nodes
when updating the node representations of the graph, and it fails to consider the relationship between non-adjacent nodes. This causes
the model to only focus on local features as in a CNN.
    To   deal   with   the   problems   above,   we   propose   a   novel   multi-level   graph   neural   network   (MLGNN)   for   text   sentiment   analysis.
Instead of building a graph for the entire corpus, we produce a graph for each input text. To imitate humans’                                                                                                          ability to understand
language at different levels, the word nodes within a small window are connected at the bottom level, the word nodes within a larger
window   are   connected   at   the   middle   level,   finally,   all   the   word   nodes   of   the   document   are   connected   at   the   top   level.   The   main
contributions can be summarized as follows:
 1     We integrate a scaled dot-product attention [11] mechanism into MLGNN for fusing the features of each word node in the graph.
 2     To enable the model to focus on both local features and global features, we propose using connection windows with different sizes
    at different levels. In previous studies, the connection windows of the nodes were fixed.
 3     The experimental results on public datasets demonstrated that, compared with similar methods, MLGNN could better handle text
    sentiment analysis tasks. At the same time, the influencing factors of MLGNN are also analyzed and discussed in this work.
    The remainder of this paper is organized as follows: In Section 2,               we review several related methods about text sentiment analysis.
In Section 3,               the MLGNN is introduced in detail. Section 4 verifies the effectiveness of MLGNN through experiments by comparing it
with previous methods. Finally, a conclusion is made in Section 5.
2.                Related  works
    At present, text sentiment analysis methods can be mainly divided into three categories: lexicon-based methods, shallow learning
methods, and deep learning methods. In this section, we will review these methods and discuss their advantages and disadvantages.
2.1.                 Lexicon-based  methods
    The lexicon-based methods summarize the widely used sentiment words as a sentiment dictionary and match the content of the
dictionary according to the input text. Then, the sentiment words in the text that coincide with the sentiment dictionary are found, and
the sentiment polarity of the text is finally determined. Esuli and Sebastiani [12] provided a publicly available lexical resource, named
SentiWordNet, for text sentiment analysis. It relies on a predefined lexicon. Although this eliminates the need for training data and
shortens  the  time  of  sentiment  analysis,  one  of  the  biggest  difficulties  with  lexicon-based  algorithms  is  that  they  require  too  many
manual   annotations.   Moreover,   because   of   its   dependence   on   manually   labeled   dictionaries,   it   cannot   be   applied   to   cross-domain
research.
2.2.                 Shallow  learning  methods
    The shallow learning methods, also known as traditional machine learning [13],                                                      mainly focus on feature engineering and classi-
fication.   For   feature   engineering,   the   shallow   learning   methods   usually   represent   text   with   sparse   lexical   features,   and   BOW   and
n-grams  are the  most popular sparse lexical features. BOW is  based on word frequency  statistics, and  it has  high computational ef-
ficiency; however, when BOW is used, the order information of words is ignored. N-grams captures partial information about word
order, and thus it has better performance than BOW. The traditional methods require various classifiers. Wang and Lin [14] discussed
the relationship between feature selection and classification performance.
    However, all of the shallow learning methods rely on hand-crafted features and the classifier has limited expression ability, which
leads to a bottleneck in model performance. To improve the model’s                                                                                   ability to express non-linear relationships, an increasing number
of researchers have turned to deep learning.

W. Liao et al.
2.3.                 Deep  learning  methods
    Deep learning methods have made great progress in many fields [15–          18]. Word2vec provides a method to learn distributed rep-
resentations of words, as a result, deep learning networks have been applied to sentiment analysis [19, 20]. Two representative deep
learning networks are RNN and CNN.
2.3.1.                Recurrent  neural  network
    The  RNN  and  its  variants  (LSTM  and  GRU)  can  relate  context  and  memorize  the  global  information,  and  thus  they  are  popular
methods for text sentiment analysis tasks. Ruan et al. [21] proposed the sequential neural encoder with latent structured description
(SNELSD) based on LSTM and achieved good results on a sentiment analysis dataset. In another study [19], a modality-gated LSTM was
proposed to learn the multimodal features for image-text sentiment analysis.
    As shown in Fig. 1, in LSTM, the input gate controls how much of the new state of the current computation is updated to the cell
memory. The forget gate determines how much information will be forgotten from the previous memory unit. The output gate controls
how  much  current  output  depends  on  the  current  memory  unit.  Although  LSTM  can  be  related  to  the  context,  the  structure  of  the
recurrent model is complicated. The multiple gates and memory cells rely on the previous time step, and the GPU parallelism cannot be
fully utilized. The RNN and its variants mainly focus on consecutive word sequences, and they do not explicitly use word co-occurrence
information.
2.3.2.                Convolutional  neural  network
    Since CNNs have achieved great success in other fields [22], especially in computer vision, researchers have also applied a CNN to
text sentiment analysis tasks. Unlike the convolution used in a computer vision [23] task, the convolution in text sentiment analysis
generally involves a one-dimensional CNN (1D-CNN). A 1D-CNN obtains the context information by imitating n-grams; if the size of the
filter region is l, then l is n in the n-grams. A typical 1D-CNN is shown in Fig. 2. Kim [4] applied a 1D-CNN to text classification using
pre-trained word embeddings. To predict the sentiment of reviews, Wehrmann et al. [24] proposed a language-agnostic translation free
method based on CNN and character-level embeddings.
    Compared with a recurrent model, a CNN is faster because of parallelization; however, a CNN only obtains the local feature in the
filter region, and it cannot obtain the contextual information of the text like a recurrent model.
2.3.3.                GNN
    In recent years, GNNs have attracted wide attention from researchers. A GNN has been used in many natural language processing
(NLP) tasks, including text classification [8, 10], neural machine translation, and relational reasoning [25]. Yao et al. [8] proposed a
GCN for text classification, known as Text_GCN, in which a single text graph is constructed for the corpus based on word co-occurrence
and document-word relationship. As shown in Fig. 3(A), Nodes beginning with “        O”            represent document nodes, and the others are word
nodes; the black lines represent the document-word relations, and the blue lines refer to the word-word relations. The Text_GCN model
builds   a   graph   for   the   whole   corpus,   which   causes   high   memory   consumption   and   does   not   support   online   testing.   Furthermore,
Text_GCN builds the graph based on word co-occurrence and document-word relations while ignoring the order information of words.
To address these problems, Huang et al. [10] proposed a new GNN-based method for text classification. Instead of building a single
corpus-level graph, they created a text-level graph. As shown in Fig. 3(B), Huang et al. connected word nodes within a small window in
the text. This method reflects the order of words in the sentence by the edge connection. However, when updating the node repre-
sentation  of   the   graph,   they   only   considered   the   adjacent   nodes   within   a   small  window   and  failed   to   consider   the   relationship   of
non-adjacent nodes; thus, the model only focuses on the local feature, and it is difficult for the model to learn the dependencies be-
tween distant nodes.
    To address the problems above, MLGNN produces a text graph with different connection windows. We propose three levels of node
connection: bottom level, middle level, and top level. The bottom level involves a small connection window, which mainly focuses on
the local features; the middle level has a larger connection window, mainly focusing on the long-distance features; the top level fully
connects all the word nodes directly, which focuses on the global features.
                                                           Fig. 1.            An illustration  of the  LSTM  model  [5].

W. Liao et al.
                                                                                                            Fig. 2.            An illustration  of the  1D-CNN  [4] model  structure.
                                                                          Fig. 3.            An illustration  of the  Text_GCN [8]  model and  the  work of Huang  et al. [10].
3.                Proposed  approaches
         In this section, we present the model structure of MLGNN in detail. First, we show how to build the graph and describe the message
passing    mechanism   at   the   bottom    level.   Second,   we    introduce    how    to   construct   the   graph    structure   and   use    the   graph   attention
network (GAT) [26] to update node representations at the middle level. Then, we demonstrate how to update the node representations
at the top level using the scaled dot-product attention mechanism. Finally, the output and loss functions of the MLGNN are introduced.
The model structure of MLGNN is as shown in Fig. 4.
3.1.                 Bottom  level
         We build the graph at the bottom level similar to a prior study [10]. The text with l words is denoted by T(0)={r(0),…          ,r(0),…          ,r(0)},
                                                                                                                                                                                                                                                                                                                          1                  i                  l
where r(0)denotes the representation of the ith word. r(0)is initialized by a word embedding with d0 dimension and updated during
                     i                                                                                                                                i
training.  For  a  given  text,  we treat  the  words  as  the  graph  nodes.  Each edge  starts  from a  word  and  ends  with  the  adjacent  words.
Supposing that p denotes the number of adjacent words connected to each word in the graph, then the graph can be defined as
                       (0 )                (0 )                     },                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (1)
                  N           ={ri              |i∈[1    ,l]
and
                      (0 )          {      (0 )                                                               }
                  E          =          eij     |i∈[1    ,l];  j∈[i p,i+p],                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (2)
where N(0) and E(0) are the node set and edge set of the graph at the bottom level.
         The message passing mechanism is employed to collect information from adjacent nodes and update its representations. Since the

W. Liao et al.
Fig. 4.            An illustration of MLGNN. For the convenience of the display, in this figure, we set the node connection windows p = 1 at the bottom level
and  set the  node connection windows  q =  2  at the  middle level.
connection  window in  the  bottom  layer is  small, the  words in  the  window have  similar  representations. The  representations of  the
adjacent nodes are averaged and used as the message passing mechanism of the bottom level, which is defined as
                         ̂r(1 )=averagea∈N                                            p  r(0 ).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (3)
                             i                                                        i     a
                        ̃r(1 )=(1     β)̂r(1 )+βr(0 ),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (4)
                            i                                               i                         i
and
                            (1 )                   (                     (1 )                         )
                         ri           =f                w     bot̃ri              +bbot                    ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           (5)
where   Np                             denotes   the   adjacent   nodes   of   the   i                                                                                                 node,   average   is   the   reduction   function   that   combines   the   average   values   on   each
                                i                                                                                                                                             th
dimension, and β     ∈R1 is a configurable parameter that indicates how much information of r(0)should be kept. Formula 5 transforms
                                                                                                                                                                                                                                                                                                                                                         i
node features into a new dimension. Here, wbot                                                                                                                                    ∈R            d0×d1, bbot                          ∈R            d1       are trainable matrices, and f is the non-linear activation function.
            At the bottom level, features are collected from adjacent nodes, the node features are updated, finally, the node representations are
converted into a new dimension d1. Since the connection window p is generally a very small value, it can be considered that the bottom
level mainly focuses on the local features of the text.
3.2.                 Middle level
            After the bottom level, the graph with l nodes is denoted by T(1)={r(1),…          ,r(1),…          ,r(1)}, where r(1)represents the ith node. In the
                                                                                                                                                                                                                                                                                  1                          i                          l                                              i
middle level, a larger node connection window is used. Suppose q denotes the number of adjacent nodes connected to each node at the
middle level, and then the graph can be defined as
                               (1 )                       (1 )                               },                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (6)
                         N               ={ri                     |i∈[1    ,l]
and
                              (1 )               {         (1 )                                                                                          }
                         E              =              eij        |i∈[1    ,l];        j∈[i q,i+q],                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     (7)
where N(1) and E(1) are the node set and edge set of the graph at the middle level, and we set q ≥ p.
            A larger connection window is used in the middle level, and the correlation between the long distant nodes and the current node
may be different; thus, it is not suitable to directly average the representations of adjacent nodes like in the bottom level. We use the
GAT [26] as the message passing mechanism at the middle level. As shown in Fig. 5, the GAT is defined as

W. Liao et al.
                                                                                                                 (                  (                       (1 )                                   (1 )))
                                 cij=  LeakyReLU                                                                        wa                wwri                           ‖wwrj                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (8)
and
                                                                         exp           c                 )
                                 αij=∑                                                              ij                      ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (9)
                                                                       k∈N          q   exp        (cik)
                                                                                    i
where     ||     represents     concatenation     operation,     wa                                                                                                                                                                                       ∈R                2d1,w                      w        ∈R                 d   1×d          1         are     trainable     parameters,     and     LeakyReLU     [27]     is     a     kind     of
non-linear function. Formula 8 can be used to calculate the correlation coefficient between adjacent nodes and the ith node. Formula 9
is a SoftMax function. Then, the representation of adjacent nodes is weighted and the node representations are transformed into a new
dimension as follows:
                                       (2 )                                     (                                  (1 ))
                                 ̂ri                =∑                                 αijwwrj                                      ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (10)
                                                                j∈N         q
                                                                            i
                                ̃r(2 )=(1     γ)̂r(2 )+γr(1 ),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (11)
                                      i                                                             i                                i
and
                                      (2 )                         (                              (2 )                                    )
                                 ri                =f                     w      mid̃ri                       +bmid                             ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (12)
where  γ     ∈R1 is a configurable parameter that indicates how much information of r(1)should be kept. Formula 12 transforms node
                                                                                                                                                                                                                                                                                                                                                                                                                                  i
features to a new dimension. Here, wmid                                                                                                                                                                  ∈R                d1×d2, bmid                                        ∈R                 d2          are trainable matrices, and f is a non-linear activation function.
3.3.                 Top  level
                After the middle level, the graph with l nodes is denoted by T(2)={r(2),...,r(2),...,r(2)}. To grasp the features of the full text, all the
                                                                                                                                                                                                                                                                                                                                                                   1                               i                                l
nodes are connected to each other at the top level. The graph can be defined as
                                         (2 )                                (2 )⃒                                             },                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            (13)
                                 N                    ={ri                             ⃒i∈[1    ,l]
and
                                                                 {                     ⃒                                                                                 }
                                        (2 )                                 (2 )⃒
                                 E                   =                  eij            ⃒i∈[1    ,l];  j∈[1    ,l],                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      (14)
where N(2) and E(2) are the node set and edge set of the graph at the top level.
                The     multi-head     attention     mechanism     is     used     as     the     message     passing     mechanism      at     the     top     level.     The     multi-head     attention
mechanism consists of multiple scaled dot-product attention mechanisms. The scaled dot-product attention is a mechanism defined as
                                                                                                                                                                                                                                              Fig.  5.            An illustration  of the  GAT [26].

W. Liao et al.
                                                                 (2 ))(                         (2 ))T
                                                  w     Q    ri                   w     K    rj
                          cij=                                          √                                            ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (15)  ̅̅̅̅̅
                                                                                d3
                                                         exp           c          )
                          α           =∑                                      ij                ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              (16)
                               ij                       l          exp        (c            )
                                                        k=1                            ik
and
                               (3 )                             l                 (                   (2 ))
                          ̂ri            =∑                     j=1      αij           w     V    rj               .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (17)
where wQ                                ∈R             d2×d3, wK                            ∈R             d2×d3, andwV                                       ∈R             d2×d3                are trainable parameters. Formula 15 calculates the correlation coefficient between
all the other nodes and the ith node.
             Supposing that there are H scaled dot-product attention in the multi-head attention mechanism, then
                                                      (                (          H                     ))
                          r(3 )=f                           W       o             ‖̂r(3 )                            ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              (18)
                              i                                                h=1              ih
where w                               ∈R             hd3×d3                 is a trainable matrix, ̂r(3)is the h                                                                                               scaled dot-product attention result of the i                                                                                                                              node, and || represents concatenation
                                o                                                                                                                                  ih                                  th                                                                                                                                                                         th
operation.
3.4.                 Output
             By concatenating the node representations of the top level output, we can obtain the following:
                          r =r(3 )‖...‖r(3 )‖...‖r(3 ),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (19)
                                             1                                i                                l
where || represents concatenation operation.
             We can obtain the sentiment tendency through a fully connected (FC) layer with SoftMax as follows:
                          ̂y  =S   oftMax                 (w                       predr +bpred),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (20)
where wpred                                    ∈R             ld3×c is a trainable matrix mapping the vector into an output space, bpred ∈Rc is trainable bias, and c is the number of
predefined emotional polarity categories.
3.5.                 Loss  function
             The goal of training is to minimize the cross-entropy loss between the ground-truth label and the predicted label as follows:
                                                                 N
                          loss= ∑                                          yilog         ̂yi,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (21)
                                                                  i
where yi is the ith ground-truth label, and N is the number of samples.
4.                Experiments
             In this section, we describe our experimental setup and report our experimental results.
4.1.                 Datasets
             The following sentiment analysis datasets of different domains were used to verify the effectiveness of MLGNN:
             SST-binary: The SST-binary dataset is from Stanford Sentiment Treebank (SST-fine) [28] of movie reviews. SST-fine contains five
classes: strong positive, positive, neutral, negative, and strong negative. The SST-binary dataset was constructed from the same dataset.
In  SST-binary,  strong  positive  and  positive  are  mapped  to  positive,  strong  negative  and  negative  are  mapped  to  negative,  and  the
sentences from the neutral class are removed. We divided the dataset into subsets of 6920/872/1821 corresponding to the training set,
validation set, and testing set, respectively.
             Sentube-A: The Sentube-A [29] dataset consists of texts taken from YouTube comments regarding automobiles. These comments
were based on the videos and commercials that contained product information. The texts include two sentiment classes: positive and
negative.  The  dataset  was  divided   into  subsets  of   3381/225/903  corresponding  to   the   training  set,  validation  set,  and  testing  set,
respectively.
             Sentube-T: The Sentube-T [29] dataset includes the texts taken from YouTube comments regarding tablets. Like Sentube-A, the
texts contain two sentiment classes: positive and negative. The dataset was divided into subsets of 4997/333/1334 corresponding to

W. Liao et al.
the training set, validation set, and testing set, respectively.
       The statistics of datasets are shown in Table 1.              In Table 1,              the “Avg.           Sentence Length”                                                                                              represents the average sentence length of the
dataset, the “Vocabulary           Size”                                                            denotes the dictionary size of the dataset, and the “Number           of Labels”                                                                                       stands for the annotation scheme.
4.2.                 Methods  for  comparison
       To objectively evaluate the performance of the proposed method in sentiment analysis tasks, five classifiers from Barnes, Klinger,
and Walde’s                                                                                      paper [30] were employed in our experiment, and their published results were used as baselines. Two of the classifiers
(BOW and AVE) are based on logistic regression, and the remaining three (LSTM, BiLSTM, and CNN) are neural network based. Be-
sides, we also compared our proposed method with the GNN-based method of Huang et al. [10].
       BOW:                                                                 BOW is a logistic regression classifier trained on a BOW representation of the training examples.
       AVE: AVE is a logistic regression classifier trained on the average of the word embedding of the training example.
       LSTM: LSTM has an embedding layer with different dimensions. These vectors are then passed to the LSTM layer. The final hidden
state   is   connected   through   a   fully   connected   layer   with   50-dimensional   training   and   then   connected   to   the   SoftMax   layer,   which
provides the probability distribution in a predefined class. Besides, we used a dropout of 0.5 before the LSTM layer.
       BiLSTM: BiLSTM is like LSTM, and BiLSTM is a bidirectional LSTM.
       CNN: CNN has a convolutional layer connected to the pre-trained word embeddings. The convolutional layer has filter sizes of 2, 3,
and 4, followed by a pooling layer. Then, it is sent to a fully connected layer through ReLU activation, and, finally, it is connected by a
SoftMax layer. Besides, we used a dropout of 0.5 before and after the convolutional layers.
       Huang et al.: This method refers to a study by Huang et al. [10],                                                    in which, the word nodes were connected within a small window,
and  the  node  representations  were  updated  by  a  message  passing  mechanism.  We  tuned  the   hyperparameters  by  grid  search.  The
connection window for the adjacent nodes was two since our experimental results showed that it could achieve the best performance.
Besides, we used a dropout of 0.5.
       MLGNN: This is the method proposed in this paper. We tuned the hyperparameters using grid search. The hyperparameters that
were finally used in the MLGNN are shown in Table 2.              Here, p is the connection windows of the bottom level and the q is the connection
windows of middle level. Besides, we used a dropout of 0.5.
       The     word     embeddings     we     used     50,     100,     200-dimensional     training     by     the     word2vec     algorithm     on     a     2016     Wikipedia     dump.
Moreover, we used the Google 300-dimensional word2vec embeddings, which were trained on part of the GoogleNews dataset.
       The  Adam  optimizer  with  an  initial  learning  rate  of  1e-3  was  used  in  our  work.  We  implemented  our  model  using  Python  on  a
machine  with  a  configuration of  NVIDIA  1080Ti. For  all the  experiments, we  selected  the  model  with  the  best  performance  on  the
validation set and evaluated its performance on the test set. All the experiments in this paper ran five times, and the mean value and
standard deviation of results were used as the result.
4.3.                 Experimental  results
       The results for the seven models are shown in Table 3.              BOW was a strong baseline. Even though it never provided the best result on
any dataset, it outperformed AVE on SST-binary and SenTube-T. As for AVE, it had better performance than BOW, LSTM, BiLSTM, and
CNN on SenTube-A. BiLSTM outperformed LSTM in all the datasets. LSTM performed better than BOW and AVE on SST-binary. CNN
did not outperform other methods on any dataset.
       The method of Huang et al. and MLGNN are both GNN based methods. The results show that GNN based methods can effectively
improve the performance of text sentiment analysis. The method of Huang et al. outperformed BOW, AVE, LSTM, BiLSTM, and CNN on
SenTube-T. Particularly, MLGNN outperformed all the other methods on all the datasets. Furthermore, MLGNN had the highest ac-
curacy with 300-dimensional word embedding on SST-binary and SenTube-A, and with 100-dimensional embedding on SenTube-T.
4.4.                 Analysis
       In Table 4,               the MLGNN/middle is the MLGNN model without the middle level, and the MLGNN/top is the MLGNN model without
the top level. As shown, neither MLGNN/middle nor MLGNN/top outperformed MLGNN, which proves the effectiveness of the middle
level   and   top   level   for   text   sentiment   analysis.   Since   no   global   features   were   extracted,   MLGNN/top   had   the   worst   performance.
MLGNN had the best performance in consideration of both local features and global features. Moreover, in MLGNN, the adjacent word
nodes were explicitly connected, and the node representations were updated by collecting features from adjacent nodes, which is not
possible to achieve using LSTM.
       The influences of different connection windows q of the middle level on the model performance are shown in Fig. 6.                The bottom
Table 1
The statistics of the datasets.
                                            Train                                                                                                                          Test                                                                                                                                    Dev.                                                                                                                          Avg. Sentence Length                                                                                                                          Vocabulary Size                                                                                                                          Number of Labels
   SST-binary                                                                                                                          6920                                                                                                                             1821                                                                                                                          872                                                                                                                                 19.67                                                                                                                                                                                                                                                                                   17,539                                                                                                                                                                                                                 2
   SenTube-A                                                                                                                           3381                                                                                                                             903                                                                                                                                       225                                                                                                                                 28.54                                                                                                                                                                                                                                                                                   18,569                                                                                                                                                                                                                 2
   SenTube-T                                                                                                                             4997                                                                                                                             1334                                                                                                                          333                                                                                                                                 28.73                                                                                                                                                                                                                                                                                   20,276                                                                                                                                                                                                                 2

W. Liao et al.
Table 2
The hyperparameters of MLGNN.
                                                                                                                                                                                                                       p                                                                                                                                                         q                                                                                                                                                        H                                                                                                    β                                                                                                                γ                                                                                                                                                                         d1                                                                                                                                      d2                                                                                                                                        d3
               SST-binary                                                                                                                                                        1                                                                                                                                                       2                                                                                                                                                        8                                                                                                                                                           0.2                                                                                                                                                       0.2                                                                                                                                                       200                                                                                                                                                       24                                                                                                                                                        24
               SenTube-A                                                                                                                                                        2                                                                                                                                                       4                                                                                                                                                        8                                                                                                                                                           0.2                                                                                                                                                       0.2                                                                                                                                                       128                                                                                                                                                       16                                                                                                                                                        16
               SenTube-T                                                                                                                                                          2                                                                                                                                                       3                                                                                                                                                        8                                                                                                                                                           0.2                                                                                                                                                       0.2                                                                                                                                                       256                                                                                                                                                       40                                                                                                                                                        40
Table 3
Accuracy on the test sets.
               Method                                                                                                                                                                                                                                                                                                                Dim.                                                                                                                                                                                                                                                                    SST-binary                                                                                                                                                                                                                                                                       SenTube-A                                                                                                                                                                                                                                                                       SenTube-T
               BOW                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  –                                                                                                                                                                                                                                                                                                                   80.7                                                                                                                                                                                                                                                                                                                                     60.6                                                                                                                                                                                                                                                                                                                                     66
               AVE                                                                                                                                                                                                                                                                                                                                                50                                                                                                                                                                                                                                                                                          74.1                                                                                                                                                                                                                                                                                                                                     62                                                                                                                                                                                                                                                                                                                                                       61.7
                                                                                                                                                                                                                                                                                                                    100                                                                                                                                                                                                                                                                              76.7                                                                                                                                                                                                                                                                                                                                     61.5                                                                                                                                                                                                                                                                                                                                     61.8
                                                                                                                                                                                                                                                                                                                    200                                                                                                                                                                                                                                                                              78.2                                                                                                                                                                                                                                                                                                                                     60.6                                                                                                                                                                                                                                                                                                                                     62.8
                                                                                                                                                                                                                                                                                                                    300                                                                                                                                                                                                                                                                              80.3                                                                                                                                                                                                                                                                                                                                     61.5                                                                                                                                                                                                                                                                                                                                     64.3
               LSTM                                                                                                                                                                                                                                                                                                                                   50                                                                                                                                                                                                                                                                                          80.5 ± 0.4                                                                                                                                                                                                                                                                        58.9 ± 0.8                                                                                                                                                                                                                                                                        63.4 ± 3.1
                                                                                                                                                                                                                                                                                                                    100                                                                                                                                                                                                                                                                              79.5 ± 0.6                                                                                                                                                                                                                                                                        58.9 ± 1.1                                                                                                                                                                                                                                                                        63.1 ± 0.4
                                                                                                                                                                                                                                                                                                                    200                                                                                                                                                                                                                                                                              80.9 ± 0.6                                                                                                                                                                                                                                                                        58.6 ± 0.6                                                                                                                                                                                                                                                                        65.2 ± 1.6
                                                                                                                                                                                                                                                                                                                    300                                                                                                                                                                                                                                                                              81.7 ± 0.6                                                                                                                                                                                                                                                                        57.4 ± 1.3                                                                                                                                                                                                                                                                        63.6 ± 0.7
               BiLSTM                                                                                                                                                                                                                                                                                                               50                                                                                                                                                                                                                                                                                          82.9 ± 0.7                                                                                                                                                                                                                                                                        59.5 ± 1.1                                                                                                                                                                                                                                                                        65.6 ± 1.2
                                                                                                                                                                                                                                                                                                                    100                                                                                                                                                                                                                                                                              79.8 ± 1.0                                                                                                                                                                                                                                                                        58.6 ± 0.8                                                                                                                                                                                                                                                                        66.4 ± 1.4
                                                                                                                                                                                                                                                                                                                    200                                                                                                                                                                                                                                                                              80.1 ± 0.6                                                                                                                                                                                                                                                                        58.9 ± 0.3                                                                                                                                                                                                                                                                        63.3 ± 1.0
                                                                                                                                                                                                                                                                                                                    300                                                                                                                                                                                                                                                                              82.6 ± 0.7                                                                                                                                                                                                                                                                        59.3 ± 1.0                                                                                                                                                                                                                                                                        66.2 ± 1.5
               CNN                                                                                                                                                                                                                                                                                                                                             50                                                                                                                                                                                                                                                                                          81.7 ± 0.3                                                                                                                                                                                                                                                                        55.2 ± 0.7                                                                                                                                                                                                                                                                        57.4 ± 3.1
                                                                                                                                                                                                                                                                                                                    100                                                                                                                                                                                                                                                                              81.6 ± 0.5                                                                                                                                                                                                                                                                        56.0 ± 2.2                                                                                                                                                                                                                                                                        61.5 ± 1.1
                                                                                                                                                                                                                                                                                                                    200                                                                                                                                                                                                                                                                              80.7 ± 0.4                                                                                                                                                                                                                                                                        56.3 ± 1.8                                                                                                                                                                                                                                                                        64.1 ± 1.1
                                                                                                                                                                                                                                                                                                                    300                                                                                                                                                                                                                                                                              81.3 ± 1.1                                                                                                                                                                                                                                                                        57.3 ± 0.5                                                                                                                                                                                                                                                                        62.1 ± 1.0
               Huang et al.                                                                                                                                                                                                                                                                    50                                                                                                                                                                                                                                                                                          82.5 ± 0.1                                                                                                                                                                                                                                                                        60.0 ± 0.3                                                                                                                                                                                                                                                                        67.3 ± 0.1
                                                                                                                                                                                                                                                                                                                    100                                                                                                                                                                                                                                                                              82.6 ± 0.1                                                                                                                                                                                                                                                                        60.2 ± 0.2                                                                                                                                                                                                                                                                        67.3 ± 0.1
                                                                                                                                                                                                                                                                                                                    200                                                                                                                                                                                                                                                                              82.5 ± 0.3                                                                                                                                                                                                                                                                        59.7 ± 0.1                                                                                                                                                                                                                                                                        67.1 ± 0.1
                                                                                                                                                                                                                                                                                                                    300                                                                                                                                                                                                                                                                              82.6 ± 0.2                                                                                                                                                                                                                                                                        59.7 ± 0.1                                                                                                                                                                                                                                                                        67.4 ± 0.2
               MLGNN                                                                                                                                                                                                                                                                                                             50                                                                                                                                                                                                                                                                                          81.3 ± 0.5                                                                                                                                                                                                                                                                        61.5 ± 2.9                                                                                                                                                                                                                                                                        67.9 ± 0.5
                                                                                                                                                                                                                                                                                                                    100                                                                                                                                                                                                                                                                              82.4 ± 0.5                                                                                                                                                                                                                                                                        61.2 ± 3.5                                                                                                                                                                                                                                                                        68.1 ±                  0.5
                                                                                                                                                                                                                                                                                                                    200                                                                                                                                                                                                                                                                              82.4 ± 1.1                                                                                                                                                                                                                                                                        61.5 ± 1.5                                                                                                                                                                                                                                                                        67.7 ± 0.6
                                                                                                                                                                                                                                                                                                                    300                                                                                                                                                                                                                                                                              83.1 ±                  0.4                                                                                                                                                                                                                                                                    62.2 ±                  0.5                                                                                                                                                                                                                                                                    68.0 ± 0.4
Table 4
The influences of the middle level and top level.
               Method                                                                                                                                                                                                                                                                                                                                            Dim.                                                                                                                                                                                                                                                           SST-binary                                                                                                                                                                                                                                                             SenTube-A                                                                                                                                                                                                                                                              SenTube-T
               MLGNN/middle                                                                                                                                                                                                                                                           50                                                                                                                                                                                                                                                                                 82.0 ± 0.2                                                                                                                                                                                                                                                               61.6 ± 1.3                                                                                                                                                                                                                                                               67.8 ± 0.1
                                                                                                                                                                                                                                                                                                                                         100                                                                                                                                                                                                                                                                     82.2 ± 0.1                                                                                                                                                                                                                                                               60.4 ± 1.7                                                                                                                                                                                                                                                               67.8 ± 0.1
                                                                                                                                                                                                                                                                                                                                         200                                                                                                                                                                                                                                                                     82.1 ± 3.8                                                                                                                                                                                                                                                               61.5 ± 1.0                                                                                                                                                                                                                                                               67.4 ± 0.5
                                                                                                                                                                                                                                                                                                                                         300                                                                                                                                                                                                                                                                     83.0 ± 2.6                                                                                                                                                                                                                                                               61.5 ± 1.1                                                                                                                                                                                                                                                               66.9 ± 1.2
               MLGNN/top                                                                                                                                                                                                                                                                                              50                                                                                                                                                                                                                                                                                 80.8 ± 0.7                                                                                                                                                                                                                                                               58.6 ± 0.2                                                                                                                                                                                                                                                               67.4 ± 0.1
                                                                                                                                                                                                                                                                                                                                         100                                                                                                                                                                                                                                                                     82.2 ± 0.1                                                                                                                                                                                                                                                               58.2 ± 0.8                                                                                                                                                                                                                                                               67.7 ± 0.9
                                                                                                                                                                                                                                                                                                                                         200                                                                                                                                                                                                                                                                     82.0 ± 1.5                                                                                                                                                                                                                                                               59.3 ± 0.2                                                                                                                                                                                                                                                               66.7 ± 1.1
                                                                                                                                                                                                                                                                                                                                         300                                                                                                                                                                                                                                                                     81.9 ± 0.4                                                                                                                                                                                                                                                               58.7 ± 0.7                                                                                                                                                                                                                                                               67.3 ± 1.1
               MLGNN                                                                                                                                                                                                                                                                                                                                        50                                                                                                                                                                                                                                                                                 81.3 ± 0.5                                                                                                                                                                                                                                                               61.5 ± 2.9                                                                                                                                                                                                                                                               67.9 ± 0.5
                                                                                                                                                                                                                                                                                                                                         100                                                                                                                                                                                                                                                                     82.4 ± 0.5                                                                                                                                                                                                                                                               61.2 ± 3.5                                                                                                                                                                                                                                                               68.1 ±                  0.5
                                                                                                                                                                                                                                                                                                                                         200                                                                                                                                                                                                                                                                     82.4 ± 1.1                                                                                                                                                                                                                                                               61.5 ± 1.5                                                                                                                                                                                                                                                               67.7 ± 0.6
                                                                                                                                                                                                                                                                                                                                         300                                                                                                                                                                                                                                                                     83.1 ±                  0.4                                                                                                                                                                                                                                                           62.2 ±                  0.5                                                                                                                                                                                                                                                           68.0 ± 0.4
connection windows of SST-binary, SenTube-A, and SenTube-T were 1, 2, and 2, respectively. As shown in Fig. 6, if the q value was too
large or too small, the performance of the models degraded. When q =2, optimal performance was achieved on SST-binary; when q =
4, optimal performance was achieved on SenTube-A; when q =3, optimal performance was achieved on SenTube-T. q is the connection
window of the middle layer. If the value of q is too small, then the features extracted from the middle layer will be like those extracted
from the bottom layer; if q is too large, then some features will be lost. Therefore, a suitable q value can improve the sentiment analysis
performance of MLGNN.
5.                Conclusion  and  future  work
                               Sentiment analysis is one of the research fields of NLP. This paper proposes a GNN model MLGNN with different sizes of connection
windows at different levels, in which the node representations are updated with different message passing mechanisms. Specifically,

W. Liao et al.
                                                              Fig. 6.            Model  performance of  different q  values.
we  propose  to  use  a  small  connection  window  at  the  bottom  layer  and  aggregate  the  feature  representations  of  adjacent  words  by
average. In the middle layer, we use a larger connection window, and GAT is applied as the message passing mechanism. All the words
in the sentence are connected at the top level, and the node features are updated by integrating multi-head attention. The experimental
results    show    that    our    approach    outperformed    the    baseline    models    and    can    be    generalized    to    different    datasets.    Besides,    we    also
explored and analyzed the influencing factors of the model.
    According  to  the  experimental  results  and  analysis,  the  following  factors  may  affect  the  performance  of  text  sentiment  analysis
methods:
 1     Methods based on logistic regression cannot capture the nonlinear relationship, and thus it is difficult to improve its performance.
 2     The CNN-based methods have a problem correlating the context, while the LSTM methods based on time-series feature processing
    fail to pay full attention to local features when processing sentiment analysis tasks.
 3     MLGNN focuses on different features in different layers and considers both global and local features; thus, it had better performance
    in the experiments.
    In  MLGNN,  we  only  use  the  connection  of  edges  to  represent  the  sequence  relationship  of  words  in  a  sentence,  but  there  is  no
explicit sequence relationship. In future work, we will focus on encoding the position information of nodes in the graph. Recently, pre-
trained  language  models  have  achieved  advanced  performance  in  many  natural  language  processing  tasks,  and  thus,  in  our  future
research, we will also consider a combination with pre-trained language models.
Declaration  of  Competing  Interest
    The authors declared that they have no conflicts of interest to this work.
Author  statement
    The research in this paper is carried out under the supervision of Bi Zeng and Jianqi Liu. All authors contributed as follows:
    Wenxiong Liao: Performing the main experiments and writing-original manuscript.
    Bi Zeng: Theoretical guidance and experiments guidance.
    Jianqi Liu: Guidance on paper revision, fund support and as the corresponding author of this article.
    Pengfei Wei: Experimental programming and data processing.
    Xiaochun Cheng: Writing-reviewing and editing.
    Weiwen Zhang: Visualization and investigation.
    All authors read and contributed to the manuscript.
Acknowledgment
    This work was partially supported by the Science and Technology Program of Guangzhou, China (No. 201804010238), the Natural
Science  Foundations  of  Guangdong  Province,  China (2018A030310540),  Guangdong  Basic and  Applied  Basic  Research  Foundation
(No. 2019A1515011056), the National Natural Science Foundation of China (No. 61701122). .
References
 [1]             Wang Z, Ho  S-B, Cambria E. A  review of emotion sensing: categorization models and algorithms. Multimedia Tools and Applications; 2020. p. 1–        30.
 [2]             Tan S, Yang L, Sun H, Guan Z, Yan X, Bu J, Chen C, He X. Interpreting the public sentiment variations on twitter. IEEE Trans Knowl Data Eng 2014;26:1158–        70.
 [3]             Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their compositionality. Adv Neural Inf Process Syst
      2013:3111–        9.
 [4]             Y. Kim. Convolutional neural networks for sentence classification. arXiv preprint 14085882(2014).
 [5]             Hochreiter S, Schmidhuber J. Long Short-Term Memory. Neural Comput 1997;9:1735–        80.
 [6]             J. Chung, C. Gulcehre, K. Cho, Y. Bengio.  Empirical evaluation of  gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:14123555.
      (2014).
 [7]             Scarselli F, Gori M, Tsoi AC, Hagenbuchner M, Monfardini G.  The graph neural network model. IEEE Trans Neural Netw 2008;20:61–        80.
 [8]             Yao L, Mao C, Luo Y. Graph convolutional networks for text classification. In: Proceedings of the AAAI conference on artificial intelligence; 2019. p. 7370–        7.

W. Liao et al.
 [9]             T.N. Kipf, M. Welling. Semi-supervised classification with graph  convolutional networks. arXiv preprint arXiv:160902907. (2016).
[10]             Huang L, Ma D, Li S, Zhang X, Wang H. Text level graph neural network for text classification. In: Proceedings of the conference on empirical methods in natural
       language processing and the 9th international joint conference on natural language  processing (EMNLP-IJCNLP); 2019. p. 3444–        50.
[11]             Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser   Ł          , Polosukhin I. Attention is all you need. Adv Neural  Inf Process Syst 2017:
       5998–        6008.
[12]             Esuli A, Sebastiani F. Sentiwordnet: a publicly available lexical resource for opinion mining. LREC Citeseer 2006:417–        22.
[13]             Haider SK, Jiang A, Jamshed MA, Pervaiz H, Mumtaz S. Performance enhancement in P300 ERP single trial by machine learning adaptive denoising mechanism.
       IEEE Network  Lett 2018;1:26–        9.
[14]             Wang Z, Lin Z. Optimal  feature selection for learning-based algorithms for sentiment classification. Cognit Comput 2020;12:238–        48.
[15]             Zhou Z, Liao H, Gu B, Huq KMS, Mumtaz S, Rodriguez J. Robust mobile crowd sensing: when deep learning meets edge computing. IEEE Netw 2018;32:54–        60.
[16]             Vinayakumar R, Soman K, Poornachandran P, Alazab M, Jolfaei A. DBD: deep learning DGA-based botnet detection. Deep learning applications for  cyber
       security. Springer; 2019.  p. 127–        49.
[17]             Huang F, Xu J, Weng J. Multi-task travel route planning with a flexible deep learning framework. IEEE Trans Intell Transp Syst 2020.
[18]             Aggarwal A, Rani A, Sharma P, Kumar M, Shankar A, Alazab M. Prediction of landsliding using univariate forecasting models. Internet Technol Lett 2020.
[19]             Huang F, Wei K, Weng J, Li Z. Attention-based modality-gated networks for image-text sentiment analysis. ACM Trans Multimed Comput Commun Appl 2020;
       16:1–        19.
[20]             Xu J, Li Z, Huang F, Li CZ, Yu PS. Social image sentiment analysis by exploiting multimodal content and heterogeneous relations. IEEE Trans Ind Inf 2020:1–        8.
       PP.
[21]             Ruan YP, Chen Q, Ling ZH. A sequential neural encoder with latent structured description for modeling sentences. IEEE/ACM Trans Audio Speech Lang Process
       2018;26:231–        42.
[22]             Khan S, Muhammad K, Mumtaz S, Baik SW, de Albuquerque VHC. Energy-efficient deep CNN for smoke detection in foggy IoT environment. IEEE Int Things J
       2019:9237–        45.
[23]             Yue Z, Ding S, Zhao L, Zhang Y,  Cao Z, Tanveer M, Jolfaei A, Zheng X. Privacy-preserving time series medical images  analysis using a hybrid deep learning
       framework. ACM Trans Int Technol  2019;37:1–        22.
[24]             Wehrmann J, Becker W, Cagnini HE, Barros RC. A character-based convolutional neural network for language-agnostic Twitter sentiment analysis. In:
       Proceedings of  the international joint conference on neural networks (IJCNN). IEEE; 2017. p. 2384–        91.
[25]             Battaglia P, Pascanu R, Lai M, Rezende DJ. Interaction networks for learning about objects, relations and physics. Adv Neural Inf Process Syst 2016:4502–        10.
[26]             P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P.  Lio, Y. Bengio. Graph attention  networks. arXiv preprint  arXiv:171010903. (2017).
[27]             Clevert D-A, Unterthiner  T, Hochreiter S. Fast and accurate deep network learning by exponential linear units (ELUs). CoRR. 2016 abs/1511.07289.
[28]             Socher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C. Recursive deep models for  semantic compositionality over a sentiment treebank. In:
       Proceedings of  the conference on empirical methods in  natural language processing; 2013. p. 1631–        42.
[29]             Uryupina O, Plank B, Severyn A, Rotondi A, Moschitti A. SenTube:  a corpus for sentiment  analysis on YouTube social media. LREC 2014:4244–        9.
[30]             J. Barnes, R. Klinger, S.S.i. Walde. Assessing  state-of-the-art sentiment models on state-of-the-art sentiment  datasets. arXiv preprint 170904219. (2017).
WenXiong Liao was born in 1995. He received the bachelor’    s degree in Information Management and Information System from Guangzhou Medical University (GZMU).
He  received  the  degree  of  M.Eng.  in  computer  technology  from  Guangdong  University  of  Technology  (GDUT), China.  His-current  research  interests  include  AI  and
natural language processing.
Bi Zeng obtained both her degrees of M.S. and Ph.D. from Guangdong University of Technology (GDUT). Now, she is a Professor in the School of Computers, GDUT. At
present, her researches focus on the areas of intelligent robot, computational intelligence, data mining, and wireless sensor networks.
Jianqi Liu received his Ph.D. degree in control science and engineering from the School of Automation, Guangdong University of Technology (GDUT) in 2016. He is an
Associate Professor in the School of Automation, GDUT, China. His-has broad research interests, including big data, the Internet of Vehicles (IoV), and the cyber-physical
systems. He is a member of IEEE.
Pengfei Wei was born in 1991. He received his B. E. degree from Zhengzhou University, China. Now, he is studying for a M.Eng. degree in Guangdong University of
Technology (GDUT), China. His-current research interests include natural language processing, task dialog system, reinforcement learning, graph neural network.
Xiaochun Cheng (SM’    04) received his Ph.D. in Computer Science in 1996, and a degree of Executive MBA in 2011. Since 2012, he has worked as the Computer Science
Project Coordinator in Middlesex University. He is the member of the IEEE SMC Technical Committee on Computational Intelligence, the IEEE Communications Society
Communications and Information Security Technical Committee.
Weiwen Zhang received the Ph.D. in Computer Engineering from Nanyang Technological University (NTU), Singapore in 2015. Now, he is an Associate Professor in the
School of Computers at Guangdong University of Technology (GDUT), China. His-research interests include the areas of cloud computing, mobile computing, big data
analytics and machine learning. He is an IEEE member.

