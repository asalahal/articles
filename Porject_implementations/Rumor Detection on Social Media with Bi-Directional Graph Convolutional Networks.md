                                                                                                          The       Thirty-Fourth       AAAI       Conference       on       Artiﬁcial       Intelligence       (AAAI-20)
                                                                                                         Rumor                Detection                on                Social                Media                with
                                                                                       Bi-Directional                Graph                Convolutional                Networks
    Tian              Bian,1,2                Xi              Xiao,1                Tingyang              Xu,2                Peilin              Zhao,2                Wenbing              Huang,2                Yu              Rong,2                Junzhou              Huang2
                                                                                                                                                                      1  Tsinghua           University
                                                                                                                                                                              2  Tencent           AI           Lab
                                      bt18@mails.tsinghua.edu.cn,           xiaox@sz.tsinghua.edu.cn,           hwenbing@126.com,           yu.rong@hotmail.com
                                                                                                                     {tingyangxu,           masonzhao,           joehhuang}@tencent.com
                                                                                Abstract                                                                                                                           Decision            Tree            (Castillo,            Mendoza,            and            Poblete            2011),            Ran-
       Social               media               has               been               developing               rapidly               in               public               due               to                     dom                  Forest                  (Kwon                  et                  al.                  2013),                  Support                  Vector                  Machine
       its              nature              of              spreading              new              information,              which              leads              to              ru-                            (SVM)                   (Yang                   et                   al.                   2012).                    Some                   studies                   apply                   more                   ef-
       mors                  being                  circulated.                  Meanwhile,                  detecting                  rumors                  from                                               fective                     features,                     such                     as                     user                     comments                     (Giudice                     2010),
       such          massive          information          in          social           media          is          becoming          an          ar-                                                               temporal-structural           features           (Wu,           Yang,           and           Zhu           2015),           and
       duous          challenge.          Therefore,          some          deep          learning          methods          are                                                                                   the               emotional               attitude               of               posts               (Liu               et               al.               2015).               However,
       applied         to         discover         rumors         through         the         way         they         spread,         such                                                                        those            methods            mainly            rely            on            feature            engineering,            which            is
       as             Recursive             Neural             Network             (RvNN)             and             so             on.             However,                                                      very                time-consuming                and                labor-intensive.                Moreover,                those
       these             deep             learning             methods             only             take             into             account             the             pat-                                     handcrafted          features           are           usually           lack           of           high-level           represen-
       terns              of              deep              propagation              but              ignore              the              structures              of              wide                            tations           extracted           from           the           propagation           and           the           dispersion           of
       dispersion          in          rumor          detection.          Actually,          propagation          and          dis-                                                                                rumors.
       persion            are            two            crucial            characteristics            of            rumors.            In            this            pa-                                                   Recent           studies           have           exploited           deep           learning           methods           that
       per,              we              propose              a              novel              bi-directional              graph              model,              named
       Bi-Directional             Graph             Convolutional             Networks           (Bi-GCN),           to                                                                                            mine           high-level           representations           from           propagation           path/trees
       explore              both              characteristics              by              operating              on              both              top-down                                                       or           networks           to           identify           rumors.           Many           deep           learning           models
       and                bottom-up                propagation                of                rumors.                It                leverages                a                GCN                             such        as        Long        Short        Term        Memory        (LSTM),        Gated        Recurrent
       with             a             top-down             directed             graph             of             rumor             spreading             to             learn                                      Unit              (GRU),              and              Recursive              Neural              Networks              (RvNN)              (Ma
       the              patterns              of              rumor              propagation;              and              a              GCN              with              an              op-                  et              al.              2016;              Ma,              Gao,              and              Wong              2018)              are              employed              since
       posite          directed          graph          of          rumor          diffusion          to          capture          the          struc-                                                             they                    are                    capable                    to                    learn                    sequential                    features                    from                    rumor
       tures                of                rumor                dispersion.                Moreover,                the                information                from                                          propagation              along              time.              However,              these              approaches              have              a
       source           post           is           involved           in           each           layer           of           GCN           to           enhance           the                                   signiﬁcant           limitation           on           efﬁciency           since           temporal-structural
       inﬂuences             from             the             roots             of             rumors.             Encouraging             empirical                                                               features             only             pay             attention             to             the             sequential             propagation             of
       results            on            several            benchmarks            conﬁrm            the            superiority            of            the                                                         rumors              but              neglect              the              inﬂuences              of              rumor              dispersion.              The
       proposed          method          over          the          state-of-the-art          approaches.
                                                                                                                                                                                                                   structures                  of                  rumor                  dispersion                  also                  indicate                  some                  spread-
                                                                     Introduction                                                                                                                                  ing                behaviors                of                rumors.                Thus,                some                studies                have                tried                to
                                                                                                                                                                                                                   involve                  the                  information                  from                  the                  structures                  of                  rumor                  dis-
With                   the                   rapid                   development                   of                   the                   Internet,                   social                   media           persion             by             invoking             Convolutional             Neural             Network             (CNN)
has           become           a           convenient           online           platform           for           users           to           obtain                                                              based            methods            (Yu            et            al.            2017;            2019).            CNN-based            methods
information,               express               opinions               and               communicate               with               each                                                                        can         obtain         the         correlation         features         within         local         neighbors         but
other.               As               more               and               more               people               are               keen               to               participate               in              cannot         handle         the         global         structural         relationships         in         graphs         or
discussions           about           hot           topics           and           exchange           their           opinions           on                                                                        trees        (Bruna        et        al.        2014).        Therefore,        the        global        structural        fea-
social               media,               many               rumors               appear.               Due               to               a               large               number                              tures               of               rumor               dispersion               are               ignored               in               these               approaches.
of           users           and           easy           access           to           social           media,           rumors           can           spread                                                    Actually,            CNN            is            not            designed            to            learn            high-level            represen-
widely            and            quickly            on            social            media,            bringing            huge            harm            to                                                       tations               from               structured               data               but               Graph               Convolutional               Net-
society            and            causing            a            lot            of            economic            losses.            Therefore,            re-                                                    work           (GCN)           is           (Kipf           and           Welling           2017).
garding             to             the             potential             panic             and             threat             caused             by             rumors,                                                    So                can                we                simply                apply                GCN                to                rumor                detection                since
it            is            urgent            to            come            up            with            a            method            to            identify            rumors            on                    it              has              successfully              made              progress              in              various              ﬁelds,              such              as
social           media           efﬁciently           and           as           early           as           possible.                                                                                            social        networks        (Hamilton,        Ying,        and        Leskovec        2017),        phys-
       Conventional                             detection                             methods                             mainly                             adopt                             hand-               ical             systems             (Battaglia             et             al.             2016),             and             chemical             drug             dis-
crafted                  features                  such                  as                  user                  characteristics,                  text                  contents                                covery         (Defferrard,         Bresson,         and         Vandergheynst         2016)?         The
and           propagation           patterns           to           train           supervised           classiﬁers,           e.g.,                                                                               answer          is          no.          As          shown          in          Figure          1(a),          GCN,          or          called          undi-
Copyright                      c⃝           2020,           Association           for           the           Advancement           of           Artiﬁcial                                                         rected           GCN           (UD-GCN),           only           aggregates           information           relied
Intelligence          (www.aaai.org).          All          rights          reserved.                                                                                                                              on               the               relationships               among               relevant               posts               but               loses               the               se-
                                                                                                                                                                                                   549

quential           orders           of           follows.           Although           UD-GCN           has           the           abil-
ity             to             handle             the             global             structural             features             of             rumor             disper-
sion,             it             does             not             consider             the             direction             of             the             rumor             propa-
gation,               which               however               has               been               shown               to               be               an               important
clue          for          rumor          detection          (Wu,          Yang,          and          Zhu          2015).          Specif-
ically,                     deep                     propagation                     along                     a                     relationship                     chain                     (Han
et                   al.                   2014)                   and                   wide                   dispersion                   across                   a                   social                   commu-
nity          (Thomas          2007)          are          two          major          characteristics          of          rumors,
which           is           eager           for           a           method           to           serve           both.                                                                                                                        (a)          UD-GCN                                               (b)          TD-GCN                                               (c)          BU-GCN
        To             deal             with             both             propagation             and             dispersion             of             rumors,
in               this               paper,               we               propose               a               novel                 Bi-directional                 GCN               (Bi-                                       Figure              1:              (a)              the              undirected              graph              with              only              node              relation-
GCN),                     which                     operates                     on                     both                     top-down                     and                     bottom-up                                   ships;                (b)                the                deep                propagation                along                a                relationship                chain
propagation                  of                  rumors.                  The                  proposed                  method                  obtains                  the                                                     from          top          to          down;          (c)          the          aggregation          of          the          wide          dispersion
features                   of                   propagation                   and                   dispersion                   via                   two                   parts,                   the                         within           a           community           to           an           upper           node.
Top-Down                   graph                   convolutional                   Networks                   (TD-GCN)                   and
Bottom-Up                 graph                 convolutional                 Networks                 (BU-GCN),                 re-
spectively.            As            shown            in            Figure            1(b)            and            1(c),            TD-GCN            for-
wards         information         from         the         parent         node         of         a         node         in         a         rumor                                                                               (Ma                   et                   al.                   2015)                   classiﬁed                   the                   rumor                   by                   using                   the                   time-
tree            to            formulate            rumor            propagation            while            BU-GCN            aggre-                                                                                              series               to               model               the               variation               of               handcrafted               social               context
gates           information           from           the           children           nodes           of           a           node           in           a           ru-                                                        features.                Wu                et                al.                (Wu,                Yang,                and                Zhu                2015)                proposed                a
mor         tree         to         represent         rumor         dispersion.         Then,         the         representa-                                                                                                     graph           kernel-based           hybrid           SVM           classiﬁer           by           combining           the
tions          of           propagation          and           dispersion          pooled           from          the           embed-                                                                                            RBF             kernel             with             a             random-walk-based             graph             kernel.             Ma             et
ding         of         TD-GCN         and         BU-GCN         are         merged         together         through                                                                                                             al.                 (Ma,                 Gao,                 and                 Wong                 2017)                 constructed                 a                 propagation
full                 connections                 to                 make                 the                 ﬁnal                 results.                 Meanwhile,                 we                                          tree         kernel         to         detect         rumors         by         evaluating         the         similarities         be-
concatenate            the            features            of            the            roots            in           rumor            trees            with            the                                                        tween              their              propagation              tree              structures.              These              methods              not
hidden          features          at          each          GCN          layer          to          enhance          the          inﬂuences                                                                                       only            were            ineffective            but            also            heavily            relied            on            handcrafted
from             the             roots             of             rumors.             Moreover,             we             employ             DropEdge                                                                            feature           engineering           to           extract           informative           feature           sets.
(Rong           et           al.           2019)           in           the           training           phase           to           avoid           over-ﬁtting                                                                         In          order          to          automatically          learn          high-level          features,          a          num-
issues          of          our          model.          The          main          contributions          of          this          work          are                                                                            ber            of            recent            methods            were            proposed            to            detect            rumor            based
as           follows:                                                                                                                                                                                                             on             deep             learning             models.             Ma             et             al.             utilized             Recurrent             Neu-
•                      We            leverage            Graph            Convolutional            Networks            to            detect            ru-                                                                        ral                  Networks                  (RNN)                  to                  capture                  the                  hidden                  representation
        mors.            To            the            best            of            our            knowledge,            this            is            the            ﬁrst            study                                       from                 temporal                 content                 features                 (Ma                 et                 al.                 2016).                 Chen                 et
        of           employing           GCN           in           rumor           detection           of           social           media.                                                                                      al.           (Chen           et           al.           2018)           improved           this           approach           by           combining
•                      We                 propose                the                 Bi-GCN                 model                that                 not                 only                 considers                          attention                mechanisms                with                RNN                to                focus                on                text                features
        the               causal               features               of               rumor               propagation               along               relation-                                                                with           different           attentions.            Yu           et           al.            (Yu           et           al.           2017)            proposed
        ship              chains              from              top              to              down              but              also              obtains              the              struc-                                a                method                based                on                Convolutional                Neural                Network                (CNN)
        tural            features            from            rumor            dispersion            within            communities                                                                                                 to           learn          key           features           scattered           among          an           input           sequence           and
        through           the           bottom-up           gathering.                                                                                                                                                            shape         high-level         interactions         among         signiﬁcant         features.         Liu
                                                                                                                                                                                                                                  et            al.            (Liu            and            Wu            2018)            incorporated            both            RNN            and            CNN
•                      We           concatenate           the           features           of           the           source           post           with           other                                                        to              get              the              user              features              based              on              time              series.              Recently,              Ma
        posts         at         each         graph         convolutional         layer         to         make         a         compre-                                                                                         et             al.             (Ma,             Gao,             and             Wong             2019)             employed             the             adversarial
        hensive              use              of              the              information              from              the              root              feature              and                                             learning            method            to            improve            the            performance            of            rumor            clas-
        achieve           excellent           performance           in           rumor           detection.                                                                                                                       siﬁer,            where            the            discriminator            is            used            as            a            classiﬁer            and            the
        Experimental                   results                   on                   three                   real-world                   datasets                   show                                                        corresponding          generator          improves          the          discriminator          by          gen-
that                our                Bi-GCN                method                outperforms                several                state-of-the-                                                                                erating           conﬂicting           noises.           In           addition,           Ma           et           al.           built           a           tree-
art          approaches;          and          for          the          task          of          early          detection          of          rumors,                                                                          structured            Recursive            Neural            Networks            (RvNN)            to            catch            the
which                 is                 quite                 crucial                 to                 identify                 rumors                 in                 real                 time                 and        hidden            representation            from            both            propagation            structures            and
prevent              them              from              spreading,              Bi-GCN              also              achieves              much                                                                                 text                 contents                 (Ma,                 Gao,                 and                 Wong                 2018).                 However,                 these
higher           effectiveness.                                                                                                                                                                                                   methods           are           too           inefﬁcient           to           learn           the           features           of           the           prop-
                                                                                                                                                                                                                                  agation             structure,             and             they             also             ignore             the             global             structural
                                                                       Related              Work                                                                                                                                  features           of           rumor           dispersion.
                                                                                                                                                                                                                                          Compared           to           the           deep-learning           models           mentioned           above,
In             recent             years,             automatic             rumor             detection             on             social             media                                                                        GCN                        is                        able                        to                        capture                        global                        structural                        features                        from
has              attracted              a              lot              of              attention.              Most              previous              work              for              ru-                                    graphs                       or                       trees                       better.                       Inspired                       by                       the                       success                       of                       CNN
mor              detection              mainly              focuses              on              extracting              rumor              features                                                                              in                      the                      ﬁeld                      of                      computer                      vision,                      GCN                      has                      demonstrated
from              the              text              contents,              user              proﬁles              and              propagation              struc-                                                               state-of-the-art                     performances                     in                     various                     tasks                     with                     graph
tures              to              learn              a              classiﬁer              from              labeled              data              (Castillo,              Men-                                                 data                 (Battaglia                 et                 al.                 2016;                 Defferrard,                 Bresson,                 and                 Van-
doza,          and          Poblete          2011;          Yang          et          al.          2012;          Kwon          et          al.          2013;                                                                    dergheynst                        2016;                        Hamilton,                        Ying,                        and                        Leskovec                        2017).
Liu                et                al.                2015;                Zhao,                Resnick,                and                Mei                2015).                Ma                et                al.     Scarselli           et           al.           (Scarselli           et           al.           2008)           ﬁrstly           introduced           GCN
                                                                                                                                                                                                                 550

as                   a                   special                   massage-passing                   model                   for                   either                   undirected                                                    Graph            Convolutional            Networks
graphs              or              directed              graphs.              Later              on,              Bruna              et              al.              (Bruna              et                                             Recently,           there           is           an           increasing           interest           in           generalizing           con-
al.               2014)               theoretically               analyzed               graph               convolutional               meth-                                                                                            volutions                        to                        the                        graph                        domain.                        Among                        all                        the                        existing
ods               for               undirected               graphs               based               on               the               spectral               graph               the-                                                  works,             GCN             is             one             of             the             most             effective             convolution             mod-
ory.                    Subsequently,                    Defferrard                    et                    al.                    (Defferrard,                    Bresson,                                                              els,             whose            convolution             operation             is            considered             as             a             general
and                  Vandergheynst                  2016)                  developed                  a                  method                  named                  the                                                               ”message-passing”           architecture           as           follows:
Chebyshev               Spectral               CNN               (ChebNet)               and               used               the               Cheby-
shev                polynomials                as                the                ﬁlter.                After                this                work,                Kipf                et                al.                                                                                 Hk      =     M  (A,       Hk−         1 ;      Wk−         1 ),                                                                                                                                                                                                                                         (1)
(Kipf            and            Welling            2017)            presented            a            ﬁrst-order            approxima-                                                                                                    where              Hk        ∈               Rn×vk                  is             the             hidden             feature             matrix             computed
tion                    of                    ChebNet                    (1stChebNet),                    where                    the                    information                    of                                               by              the      k      −          th              Graph              Conventional              Layer              (GCL)              and      M                   is
each             node             is             aggregated             from             the             node             itself             and             its             neigh-                                                       the         message         propagation         function,         which         depends         on         the         ad-
boring            nodes.            Our            rumor            detection            model            is            inspired            by            the                                                                             jacency               matrix               A,               the               hidden               feature               matrix               Hk−         1                 and               the
GCN.                                                                                                                                                                                                                                      trainable           parameters         Wk−         1  .
                                                                           Preliminaries                                                                                                                                                           There              are              many              kinds              of              message              propagation              functions
                                                                                                                                                                                                                                          M                      for                 GCN                 (Bruna                 et                 al.                 2014;                 Defferrard,                 Bresson,                 and
We          introduce          some          fundamental          concepts          that          are          necessary                                                                                                                  Vandergheynst               2016).               Among               them,               the               message               propaga-
for                our                method.                First                the                notation                used                in                this                paper                is                as          tion                      function                      deﬁned                      in                      the                      ﬁrst-order                      approximation                      of
follows.                                                                                                                                                                                                                                  ChebNet                (1stChebNet)                (Kipf                and                Welling                2017)                is                as                fol-
                                                                                                                                                                                                                                          lows:
Notation                                                                                                                                                                                                                                                Hk      =     M  (A,       Hk−         1 ;      Wk−         1)=     σ  (  ˆAHk−         1 Wk−         1 ).                                                                       (2)
Let        C            =           {c1,c2 ,   ...,   cm }                  be                  the                  rumor                  detection                  dataset,
where   ci           is        the   i-th        event        and   m        is        the        number        of        events.   ci      =                                                                                             In                      the                      above                      equation           ˆA              =                ˜D−           12     ˜A  ˜D−           12                             is                      the                      normal-
{ri,wi1,wi2 ,   ...,   w ini −         1,Gi },               where      ni                 refers               to               the               number                                                                                 ized              adjacency              matrix,              where        ˜A=A     +            IN                   (i.e.,              adding              self-
of              posts              in      ci  ,      ri                is              the              source              post,              each      wij                  represents              the                                connection),          ˜Dii  =Σ       j    ˜Aij                       that                   represents                   the                   degree                   of                   the
j  -th             relevant             responsive             post,             and     Gi               refers             to             the             propaga-                                                                      i     −          th              node;           Wk−         1        ∈               Rvk−       1 ×vk    ;              and      σ  (·)              is              an              activation
tion           structure.           Speciﬁcally,           Gi             is           deﬁned           as           a           graph           ⟨Vi,Ei ⟩                                                                                 function,           e.g.,           the           ReLU           function.
with     ri               being            the            root            node            (Wu,             Yang,            and            Zhu            2015;            Ma,
Gao,                 and                 Wong                 2017),                 where       Vi           =          {ri,wi1,...,w                        ini −         1 },                                                          DropEdge
and    Ei      =     {eist |s,   t   =0,...,n                       i  −      1}         that         represents         the         set         of                                                                                       DropEdge          is          a          novel          method          to          reduce          over-ﬁtting          for          GCN-
edge          from          responded          posts          to          the          retweeted          posts          or          respon-                                                                                              based             models             (Rong             et             al.             2019).             In             each             training             epoch,             it
sive          posts,          as          shown          in          Figure          1(b).          For          example,          if    wi2            has          a                                                                    randomly               drops               out               edges               from               the               input               graphs               to               gener-
response         to    wi1  ,         there         will         be         an         directed         edge    wi1      →               wi2  ,         i.e.,                                                                             ate             different             deformed             copies             with             certain             rate.             As             a             result,
ei12 .If    wi1            has          a          response          to    ri  ,          there          will          be          an          directed          edge                                                                     this               method               augments               the               randomness               and               the               diversity               of
ri      →               wi1  ,         i.e.,    ei01  .         Denote         Ai    ∈{   0,   1}ni ×ni              as         an         adjacency                                                                                      input              data,              just              like              rotating              or              ﬂapping              images              at              random.
matrix           where                                                                                                                                                                                                                    Formally,            suppose            the            total            number            of            edges            in            the            graph            A
                                                                                   {1,                                             if     ei                                                                                              is     Ne               and             the             dropping             rate             is     p,             then             the             adjacency             matrix
                                                             aits      =                                              st      ∈        Ei                                                                                                 after           DropEdge,         A′  ,           is           computed           as           below:
                                                                                          0,                                             otherwise               .
                                                                                                                                                                                                                                                                                                                      A′      =             A    −                       Adrop                                                                                                                                                                                                                                                                                                                                                   (3)
Denote              Xi    =[xi⊤0           ,       xi⊤1           ,   ...,       xi⊤ni −         1 ]⊤                as             a             feature             matrix             ex-                                              where              Adrop                is              the              matrix              constructed              using      Ne      ×     p              edges
tracted            from            the            posts            in     ci  ,            where            xi0              represents            the            feature                                                                 randomly           sampled           from           the           original           edge           set.
vector          of    ri            and          each          other          row          feature          xij              represents          the          fea-
ture           vector           of    wij    .                                                                                                                                                                                                                           Bi-GCN              Rumor              Detection              Model
        Moreover,           each           event     ci             is           associated           with           a           ground-truth                                                                                             In            this            section,            we            propose            an            effective            GCN-based            method
label       yi     ∈{   F,  T  }                 (i.e.,                 False                 Rumor                 or                 True                 Rumor).                 In                                                    for           rumor           detection           based           on           the           rumor           propagation           and           the
some                    cases,                    the                    label        yi                      is                    one                    of                    the                    four                    ﬁner-grainedrumor                   dispersion,                   named                   as                     Bi-directional                     Graph                     Convo-
classes       {N,  F,  T,  U  }                 (i.e.,                 Non-rumor,                 False                 Rumor,                 True                                                                                       lutional                  Networks                (Bi-GCN).                The                core                idea                of                Bi-GCN                is
Rumor,            and            Unveriﬁed            Rumor)            (Ma,            Gao,            and            Wong            2017;                                                                                              to           learn           suitable           high-level           representations           from           both           rumor
Zubiaga               et               al.               2018).               Given               the               dataset,               the               goal               of               rumor                                    propagation                and                rumor                dispersion.                In                our                Bi-GCN                model,
detection           is           to           learn           a           classiﬁer                                                                                                                                                       two-layer           1stChebNet           are           adopted           as           the           fundamental           GCN
                                                                                    f       :     C      →             Y,                                                                                                                 components.            As            shown            in            Figure            2,            we            elaborate            the            rumor
                                                                                                                                                                                                                                          detection           process           using           Bi-GCN           in           4           steps.
where    C             and    Y                   are         the         sets         of         events         and         labels         respectively,                                                                                          We              ﬁrst              discuss              how              to              apply              the              Bi-GCN              model              to              one
to             predict             the             label             of             an             event             based             on             text             contents,             user                                         event,              i.e.,      ci        →                 yi                for              the      i-th              event.              The              other              events              are
information          and          propagation          structure          constructed          by          the          re-                                                                                                               calculated         in         the         same         manner.         To         better         present         our         method,
lated           posts           from           that           event.                                                                                                                                                                      we           omit           the           subscript        i             in           the           following           content.
                                                                                                                                                                                                                         551

Figure              2:              Our              Bi-GCN              rumor              detection              model.              X              denotes              the              original              feature              matrix              input              to              the              Bi-GCN              model,              and              Hk                 is
the            hidden            features            matrix            generated            from            the     k     −          th            GCL.            Xroot              and            Hroot1                                                               represents            the            matrix            extended            by            the            features            of
source           post.
1            Construct            Propagation            and            Dispersion            Graphs                                                                                                                                                 W   TD1                              ∈                  Rv1 ×v2                    are                the                ﬁlter                parameter                matrices                of                TD-
Based                 on                 the                 retweet                 and                 response                 relationships,                 we                 con-                                                             GCN.            Here            we            adopt            ReLU            function            as            the            activation            func-
struct                the                propagate                structure      ⟨V, E ⟩                for                a                rumor                event       ci  .                                                                   tion,        σ  (·).                   Dropout                   (Srivastava                   et                   al.                   2014)                   is                   applied                   on
Then,                  let                   A           ∈                    Rni ×ni                       and                   X                  be                  its                  corresponding                  adja-                   GCN                Layers                (GCLs)                to                avoid                over-ﬁtting.                Similar                to                Eqs.
cency              matrix              and              feature              matrix              of      ci                based              on              the              spread-                                                               (4)         and         (5),         we         calculate         the         bottom-up         hidden         features         HBU1
ing              tree              of              rumors,              respectively.               A              only              contains              the              edges                                                                    and               HBU2                                                       for              BU-GCN              in              the              same              manner              as              Eq.              (4)              and
from         the         upper         nodes         to         the         lower         nodes         as         illustrated         in         Fig-                                                                                               Eq.           (5).
ure             1(b).             At             each             training             epoch,     p             percentage             of             edges             are
dropped         via         Eq.         (3)         to         form         A′  ,         which         avoid         penitential         over-                                                                                                      3            Root            Feature            Enhancement
ﬁtting           issues           (Rong           et           al.           2019).           Based           on                           A′             and           X,           we           can                                                As               we               know,               the               source               post               of               a               rumor               event               always               has
build         our         Bi-GCN         model.         Our         Bi-GCN         consists         of         two         com-                                                                                                                      abundant          information          to          make          a          wide          impact.          It          is          necessary
ponents:                a                Top-Down                Graph                Convolutional                Network                (TD-                                                                                                       to             better             make             use             of             the             information             from             the             source             post,
GCN)        and        a        Bottom-Up        Graph        Convolutional        Network        (BU-                                                                                                                                               and           learn           more           accurate           node           representations           from           the           rela-
GCN).              The              adjacency              matrices              of              two              components              are              dif-                                                                                      tionship           between           nodes           and           the           source           post.
ferent.           For           TD-GCN,           the           adjacency           matrix           is           represented           as                                                                                                                    Consequently,           besides           the           hidden           features           from           TD-GCN
ATD           =                      A′  .                Meanwhile,                for                BU-GCN,                the                adjacency                ma-                                                                        and              BU-GCN,              we              propose              an              operation              of              root              feature              en-
trix           is            ABU        =             A′⊤  .           TD-GCN           and           BU-GCN           adopt           the           same                                                                                            hancement              to              improve              the              performance              of              rumor              detection
feature           matrix           X.                                                                                                                                                                                                                as                shown                in                Figure                2.                Speciﬁcally,                for                TD-GCN                at                the              k -
2            Calculate            the            High-level            Node            Representations                                                                                                                                               th            GCL,            we            concatenate            the            hidden            feature            vectors            of            every
                                                                                                                                                                                                                                                     nodes             with             the             hidden             feature             vector             of             the             root             node             from
After                    the                    DropEdge                    operation,                    the                    top-down                    propagation                                                                             the    (k     −         1)-th           GCL           to           construct           a           new           feature           matrix           as
features        and        the        bottom-up        propagation        features        are        obtained                                                                                                                                                                                       ˜
by           TD-GCN           and           BU-GCN,           respectively.                                                                                                                                                                                                                        HTDk                       =            concat(HTDk                 ,   (HTDk−         1 )root )                                                                                                                                                                            (6)
         By          substituting          ATD             and          X          to          Eq.          (2)          over          two          layers,          we                                                                              with               HTD
write           the           equations           for           TD-GCN           as           below:(  ˆ                                               )                                                                                                                       0                          =                    X.               Therefore,               we               express               TD-GCN               with               the
                                                                                                             TD   XW   TD                                                                                                                            root           feature           enhancement           by           replacing            HTD1                                                    in           Eq.           (5)           with
                                                          HTD1                       =     σ           A                                  0                   ,                                                                                                                                                                                                                                            (4)HTD˜ HTD
                                                                                           (  ˆ                                                               )                                                                                             1                       =            concat(HTD1                  ,       Xroot ),           and           then           get       ˜(  ˆ              )       2                                                    as           follows:
                                                                                                       TD   HTD                                                                                                                                                                                                                                             TD    ˜
                                                    HTD2                       =     σ          A                         1                                 W   TD1  ,                                                                                                                                                                                                             (5)HTD2                       =     σAHTD1                                 W   TD1,                                                                                                                                                                                                             (7)
where            HTD1                       ∈            Rn×v1                and            HTD2                       ∈            Rn×v2                represent           the           hid-                                                                                                    ˜
den                 features                 of                 two                 layer                 TD-GCN.             W   TD0                               ∈                  Rd×v1                     and                                                                               HTD2                       =            concat(HTD2                  ,   (HTD1                  )root ).                                                                                                                                                                           (8)
                                                                                                                                                                                                                                  552

Similarly,             the             hidden             feature             metrics             of             BU-GCN             with             root                                                                                                                                                                                      Table           1:           Statistics           of           the           datasets
feature                 enhancement,          ˜HBU1                                                          and          ˜HBU2                                         ,                 are                 obtained                 in                 the                                 Statistic                                                                                 Weibo                                                                                                                                                                                   Twitter15                                                                                              Twitter16
same           manner           as           Eq.           (7)           and           Eq.           (8).
                                                                                                                                                                                                                                                                                              #         of         posts                                                                3,805,656                                                                                                        331,612                                                                                                        204,820
4            Representations            of            Propagation            and            Dispersion                                                                                                                                                                                        #         of         Users                                                                2,746,818                                                                                                        276,663                                                                                                        173,487
for            Rumor            Classiﬁcation                                                                                                                                                                                                                                                 #         of         events                                                               4664                                                                                                                                                                                1490                                                                                                                                                     818
The                 representations                 of                 propagation                 and                 dispersion                 are                 the                                                                                                                     #         of         True         rumors                                                  2351                                                                                                                                                                                374                                                                                                                                                                       205
aggregations          from          the          node          representations          of          TD-GCN          and                                                                                                                                                                       #         of         False         rumors                                                 2313                                                                                                                                                                                370                                                                                                                                                                       205
BU-GCN,              respectively.              Here              we              employ              mean-pooling              op-                                                                                                                                                           #         of         Unveriﬁed         rumors                                             0                                                                                                                                                                                                                                      374                                                                                                                                                                       203
erators              to              aggregate              information              from              these              two              sets              of              the
node           representations.           It           is           formulated           as                                                                                                                                                                                                   #         of         Non-rumors                                                           0                                                                                                                                                                                                                                      372                                                                                                                                                                       205
                                                                                                                                                                                                                                                                                              Avg.         time         length         /         event                                  2,460.7         Hours                                           1,337         Hours                                           848         Hours
                                                                            STD      =            MEAN(  ˜HTD2                  ),                                                                                                                                                                                                                                                                                     (9)Avg.         #         of         posts         /         event816                                                                                                                                                                                                  223                                                                                                                                                                       251
                                                                            SBU        =            MEAN(  ˜HBU                                                                                                                                                                               Max         #         of         posts         /         event                            59,318                                                                                                                                                     1,768                                                                                                                                            2,765
                                                                                                                                                         2                  ).                                                                                                                                                                                                                                                               (10)Min         #         of         posts         /         event10                                                                                                                                                                                                                    55                                                                                                                                                                                         81
Then,         we         concatenate         the         representations         of         propagation         and
the           representation           of           dispersion           to           merge           the           information           as
                                                                          S     =            concat(STD ,       SBU  ).                                                                                                                                                                                                                                                     (11)Experimental                  Setup                                             We                  compare                  the                  proposed                  method
Finally,            the            label            of            the            event     ˆy            is            calculated            via            several            full                                                                                                   with           some           state-of-the-art           baselines,           including:
connection           layers           and           a           softmax           layer:                                                                                                                                                                                              •                      DTC          (Castillo,          Mendoza,          and          Poblete          2011):          A          rumor          de-
                                                                         ˆ                                                                                                                                                                                                                       tection             method             using             a             Decision             Tree             classiﬁer             based             on
                                                                         y     =     S of  tmax(FC (S)).                                                                                                                                                                                                                                                 (12)
where        ˆy            ∈                     R1×C                       is                   a                   vector                   of                   probabilities                   for                   all                   the                                               various             handcrafted             features             to             obtain             information             credi-
classes           used           to           predict           the           label           of           the           event.                                                                                                                                                                  bility.
          We         train         all         the         parameters         in         the         Bi-GCN         model         by         mini-                                                                                                                                    •                      SVM-RBF           (Yang           et           al.           2012):           A           SVM-based           model           with
mizing          the          cross-entropy          of          the          predictions          and          ground          truth                                                                                                                                                             RBF         kernel,         using         handcrafted         features         based         on         the         over-
distributions,    Y          ,          over          all          events,    C   .    L2            regularizer          is          applied                                                                                                                                                    all           statistics           of           the           posts.
in           the           loss           function           over           all           the           model           parameters.                                                                                                                                                   •                      SVM-TS               (Ma               et               al.               2015):               A               linear               SVM               classiﬁer               that
                                                                                           Experiments                                                                                                                                                                                           leverages                      handcrafted                      features                      to                      construct                      time-series
In              this              section,              we              ﬁrst              evaluate              the              empirical              performance                                                                                                                              model.
of                our                proposed                Bi-GCN                method                in                comparison                with                sev-                                                                                                         •                      SVM-TK             (Ma,             Gao,             and             Wong             2017):             A             SVM             classiﬁer
eral          baseline          models.          Then,          we          investigate          the          effect          of          each                                                                                                                                                   with           a           propagation           Tree           Kernel           on           the           basis           of           the           propa-
variant                 of                 the                 proposed                 method.                 Finally,                 we                 also                 examine                                                                                                         gation           structures           of           rumors.
the          capability          of          early          rumor          detection          for          both          the          proposed                                                                                                                                        •                      RvNN         (Ma,         Gao,         and         Wong         2018):         A         rumor         detection         ap-
method           and           the           compared           methods.                                                                                                                                                                                                                         proach          based          on         tree-structured          recursive          neural          networks
Settings            and            Datasets                                                                                                                                                                                                                                                      with               GRU               units               that               learn               rumor               representations               via               the
                                                                                                                                                                                                                                                                                                 propagation           structure.
Datasets                                             We            evaluate            our            proposed            method            on            three            real-                                                                                                      •                      PPC       RNN+CNN            (Liu            and            Wu            2018):            A            rumor            detection
world          datasets:           Weibo          (Ma          et          al.          2016),           Twitter15          (Ma,          Gao,                                                                                                                                                   model         combining         RNN         and         CNN,         which         learns         the         rumor
and          Wong          2017),          and           Twitter16           (Ma,          Gao,          and          Wong          2017).                                                                                                                                                       representations            through            the            characteristics            of            users            in            the
Weibo                and                 Twitter                 are                the                most                popular                social                media                sites                                                                                               rumor           propagation           path.
in           China           and           the           U.S.,           respectively.           In           all           the           three           datasets,                                                                                                                   •                      Bi-GCN:               Our               GCN-based               rumor               detection               model               utiliz-
nodes           refer           to           users,           edges           represent           retweet           or           response           re-                                                                                                                                          ing           the           Bi-directional           propagation           structure.
lationships,          and          features          are          the          extracted          top-5000          words          in
terms                 of                 the                 TF-IDF                 values                 as                 mentioned                 in                 the                 Bi-GCN                                                                                            We            implement            DTC            and            SVM-based            models            with            scikit-
Rumor                  Detection                  Model                  Section.                  The                    Weibo                  dataset                  con-                                                                                                        learn1  ;                               PPC               RNN+CNN                               with                               Keras2  ;                               RvNN                               and                               our
tains                two                binary                labels:                False                Rumor                (F)                and                True                Rumor                                                                                        method             with             Pytorch3  .             To             make             a             fair             comparison,             we             ran-
(T),          while           Twitter15          and           Twitter16           datasets          contains          four          la-                                                                                                                                              domly                       split                       the                       datasets                       into                       ﬁve                       parts,                       and                       conduct                       5-
bels:         Non-rumor         (N),         False         Rumor         (F),         True         Rumor         (T),         and                                                                                                                                                     fold            cross-validation            to            obtain            robust            results.            For            the              Weibo
Unveriﬁed             Rumor              (U).             The             label              of             each             event              in                                Weibo             is                                                                                dataset,              we              evaluate              the              Accuracy              (Acc.)              over              the              two              cat-
annotated                according                to                Sina                community                management                cen-                                                                                                                                       egories                     and                     Precision                     (Prec.),                     Recall                     (Rec.),                     F1                     measure
ter,             which             reports             various             misinformation             (Ma             et             al.             2016).                                                                                                                           (F1  )               on               each               class.               For               the               two                 Twiter                datasets,               we               evalu-
And                the                label                of                each                event                in                  Twitter15                and                  Twitter16                 is                                                                  ate            Acc.            over            the            four            categories            and     F1               on            each            class.            The
annotated              according              to              the              veracity              tag              of              the              article              in              ru-
mor               debunking               websites               (e.g.,               snopes.com,               Emergent.info,                                                                                                                                                                     1  https://scikit-learn.org
etc)              (Ma,              Gao,              and              Wong              2017).              The              statistics              of              the              three                                                                                                       2  https://keras.io/
datasets           are           shown           in           Table           1.                                                                                                                                                                                                                   3  https://pytorch.org/
                                                                                                                                                                                                                                                                 553

Table            2:            Rumor            detection            results            on             Weibo            dataset            (F:            False                                                                     Table          3:          Rumor          detection          results          on           Twitter15          and           Twitter16
Rumor;           T:           True           Rumor)                                                                                                                                                                                 datasets              (N:              Non-Rumor;              F:              False              Rumor;              T:              True              Rumor;
                                                                                                                                                                                                                                    U:           Unveriﬁed           Rumor)
                      Method                                           Class                       Acc.                       Prec.                       Rec.                          F1
                                                                             F                                               0.847                      0.815                      0.831                                                                                                                                   Twitter15
                          DTC                                                T                    0.831                      0.815                      0.824                      0.819                                                                  Method                                            Acc.                           NFTU
                 SVM-RBF                                                     F                    0.879                      0.777                      0.656                      0.708                                                                                                                                                 F1                                             F1                                             F1                                           F1
                                                                             T                                               0.579                      0.708                      0.615                                                                      DTC                                         0.454                      0.415                                                      0.355                                                      0.733                                                 0.317
                    SVM-TS                                                   F                    0.885                      0.950                      0.932                      0.938                                                             SVM-RBF                                              0.318                      0.225                                                      0.082                                                      0.455                                                 0.218
                                                                             T                                               0.124                      0.047                      0.059                                                                SVM-TS                                            0.544                      0.796                                                      0.472                                                      0.404                                                 0.483
                        RvNN                                                 F                    0.908                      0.912                      0.897                      0.905                                                               SVM-TK                                             0.750                      0.804                                                      0.698                                                      0.765                                                 0.733
                                                                             T                                               0.904                      0.918                      0.911
       PPC              RNN+CNN                                              F                    0.916                      0.884                      0.957                      0.919                                                                    RvNN                                          0.723                      0.682                                                      0.758                                                      0.821                                                 0.654
                                                                             T                                               0.955                      0.876                      0.913                                                   PPC             RNN+CNN                                        0.477                      0.359                                                      0.507                                                      0.300                                                 0.640
                     Bi-GCN                                                  F                    0.961                      0.961                      0.964                      0.961                                                                 Bi-GCN                                           0.886                      0.891                                                        0.860                                                        0.930                                                   0.864
                                                                             T                                               0.962                      0.962                      0.960                                                                                                                                   Twitter16
                                                                                                                                                                                                                                                          Method                                            Acc.                           NFTU
parameters               of               Bi-GCN               are               updated               using               stochastic               gradi-                                                                                                                                                                               F1                                             F1                                             F1                                           F1
ent           descent,           and           we           optimize           the           model           by           Adam           algorithm                                                                                                            DTC                                         0.473                      0.254                                                      0.080                                                      0.190                                                 0.482
(Kingma             and             Ba             2014).             The             dimension             of             each             node’s             hid-                                                                                  SVM-RBF                                              0.553                      0.670                                                      0.085                                                      0.117                                                 0.361
den              feature              vectors              are              64.              The              dropping              rate              in              DropEdge
is            0.2            and            the            rate            of            dropout            is            0.5.            The            training            process            is                                                      SVM-TS                                            0.574                      0.755                                                      0.420                                                      0.571                                                 0.526
iterated          upon          200          epochs,          and          early          stopping          (Yao,          Rosasco,                                                                                                                    SVM-TK                                             0.732                      0.740                                                      0.709                                                      0.836                                                 0.686
and                 Caponnetto                2007)                 is                 applied                when                 the                 validation                 loss                                                                      RvNN                                          0.737                      0.662                                                      0.743                                                      0.835                                                 0.708
stops            decreasing            by            10            epochs.            Note            that            we            do            not            employ
SVM-TK             on             the               Weibo             dataset             due             to             its             exponential             com-                                                                      PPC             RNN+CNN                                        0.564                      0.591                                                      0.543                                                      0.394                                                 0.674
plexity           on           large           datasets.                                                                                                                                                                                                 Bi-GCN                                           0.880                      0.847                                                              0.869                                                   0.937                                                   0.865
Overall            Performance
Table             2             and             Table             3             show             the             performance             of             the             proposed                                                    models           much           more.
method                   and                   all                   the                   compared                   methods                   on                   the                     Weibo                   and
Twitter            datasets,           respectively.                                                                                                                                                                                Ablation            Study
        First,           among           the           baseline           algorithms,           we           observe           that           the
deep                     learning                     methods                     performs                     signiﬁcantly                     better                     than                                                     To              analyze              the              effect              of              each              variant              of              Bi-GCN,              we              com-
those             using             hand-crafted             features.             It             is             not             surprising,             since                                                                      pare                 the                 proposed                 method                 with                 TD-GCN,                 BU-GCN,                 UD-
the          deep          learning          methods          are          able          to          learn          high-level          repre-                                                                                      GCN                 and                 their                 variants                 without                 the                 root                 feature                 enhance-
sentations             of             rumors             to             capture             valid             features.             This             demon-                                                                         ment.        The        empirical        results        are        summarized        in        Figure        3.        UD-
strates           the           importance           and           necessity           of           studying           deep           learn-                                                                                        GCN,                   TD-GCN,                   and                   BU-GCN                   represent                   our                   GCN-based
ing           for           rumor           detection.                                                                                                                                                                              rumor               detection               models               utilize               the               UnDirected,               Top-Down
        Second,                                                   the                                                   proposed                                                   method                                                   outperforms                                                   theand              Bottom-Up              structures,              respectively.              Meanwhile,              ”root”
PPC               RNN+CNN                                 method                                 in                                 terms                                 of                                 all                                 the                                 perfor-refers          to          the           GCN-based          model          with          concatenating          root           fea-
mance                               measures,                               which                               indicates                               the                               effectiveness                               oftures              in              the              networks              while              ”no              root”              represents              the              GCN-
incorporating                the                dispersion                structure                for                rumor                detection.                                                                               based            model            without            concatenating            root            features            in            the            net-
Since                 RNN                 and                 CNN                 cannot                 process                 data                 with                 the                 graph                                works.         Some         conclusions         are         drawn         from         Figure         3.         First,         Bi-
structure,                             PPC              RNN+CNN                             ignores                             important                             structural                                                    GCN,          TD-GCN,          BU-GCN,          and          UD-GCN          outperforms          their
features         of         rumor         dispersion.         This         prevents         it         from         obtaining                                                                                                       variants            without            the            root            feature            enhancement,            respectively.
efﬁcient                  high-level                  representations                  of                  rumors,                  resulting                  in                                                                   This             indicates             that             the             source             posts             plays             an             important             role
worse           performance           on           rumor           detection.                                                                                                                                                       in          rumor          detection.          Second,          TD-GCN          and          BU-GCN          can          not
        Finally,                   Bi-GCN                   is                   signiﬁcantly                   superior                   to                   the                   RvNN                                          always           achieve           better           results           than           UD-GCN,           but           Bi-GCN           is
method.           Since           RvNN           only           uses           the           hidden           feature           vector           of                                                                                 always            superior            to            UD-GCN,            TD-GCN            and            BU-GCN.            This
all            the            leaf            nodes            so            that            it            is            heavily            impacted            by            the            infor-                                 implies          the          importance          to          simultaneously          consider          both          top-
mation              of              the              latest              posts.              However,              the              latest              posts              are              al-                                     down            representations            from            the            ancestor            nodes,            and            bottom-
ways          lack          of          information          such          as          comments,          and          just          follow                                                                                         up         representations          from         the         children         nodes.          Finally,         even          the
the                former                posts.                Unlike                RvNN,                the                root                feature                enhance-                                                    worst          results          in          Figures          3          are          better          than          those          of          other          base-
ment              allows              the              proposed              method              to              pay              more              attention              to                                                       line              methods              in              Table              2              and              3              by              a              large              gap,              which              again
the         information         of         the         source         posts,         which         helps         improve         our                                                                                                veriﬁes          the          effectiveness          of          graph          convolution          for          rumor          de-
                                                                                                                                                                                                                  554

                                                     (a)           Weibo          dataset                                                                                       (b)           Twitter15          dataset                                                                                         (c)           Twitter16          dataset
                                                              Figure           3:           The           rumor           detection           performance           of           the           GCN-based           methods           on           three           datasets
                                                     (a)           Weibo          dataset                                                                                       (b)           Twitter15          dataset                                                                                         (c)           Twitter16          dataset
                                                                                                               Figure           4:           Result           of           rumor           early           detection           on           three           datasets
tection.                                                                                                                                                                                                                  model              gives              the              proposed              method              the              ability              of              processing
                                                                                                                                                                                                                          graph/tree                  structures                  and                  learning                  higher-level                  representa-
Early            Rumor            Detection                                                                                                                                                                               tions                  more                  conducive                  to                  rumor                  detection.                  In                  addition,                  we
Early                 detection                 aims                 to                 detect                 rumor                 at                the                 early                 stage                 of also                    improve                    the                    effectiveness                    of                    the                    model                    by                    concate-
propagation,              which              is              another              important              metric              to              evaluate                                                                     nating                   the                   features                   of                   the                   source                   post                   after                   each                   GCL                   of
the                quality                of                the                method.                To                construct                an                early                detection                         GCN.           Meanwhile,           we           construct           several           variants           of           Bi-GCN
task,             we             set             up             a             series             of             detection             deadlines             and             only             use                          to           model            the           propagation           patterns,            i.e.,           UD-GCN,            TD-GCN
the           posts           released           before           the           deadlines           to           evaluate           the           accu-                                                                   and            BU-GCN.            The            experimental            results            on            three            real-world
racy                of                the                proposed                method                and                baseline                methods.                Since                                           datasets                 demonstrate                 that                 the                 GCN-based                 approaches                 out-
it           is           difﬁcult           for           the           PPC       RNN+CNN           method           to           process           the                                                                  perform                state-of-the-art                baselines                in                very                large                margins                in
data         of         variational         lengths,         we         cannot         get         the         accurate         results                                                                                   terms             of             both             accuracy             and             efﬁciency.             In             particular,             the             Bi-
of           PPC          RNN+CNN           at           each           deadline           in           this           task,           so           it           is           not                                         GCN               model               achieves               the               best               performance               by               considering
compared           in           this           experiment.                                                                                                                                                                both                 the                 causal                 features                 of                 rumor                 propagation                 along                 rela-
        Figure            4            shows           the            performances            of            our            Bi-GCN            method                                                                       tionship                chains                from                top                to                down                propagation                pattern                and
versus                    RvNN,                    SVM-TS,                    SVM-RBF                    and                    DTC                    at                    various                                      the                 structural                 features                 from                 rumor                 dispersion                 within                 com-
deadlines              for              the               Weibo              and               Twitter               datasets.              From              the              ﬁg-                                        munities           through           the           bottom-up           gathering.
ure,        it        can        be        seen        that        the        proposed        Bi-GCN        method        reaches
relatively                     high                     accuracy                     at                     a                     very                     early                     period                     after                     the                                        Acknowledgments
source                 post                 initial                 broadcast.                 Besides,                 the                 performance                 of
Bi-GCN                     is                     remarkably                     superior                     to                     other                     models                     at                     each     The              authors              would              like              to              thank              the              support              of              Tencent              AI
deadline,          which          demonstrates          that          structural          features          are          not                                                                                              Lab               and               Tencent               Rhino-Bird               Elite               Training               Program.               This
only            beneﬁcial            to            long-term            rumor            detection,            but            also            help-                                                                       work            is            supported            by            the            National            Natural            Science            Founda-
ful           to           the           early           detection           of           rumors.                                                                                                                         tion               of               Guangdong               Province               (2018A030313422),               National
                                                                        Conclusions                                                                                                                                       Natural           Science           Foundation           of           China           (Grant           No.           61773229,
                                                                                                                                                                                                                          No.         61972219)         and         Overseas         Cooperation         Research         Fund         of
In          this          paper,          we          propose          a          GCN-based          model          for          rumor          de-                                                                       Graduate               School               at               Shenzhen,               Tsinghua               University               (Grant
tection               on               social               media,               called               Bi-GCN.               Its               inherent               GCN                                                  No.           HW2018002).
                                                                                                                                                                                                         555

                                                                                References                                                                                                                                              Ma,          J.;          Gao,          W.;          and          Wong,          K.-F.               2018.               Rumor          detection          on          twit-
Battaglia,          P.;          Pascanu,          R.;          Lai,          M.;          Rezende,          D.          J.;          et          al.               2016.               In-                                             ter           with           tree-structured           recursive           neural           networks.                  In            Proceedings
teraction       networks       for       learning       about       objects,       relations       and       physics.                                                                                                                   of              the              56th              Annual              Meeting              of              the              Association              for              Computational
In         Advances         in         neural         information         processing         systems,        4502–4510.                                                                                                                 Linguistics           (Volume           1:           Long           Papers),          1980–1989.
Bruna,              J.;              Zaremba,              W.;              Szlam,              A.;              and              Lecun,              Y.                            2014.                            Spec-              Ma,           J.;           Gao,           W.;           and           Wong,           K.-F.                  2019.                  Detect           rumors           on           twitter
tral               networks               and               locally               connected               networks               on               graphs.                               In                 In-                          by              promoting              information              campaigns              with              generative              adversarial
ternational            Conference            on            Learning            Representations            (ICLR2014),                                                                                                                   learning.              In           The           World           Wide           Web           Conference,          3049–3055.              ACM.
CBLS,           April           2014,          http–openreview.                                                                                                                                                                         Rong,              Y.;              Huang,              W.;              Xu,              T.;              and              Junzhou,              H.                           2019.                           The              truly
Castillo,                C.;                Mendoza,                M.;                and                Poblete,                B.                                 2011.                                 Information                  deep             graph             convolutional             networks             for             node             classiﬁcation.                                          arXiv
credibility         on         twitter.            In          Proceedings          of          the          20th          international          con-                                                                                  preprint           arXiv:1907.10903.
ference           on           World           wide           web,          675–684.              ACM.                                                                                                                                  Scarselli,        F.;        Gori,        M.;        Tsoi,        A.        C.;        Hagenbuchner,        M.;        and        Monfar-
                                                                                                                                                                                                                                        dini,       G.         2008.         The       graph       neural       network       model.           IEEE        Transactions
Chen,             T.;             Li,             X.;             Yin,             H.;             and             Zhang,             J.                          2018.                          Call             attention             toon           Neural           Networks          20(1):61–80.
rumors:             Deep             attention             based             recurrent             neural             networks             for             early                                                                        Srivastava,                     N.;                     Hinton,                     G.;                     Krizhevsky,                     A.;                     Sutskever,                     I.;                     and
rumor              detection.                            In                Paciﬁc-Asia                Conference                on                Knowledge                Dis-                                                         Salakhutdinov,             R.                         2014.                         Dropout:             a             simple             way             to             prevent             neu-
covery           and           Data           Mining,          40–52.              Springer.                                                                                                                                            ral               networks               from               overﬁtting.                                                    The                 journal                 of                 machine                 learning
Defferrard,              M.;              Bresson,              X.;              and              Vandergheynst,              P.                           2016.                           Con-                                         research          15(1):1929–1958.
volutional              neural              networks              on              graphs              with              fast              localized              spectral                                                               Thomas,            S.            A.                    2007.                    Lies,            damn            lies,            and            rumors:            an            analysis            of
ﬁltering.                            In                               Advances                in                neural                information                processing                systems,                                     collective          efﬁcacy,          rumors,          and          fear          in          the          wake          of          katrina.                                  Socio-
3844–3852.                                                                                                                                                                                                                              logical           Spectrum          27(6):679–703.
Giudice,             K.             D.                          2010.                          Crowdsourcing             credibility:             The             impact             of                                                 Wu,          K.;          Yang,          S.;          and          Zhu,          K.          Q.              2015.              False          rumors          detection          on
audience           feedback           on           web           page           credibility.                   In             Proceedings             of             the                                                                sina           weibo           by           propagation           structures.                   In            2015            IEEE            31st            interna-
73rd             ASIS&T             Annual             Meeting             on             Navigating             Streams             in             an             Infor-                                                               tional           conference           on           data           engineering,          651–662.              IEEE.
mation           Ecosystem-Volume           47,                    59.               American          Society          for          Informa-                                                                                           Yang,             F.;             Liu,             Y.;             Yu,             X.;             and             Yang,             M.                        2012.                        Automatic             detec-
tion          Science.                                                                                                                                                                                                                  tion          of          rumor          on          sina          weibo.               In           Proceedings           of           the           ACM           SIGKDD
Hamilton,        W.;        Ying,        Z.;        and        Leskovec,        J.           2017.           Inductive        represen-                                                                                                 Workshop           on           Mining           Data           Semantics,                    13.              ACM.
tation         learning         on         large         graphs.            In          Advances          in          Neural          Information                                                                                       Yao,              Y.;              Rosasco,              L.;              and              Caponnetto,              A.                           2007.                           On              early              stop-
Processing           Systems,          1024–1034.                                                                                                                                                                                       ping                 in                 gradient                 descent                 learning.                                            Constructive                    Approximation
Han,              S.;              Zhuang,              F.;              He,              Q.;              Shi,              Z.;              and              Ao,              X.                           2014.                           Energy26(2):289–315.
model           for           rumor           propagation           on           social           networks.                     Physica            A:             Sta-                                                                  Yu,              F.;              Liu,              Q.;              Wu,              S.;              Wang,              L.;              and              Tan,              T.                            2017.                            A              convo-
tistical           Mechanics           and           its           Applications          394:99–109.                                                                                                                                    lutional              approach              for              misinformation              identiﬁcation.                             In                               Proceed-
Kingma,            D.            P.,            and            Ba,            J.                      2014.                       Adam:            A            method            for            stochastic                             ings             of             the             26th             International             Joint             Conference             on             Artiﬁcial             Intel-
optimization.                arXiv           preprint           arXiv:1412.6980.                                                                                                                                                        ligence,          3901–3907.              AAAI          Press.
Kipf,         N.,         T.,         and         Welling,         M.             2017.             Semi-supervised         classiﬁcation                                                                                               Yu,             F.;             Liu,             Q.;             Wu,             S.;             Wang,             L.;             and             Tan,             T.                         2019.                         Attention-
with          graph          convolutional          networks.              In           Proceedings           of           the           Interna-                                                                                       based                     convolutional                     approach                     for                     misinformation                     identiﬁcation
tional           Conference           on           Learning           Representations.                                                                                                                                                  from             massive             and             noisy             microblog             posts.                             Computers               &               Security
Kwon,        S.;        Cha,        M.;        Jung,        K.;        Chen,        W.;        and        Wang,        Y.          2013.          Promi-                                                                                83:106–121.
nent         features         of         rumor         propagation         in         online         social         media.             In          2013                                                                                 Zhao,           Z.;           Resnick,           P.;           and           Mei,           Q.                   2015.                   Enquiring           minds:           Early
IEEE            13th            International            Conference            on            Data            Mining,           1103–1108.                                                                                               detection              of              rumors              in              social              media              from              enquiry              posts.                           In               Pro-
IEEE.                                                                                                                                                                                                                                   ceedings          of          the          24th          International          Conference          on          World          Wide          Web,
Liu,         Y.,         and         Wu,         Y.-F.            2018.            Early         detection         of         fake         news         on         social                                                               1395–1405.                    International            World            Wide            Web            Conferences            Steering
media              through              propagation              path              classiﬁcation              with              recurrent              and                                                                              Committee.
convolutional               networks.                                In                 32nd                 AAAI                 Conference                 on                 Artiﬁcial                                               Zubiaga,               A.;               Aker,               A.;               Bontcheva,               K.;               Liakata,               M.;               and               Procter,
Intelligence,           AAAI           2018,          354–361.              AAAI          press.                                                                                                                                        R.                    2018.                    Detection            and            resolution           of            rumours            in            social           media:            A
Liu,               X.;               Nourbakhsh,               A.;               Li,               Q.;               Fang,               R.;               and               Shah,               S.                                 2015.survey.                ACM           Computing           Surveys           (CSUR)          51(2):32.
Real-time           rumor           debunking           on           twitter.                 In            Proceedings            of            the            24th
ACM            International            on            Conference            on            Information            and            Knowledge
Management,          1867–1870.              ACM.
Ma,         J.;         Gao,         W.;         Wei,         Z.;         Lu,         Y.;         and         Wong,         K.-F.             2015.             Detect         ru-
mors         using         time         series         of         social         context         information         on         microblog-
ging             websites.                        In              Proceedings              of              the              24th              ACM              International              on
Conference                 on                 Information                 and                 Knowledge                 Management,               1751–
1754.              ACM.
Ma,       J.;       Gao,       W.;       Mitra,       P.;       Kwon,       S.;       Jansen,       B.       J.;       Wong,       K.-F.;       and
Cha,            M.                    2016.                    Detecting            rumors            from            microblogs            with            recurrent
neural          networks.              In           Ijcai,          3818–3824.
Ma,               J.;               Gao,               W.;               and               Wong,               K.-F.                              2017.                              Detect               rumors               in               mi-
croblog             posts             using             propagation             structure             via             kernel             learning.                         In
Proceedings                   of                   the                   55th                   Annual                   Meeting                   of                   the                   Association                   for
Computational           Linguistics           (Volume           1:           Long           Papers),          708–717.
                                                                                                                                                                                                                       556

