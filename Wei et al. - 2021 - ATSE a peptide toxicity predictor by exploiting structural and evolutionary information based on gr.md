_Briefings in Bioinformatics,_ 22(5), 2021, 1–13


**[https://doi.org/10.1093/bib/bbab041](https://doi.org/10.1093/bib/bbab041)**
Problem Solving Protocol

# **ATSE: a peptide toxicity predictor by exploiting** **structural and evolutionary information based on** **graph neural network and attention mechanism**

## Lesong Wei, Xiucai Ye, Yuyang Xue, Tetsuya Sakurai and Leyi Wei


Corresponding authors: Xiucai Ye, E-mail: yexiucai@cs.tsukuba.ac.jp; Leyi Wei, E-mail: weileyi@sdu.edu.cn


Abstract


Motivation: Peptides have recently emerged as promising therapeutic agents against various diseases. For both research and
safety regulation purposes, it is of high importance to develop computational methods to accurately predict the potential
toxicity of peptides within the vast number of candidate peptides. Results: In this study, we proposed ATSE, a peptide
toxicity predictor by exploiting structural and evolutionary information based on graph neural networks and attention
mechanism. More specifically, it consists of four modules: (i) a sequence processing module for converting peptide
sequences to molecular graphs and evolutionary profiles, (ii) a feature extraction module designed to learn discriminative
features from graph structural information and evolutionary information, (iii) an attention module employed to optimize
the features and (iv) an output module determining a peptide as toxic or non-toxic, using optimized features from the
attention module. Conclusion: Comparative studies demonstrate that the proposed ATSE significantly outperforms all other
competing methods. We found that structural information is complementary to the evolutionary information, effectively
improving the predictive performance. Importantly, the data-driven features learned by ATSE can be interpreted and
visualized, providing additional information for further analysis. Moreover, we present a user-friendly online computational
[platform that implements the proposed ATSE, which is now available at http://server.malab.cn/ATSE. We expect that it can](http://server.malab.cn/ATSE)
be a powerful and useful tool for researchers of interest.


**Lesong Wei** received the B.S. degree in computer science and Technology from the Taiyuan University of Technology, China, in 2017, the M.S. degree in
computer technology from Fuzhou University, China, in 2021. He currently is a Ph.D. student at the University of Tsukuba, Japan. His research interests are
bioinformatics and machine learning.
**Xiucai Ye** received a Ph.D. degree in computer science from the University of Tsukuba, Tsukuba, Japan, in 2014. She is currently an assistant professor with
the Department of Computer Science, and Center for Artificial Intelligence Research (C-AIR), University of Tsukuba. Her current research interests include
feature selection, clustering, machine learning and bioinformatics.
**Yuyang Xue** is a Ph.D. student at the University of Tsukuba. He received his Master’s degree in computer science in 2018 at the University of Southampton
and his Bachelor’s degree in Computer Science in 2016, at Fuzhou University. His research interests are bioinformatics, computer vision and machine
learning.
**Tetsuya Sakurai** received a Ph.D. degree in computer engineering from Nagoya University, in 1992. He is currently a professor of the Department of
Computer Science, and the Director of the Center for Artificial Intelligence Research (C-AIR), University of Tsukuba. He is also a visiting professor with
the Open University of Japan and also a visiting researcher of the Advanced Institute of Computational Science, RIKEN. His research interests include
high-performance algorithms for large-scale simulations, data and image analysis and deep neural network computations.
**Leyi Wei** received his Ph.D. in computer science from Xiamen University, China. He is currently a professor in the School of software, at Shandong University,
China. His research interests include machine learning and its applications to bioinformatics.
**Submitted:** 17 December 2020; **Received (in revised form):** 11 January 2021


© The Author(s) 2021. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com


1


2 _Wei et al._


Introduction


A peptide is defined as a short chain with 50 or fewer amino
acids linked by peptide bonds, to distinguish it from proteins
such as antibodies or other biologics [1, 2]. Nowadays, more
than 7000 naturally occurring peptides have been identified, and
they play an essential role in various physiological processes,
such as hormones, antimicrobials, growth factors, ion channel
ligands, neurotransmitters and immunomodulators [3–6]. They
reach their _in vivo_ targets with exquisite specificity, resulting in
extremely high potency of action and relatively few off-target
side effects [7]. Compared with traditional molecules (such as
proteins and antibodies, etc.), peptides have several advantages,
including high penetration, low production cost and complexity,
and high specificity and biological activity [8–10]. Given the
attractive inherent properties, peptides have been regarded as
promising molecules to design novel drugs.
The last decade has witnessed unprecedented research interests from the pharmaceutical industry and the scientific community in peptide-based therapeutics [11]. In the development
of peptide-based therapeutics, the bioactivity of the therapeutics is closely involved with three major factors, including _in_
_vivo_ stability, immunogenicity and toxicity of peptides [12]. To
enhance the stability of peptides [13], numerous methods have
been proposed from the following aspects, such as by changing
the backbone structures, cyclizing peptides and incorporating
_α_ -aminoxy acids [14]. Similarly, a number of _in silico_ tools have
been developed to predict whether a peptide is immunogenic
or not [15–17]. However, little attention is paid on peptide toxicity analysis, which plays a crucial role in the discovery and
development of safe peptide-based drugs.
Traditional experimental methods in wet lab to determine
the toxicity of peptides are time consuming and costly,
particularly with the explosive growth of peptide candidates.
In the last few decades, data-driven computational methods,
like machine learning approaches, are attracting more and
more concerns. The key to build a successful machine learning
model for the prediction of peptide toxicity is peptide feature
representation, that is, designing and extracting discriminative
features to represent the inherent characteristics between toxic
peptides and non-toxic ones. In 2009, Naamati _et al._ developed
a machine learning-based predictor called ClanTox for animal
toxins. To discriminate toxic peptides from non-toxic ones, they
used the features extracted from primary protein sequences
and trained the predictive model based on boosted stump
classifiers [18]. Later on, Gupta _et al._ proposed an _in silico_
method, namely ToxinPred, also based on machine learning
to predict the toxicity of peptides and proteins, in which they
used various sequence-based feature descriptors, including
amino acid composition (AAC), dipeptide composition (DPC)
and sequential motif, etc. [19]. Much progress has been made
and other feature descriptors have been proposed, such as
twenty-bit features, and adaptive skip dipeptide composition
(ASDC) [20–23], etc. However, due to the short length of peptides,
they can’t contain much context information, which distinctly
increases the difficulty of capturing the discriminative features.
To address this problem, some strategies directly integrate
different types of sequential features, which can improve
the predictive performance to some extent. However, they
often lead to data redundancy and the dimensional disaster
in feature space, increasing the model complexity. Moreover,
most existing feature descriptors are designed to capture
sequential characteristics of peptides, ignoring the structural
and evolutionary information, which might facilitate to design a



more discriminative classifier [24, 25]. The study demonstrates
that some sequences have low sequential similarity, but they
may have highly similar structures, thus sharing similar or
even the same biological functions [26]. Therefore, exploring and
integrating evolutionary and structural information sometimes
might be more effective than sequence-based information,
providing potential performance improvement.
Recently, deep learning techniques have been widely applied
in multiple fields of bioinformatics, such as protein fold recognition [27, 28], compound–protein interaction prediction [29–33]
and protein structure prediction [34–36], and achieved excellent
performance [37–41]. Compared with traditional machine learning methods using manually extracted features, deep learning
techniques can automatically learn high-latent feature representations with limited knowledge of proteins and peptides [42].
In this study, we propose a novel deep learning-based method,
called ATSE (a peptide toxicity predictor by exploiting Structural
and Evolutionary information based on graph neural network
and ATtention mechanism), which is effective in distinguishing between toxic and non-toxic peptides using both structural
information and evolutionary information. To capture structural
information, we use Graph Neural Networks (GNNs) to extract
layer-wise features from molecular graphs of peptides; on the
other hand, to encode the information embedded in evolutionary profiles,we adopt a hybrid network architecture of a convolutional neural network (CNN) and a bidirectional long short-term
memory (BiLSTM) to automatically learn high-latent evolutionary features. Via attention mechanism, we successfully integrate
the structural information and evolutionary information, and
train a deep learning framework to predict if the peptide is
toxic or not. Benchmarking results show that our proposed ATSE
can achieve significantly better performance than state-of-theart methods. Importantly, the data-driven features learned by
ATSE can be interpreted and visualized, providing additional
information for further analysis. In addition, a user-friendly and
[publicly accessible web server has been established at http://se](http://server.malab.cn/ATSE)
[rver.malab.cn/ATSE.](http://server.malab.cn/ATSE)


Methods and materials


**Datasets**


Previous research has demonstrated that a well-established

dataset is essential to train a predictive model with strong
generalization ability [20, 43–45]. In this study, we constructed
a new dataset for peptide toxicity prediction. For a binary
classification task, a dataset usually consists of positive samples
and negative samples. For positive samples, we collected
experimentally validated toxic peptides with a range of 10
to 50 residues from three public databases: ConoServer [46],
ArachnoServer [47] and SwissProt [48], respectively. In the
SwissProt database, we used the keyword ‘KW-0800’ as described
in the study [19] to search for toxic peptides. It is worth pointing
out that we limited the length distribution of toxic peptides
for collection, since most bioactive peptides are distributed
in this range. After removing some duplicate peptides that
appeared in the three databases, we obtained a total of 3992
toxic peptides, which were considered as the initial positive
dataset. For negative samples collection, we used keywords ‘NOT
KW-0800 and NOT KW-0020’ to search non-toxic peptides that
are manually annotated and reviewed in SwissProt database.
To keep the same length distribution with the positive dataset,
we selected the non-toxic peptides with length from 10 to 50,


_ATSE: a peptide toxicity predictor by exploiting structural and evolutionary_ 3



yielding a total of 7009 non-toxic peptides as the initial negative
dataset.

To avoid the evaluation bias caused by sequence homology,
we eliminated the sequences with more than 90% similarity in
the initial positive and negative dataset by using the CD-HIT program [49] with the threshold set at 0.9. As a result, 1932 positive
samples were retained in the positive dataset. To construct a
balanced dataset, we randomly picked up the same number of
negative samples from the remaining negative samples. To this
end, we constructed a dataset containing 3864 samples, where
the number of positive and negative samples is the same. For
model training, we randomly selected 85% of the data as the
training dataset to train our model, and the rest of the data
was taken as the testing dataset to evaluate the performance
of our model. To avoid the randomness of data splitting, the
evaluation results in this study are averaged by running 20 times
of random data splitting. Note that there are none of the positive
or negative examples in the testing dataset appearing in the
training dataset, which ensures the validity of the experimental
results.


**Network architecture of the proposed ATSE**


The architecture of our model ATSE is shown in Figure 1, which
consists of four modules, including (1) sequence processing
module, (2) feature extraction module, (3) attention module and
(4) output module. The first module is employed to convert
primary peptide sequences to molecular graphs and evolutionary profiles, which contain structural information and evolutionary information, respectively. The second module consists
of GNNs that are designed to learn layer-wise graph features
from the molecular graphs and a CNN_BiLSTM network that
is used to generate a feature vector from the position-specific
scoring matrix (PSSM). In particular, each layer-wise feature
vector from the GNNs is integrated with the feature vector
from the CNN_BiLSTM network, respectively. Afterwards, the
combined feature vectors are sent into the third module to fuse

and optimize them, and then yield a more discriminative feature
representation by attention mechanism. The final output module, consisting of a fully connected layer and a _softmax_ output
layer, generates a toxicity probability of a given peptide. For more
details, we describe each module below.


_**Sequence processing module**_


Unlike existing feature descriptors to explore sequential information, here we represent peptide sequences from the following
two perspectives: structural information and evolutionary information. More specifically, structural information is obtained by
using RDKit [50], a tool that converts raw peptide sequences
(with FASTA format) into their corresponding chemical structures (with MOL format), which contain various molecular data
information, like atoms, bonds and coordinates, etc. The evolutionary information is represented by the PSSM that is obtained
by PSI-BLAST [51].
_Position-specific scoring matrix (PSSM)_ . The PSSM is one kind
of evolutionary profiles [52], which contains evolutionary
information extracted through the multiple sequence alignment
(MSA) generated by PSI-BLAST [51]. PSSM records the probability
of each amino acid to convert into another at different positions
in a protein sequence. Previous studies demonstrate that using
evolutionary information embedded in PSSM to represent an
amino acid sequence is efficient for different classification
tasks [27, 28, 53]. Here, we also employed PSSM for peptide



sequence representation. For given a peptide sequence, its
corresponding MSA can be obtained by running PSI-BLAST
to search against non-redundant protein sequence database
(i.e. nr.00 and nr.01) with the parameters ‘iterations = 3 and
e-value = 0.01’. Afterwards, based on the MSA, the PSSM of
a peptide sequence _P_ can be generated and represented as a
matrix:



where _l_ denotes the length of _P_ ; 20 denotes the number of
standard amino acids.

_Molecular graph representation_ . To encode structural information for peptide feature representation, we employed the molecular graph to represent a given peptide by using RDKit. The
molecular graph represents the chemical structure of the peptide sequence. For the convenience of discussion, a graph is
denoted as _G_ = ( _V_, _E_ ), in which _V_ is the set of nodes and _E_ is the
set of edges of _G_ . Moreover, _N_ ( _v_ ) denotes a set of all neighboring
nodes of the node _v_ ∈ _V_, i.e. _N_ ( _v_ ) = { _u_ ∈ _V_ |( _u_, _v_ ) ∈ _E_ }. In
a molecular graph, _u_ ∈ _V_ represents an atom, and ( _u_, _v_ ) ∈ _E_
represents a chemical bond between atoms _u_ and _v_ . Note that
the edges ( _u_, _v_ ) and ( _v_, _u_ ) are identical.


_**Feature extraction module**_


_Graph neural networks (GNNs) for molecular graphs._ Here, we used
the GNNs to extract the structural information in the molecular

graph of a given peptide. Since the graph-structured data can’t
be taken as the input of GNNs directly, we add an embedding
layer on the top of GNNs to embed graph-structured data into
numerical vectors. Here, following the study [31], we consider
the information of atoms and edges, and introduce the idea of
1-dimensional Weisfeiler-Lehman algorithm (1-WL) [54] to
embed the molecular graph of a given peptide. The 1-WL
iteratively assigns a label for each node based on the previous
labels of itself and its neighbors. However, in this study, when
assigning a label for a node, we not only consider the previous
labels of itself and its neighbors but also the previous labels of
the edges between itself and its neighbors. We also update the
label of each edge considering two nodes on its both sides.
We now describe the embedding method for a molecular
graph in this work. Let _h_ [(] _[t]_ [)] ( _v_ ) ∈ N and _g_ [(] _[t]_ [)] ( _u_, _v_ ) ∈ N represent
the labels of node _v_ and edge ( _u_, _v_ ) in iteration _t_, respectively,
in which N represents the natural number. In iteration 0, we
initialize the nodes or the edges in terms of the types of nodes or
edges in the molecular graph of a peptide, such as _h_ [(0)] ( _oxygen_ ) =
0, _h_ [(0)] ( _nitrogen_ ) = 1, and so on. In iteration _t >_ 0, they can be
represented as follows:


_h_ [(] _[t]_ [)] ( _v_ ) = _HASH_ �� _h_ _[(][t]_ [−][1] _[)]_ ( _v_ ), �� _h_ _[(][t]_ [−][1] _[)]_ ( _u_ ), _g_ _[(][t]_ [−][1] _[)]_ _(u_, _v)_ � | _u_ ∈ _N_ ( _v_ )��� (2)


_g_ [(] _[t]_ [)] _(u_, _v)_ = _HASH_ �� _g_ _[(][t]_ [−][1] _[)]_ _(u_, _v)_, �� _h_ [(] _[t]_ [)] ( _u_ ), _h_ [(] _[t]_ [)] ( _v_ )� | _u_ ∈ _N_ ( _v_ )��� (3)


where [· · · ] denotes a multiset, a collection of tuples, in which
order is not important and tuples may appear multiple times;
_HASH_ is used to map the pair in Equation (2) or (3) to a unique
value in N as corresponding node’s or edge’s label, which does
not appear in previous iterations.
For a molecular graph of a given peptide sequence, through
the method described above, we can get its fingerprint. Afterwards, by using word embedding, the fingerprint is encoded into



_p_ 1,1 _p_ 1,2 - · · _p_ 1,20

_p_ 2,1 _p_ 2,2 - · · _p_ 2,20

... ... ... ...

_p_ _l_,1 _p_ _l_,2 - · · _p_ _l_,20



⎤


(1)
⎥⎥⎥⎥⎦



_PSSM_ =



⎡

⎢⎢⎢⎢⎣


4 _Wei et al._


**Figure 1.** The flowchart of the proposed ATSE. ATSE method comprises of four modules. First, we have a sequence processing module that generates molecular graphs
and PSSMs from primary peptide sequences. Second, we have a feature extraction module that extracts the features from molecular graphs and PSSMs, respectively.
Next, we have an attention module that takes the outputs of the last module as input. Finally, the resulting feature representation is fed into an output module to
generate a toxicity probability.



a matrix _X_ with the size _n_ × _m_, where _n_ is the number of nodes in a
molecular graph and _m_ is the hyperparameter of the embedding
layer. After embedding, _X_ is submitted into GNNs to extract the
structural information, that is,


_R_ = _relu_ � _X_ [(] _[k]_ [)] _W_ _gnn_ � (4)

_X_ _[(]_ _[k]_ [+][1] _[)]_ = _X_ [(] _[k]_ [)] + _AR_ (5)


where _k_ represents the _k_ -th GNN layer; A is the adjacency matrix
of the molecular graph; _relu_ (Rectified Linear Unit) is a non-linear
activation function; _W_ _gnn_ is the weight matrix with the size _m_ × _m_ .
In order to obtain the output y _graph_ ( _k_ ) [of each GNN layer, we used]

the mean value of each column of _X_ [(] _[k]_ [)] = ( _x_ 1( _k_ ) [,] _[ x]_ 2( _k_ ) [,] _[ . . .]_ [,] _[ x]_ ( _nk_ ) [) as]
follows:



_relu_ ) to extract the hidden features of the local peptide sequence.
Next, the BiLSTM layer is used to capture the long-range dependency relationship amongst extracted hidden features in different sequence positions and sequence order information [27].
Note that during learning the toxin-specific features process,
zero-padding is performed to deal with the variable length of
the input sequence and reach a fixed length of 50 due to neural
networks requiring fixed-length features as inputs. In addition,
the dropout technique is employed to prevent the predictive
model from overfitting.


_**Attention module**_


Attention Mechanism is an attempt to mimic human brain
action to selectively concentrate on a few important parts, while
ignoring others in machine learning [58, 59]. In our network
architecture, the attention module cooperates with the feature
extraction module to capture interpretable features. It assigns
different weights to the concatenated feature vectors obtained
by separately combining the output of the CNN-BLSTM network
with the output of each GNN layer in the feature extraction
module. The higher the weight, the more important the corresponding feature vector.
For the convenience of description, we denote all the concatenated feature vectors as F with the size _h_ × _u_ :


_F_ = � _f_ 1, _f_ 2, _. . ._, _f_ _h_ � (7)



_y_ _graph_ ( _k_ ) [=] _n_ [1]



_n_
� _x_ _i_ ( _k_ ) (6)


_i_



where _x_ ( _ik_ ) denotes the vector presentation of _i_ - th node in a
molecular graph in the _k_ -th GNN layer; _n_ is the number of nodes
in a molecular graph; the y _graph_ ( _k_ ) [denotes the vector presentation]
of a peptide sequence in the _k_ -th GNN layer.
_CNN_BiLSTM network for PSSMs_ . The CNN-BiLSTM network is
mainly designed to extract the toxin-specific features considering the evolutionary information derived from PSSMs. This
network contains two widely using neural networks: a CNN [55,
56] and a BiLSTM [57]. In our CNN-BiLSTM network, the PSSM
of a peptide is fed to a normalization layer to scale the input,
and then the scaled PSSM is processed by a two-dimensional
convolutional layer with a non-linear activation function (e.g.


_ATSE: a peptide toxicity predictor by exploiting structural and evolutionary_ 5



where _f_ _i_ denotes _i_ -th concatenated feature vector; _h_ denotes
the number of the concatenated feature vectors, which is determined by the number of GNN layers; _u_ denotes the length of
the concatenated feature vector. In this work, the attention
mechanism takes all the concatenated feature vectors _F_ as input
to calculate a weight vector _a_, in which each element is the
weight of the corresponding feature vector:


_a_ = _softmax_ � _w_ 2 tanh � _W_ 1 _F_ _[T]_ [��] (8)


_z_ = _aF_ (9)


where _W_ 1 denotes a weight matrix with the size _d_ × _u_ ; _w_ 2 is a
weight vector with the size _d_ . Note that _d_ is a hyperparameter,
which can be set randomly and fine-tuned during network training. The _softmax_ was used to ensure all the computed weights
sum up to 1 and _tanh_ is a non-linear activation function. To this
end, we multiplied the weight vector a with _F_, yielding the final
representation _z_ of a peptide sequence.


_**Output module**_


The feature vector _z_, generated by the attention module and
containing structural and evolutionary information, is fed
into a fully connected layer with a _relu_ activation function,
that is,


_y_ = _relu_ � _Wz_ + _b_ � (10)


where _W_ denotes the weight matrix and b denotes the
bias vector. Finally, a _softmax_ layer is added on the top of
the output _y_ = [ _y_ 0, _y_ 1 ] to perform the final prediction as
follows:



3. Only-GNN: This is a variant of ATSE. It consists of GNNs
and an output module. This variant only uses the structural information from the molecular graphs of peptides to
distinguish toxic and non-toxic peptides.
4. Only-CNN_BiLSTM: For this variant, it includes a CNN_
BiLSTM network and an output module and only takes the
PSSMs of peptides as input.


**Evaluation metrics**


To evaluate the performance of our proposed predictor, we used
six traditional evaluation metrics commonly used in binary classification tasks [60–64], including Sensitivity (SN), Specificity
(SP), False discovery rate (FDR), False positive rate (FPR), Accuracy
(ACC) and Mathew’s correlation coefficient (MCC). The metrics
are calculated as follows:



⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨

⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩



_TP_
_SN_ = _TP_ + _FN_


_TN_
_SP_ = _TN_ + _FP_


_FDR_ = _TPFP_ + _FP_


_FPR_ = _FPFP_ + _TN_
_ACC_ = _TP_ + _TNTP_ ++ _TNFP_ + _FN_



(12)



_TP_ × _TN_ − _FP_ × _FN_
_MCC_ = ~~√~~ _(TP_ + _FN)(TP_ + _FP)(TN_ + _FP)(TN_ + _FN)_



exp � _y_ _j_ �
_p_ _j_ =



where TP (true positive) and TN (true negative) represent the
numbers of correctly predicted positive samples and negative
samples, respectively; FP (false positive) and FN (false negative)
represent the numbers of incorrectly predicted positive samples
and negative samples, respectively. The metric SN measures
the prediction ability of a predictor for positive samples, while
the metric SP measures the ability of the predictor for negative
samples. FDR calculates the proportion of errors in the positive
samples predicted by the predictor, while FPR calculates the
proportion of negative samples that are mistaken as positives
by the predictor. ACC and MCC are used to evaluate the overall
performance of a predictor.
Moreover, the ROC (receiver operating characteristic) curve
and PR (precision-recall) curve are often used to intuitively evaluate the overall predictive performance of a predictor. Here,
we calculated the area under the ROC curve (auROC) and the
area under the PR curve (auPRC) to assess the overall predictive
performance. The values of auROC and auPRC are from 0.5 to
1. The larger the values of them, the better and more robust
performance.


Results and Discussion


**The proposed ATSE outperforms state-of-the-art**
**methods**


To evaluate the prediction performance of the proposed ATSE,
we carried out multiple comparison experiments between ATSE
and various baseline methods. The comparative results are
shown in Table 1. We can observe that when compared to the
machine learning-based methods (i.e. ClanTox, ToxinPred-RF
and ToxinPred-SVM), the proposed ATSE achieves significantly
higher performance than them in terms of all six metrics (SN, SP,
FDR, FPR, ACC and MCC). To be specific, ATSE achieves an SN of
0.965, an SP of 0.940, an ACC of 0.952 and an MCC of 0.903, generating a relative improvement over the runner-up ToxinPred-RF



~~�~~



(11)
_c_ [exp] ~~�~~ _y_ _c_ ~~�~~



where _j_ ∈{0, 1}, denotes the binary label (i.e. non-toxic or toxic)
and _p_ _j_ is the probability of a peptide belonging to category _j._


**Baseline methods**


To assess the effectiveness of the proposed ATSE, we compared
the ATSE with various baseline methods, including ClanTox

[18],ToxinPred [19], and the variants of ATSE.


1. ClanTox: ClanTox is a meta-classifier for short animal

toxins that is based on the sequence-driven global features
extracted from the original protein sequences. Given a
sequence, the ClanTox web server can identify whether
the sequence is a toxin or toxin-like protein. In this study,
we uploaded our test dataset in FASTA format to the web
server and obtained the prediction results of the uploaded
test dataset.

2. ToxinPred: ToxinPred is a traditional machine-learning
method for predicting the toxicity of peptides and proteins.
In the study [19], the authors used the dipeptide composition (DPC) as the representation of a peptide sequence to
train an SVM and got better performance than using other
manually extracted features. Therefore, here we used DPC
as the feature to train both an SVM and a random forest (RF),
which are denoted as ToxinPred-SVM and ToxinPred-RF,
respectively.


6 _Wei et al._


**Table 1.** The predictive performance of the proposed ATSE and baseline methods


Methods SN SP FDR FPR ACC MCC


ClanTox 0.855 0.888 0.132 0.113 0.872 0.743

ToxinPred-RF 0.918 (±0.016) 0.904 (±0.018) 0.094 (±0.017) 0.096 (±0.018) 0.911 (±0.010) 0.823 (±0.021)
ToxinPred-SVM 0.893 (±0.012) 0.924 (±0.016) 0.077 (±0.015) 0.076 (±0.016) 0.909 (±0.011) 0.817 (±0.023)
Only-GNN 0.869 (±0.008) 0.898 (±0.006) 0.112( ±0.005) 0.102 (±0.006) 0.885 (±0.002) 0.778 (±0.003)
Only-CNN_BiLSTM 0.947 (±0.012) 0.895 (±0.019) 0.114 (±0.017) 0.105 (±0.019) 0.919 (±0.007) 0.840 (±0.012)
ATSE (This study) **0.965 (** ± **0.003)** **0.940 (** ± **0.003)** **0.068 (** ± **0.003)** **0.060 (** ± **0.003)** **0.952 (** ± **0.002)** **0.903 (** ± **0.004)**


Note: except for the ClanTox method, we report the average after performing each experiment 20 times by splitting the dataset for other methods. The boldface
indicates the best performance among the compared methods.



of 5.12, 3.98, 4.50 and 9.72%, respectively. Meanwhile, in terms
of FDR and FPR, ATSE reduces the FDR from 0.094 to 0.068 (a
relative reduction of 27.7%), and the FPR from 0.096 to 0.060
(a relative reduction of 37.5%), demonstrating the superiority
of ATSE as compared with these machine learning-based
methods.

Next, we investigated the effect of different information (i.e.
structural information and evolutionary information) on the
performance. We compared our proposed ATSE with Only-GNN
and Only-CNN_BiLSTM, respectively. Note that the GNN network
represents the structural information, while the CNN_BiLSTM
network encodes the evolutionary information. The comparative
results are presented in Table 1. We observed that OnlyCNN_BiLSTM outperforms Only-GNN in three metrics (SN,
ACC and MCC), which suggests that evolutionary information
is more effective than structural information to capture
the characteristics of peptide toxicity. When integrating the
information from the two networks, the predictive performance
is improved. Specifically, the SN, SP, ACC and MCC of the
proposed ATSE are 1.90, 5.03, 3.59 and 7.50% higher than ones
of Only-CNN_BiLSTM. Moreover, the ATSE reduces the FDR and
FPR of Only-CNN_BiLSTM from 0.114 to 0.068 and from 0.105
to 0.060, respectively, demonstrating that ATSE exhibits better
performance than Only-CNN_BiLSTM. To this end, it can be
concluded that the structural information and evolutionary
information are complementary, and combining them is of
benefit to performance improvement. For intuitive comparison,
we further compared the ROC and PR curves of all these methods
as illustrated in Figure 2. Figure 2B shows a partial zoomed-in
view of Figure 2A and Figure 2D shows a partial zoomed-in view
of Figure 2C. As shown, we can see that ATSE has the highest
auROC and auPRC, which are 0.976 and 0.965, respectively.
However, since the use of evolutionary information and deep
neural networks, the running efficacy of our model is actually
worse than existing methods, which use handcrafted sequential features. The comparison of running time can be seen in
[Supporting Information (Figure S1). Despite this, the above com-](https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbab041#supplementary-data)
parison results demonstrate that the proposed ATSE is actually
better than all competing methods in the following two aspects.
First, our prediction accuracy is the highest, resulting in fewer
false negatives and false positives. On the other hand, our model
provides completely automated training and prediction without
any need for any professional knowledge, while existing models
use the features designed based on experience to characterize
peptides. All in all, our ATSE holds the potential to be an effective
predictor for peptide toxicity prediction.



**Effectiveness of attention mechanism**


In this section, we firstly used learning curves to analyze the
effect of the number of GNN layers, in which the _x_ -axis is the
epoch and the y-axis is the score of corresponding metrics.
The results are illustrated in Figure 3. We can see that when
the number of GNN layers is set to 4, the values of SN, SP, ACC
and MCC are significantly higher than ones of other layers, and
the values of FDR and FPR are lower than ones of other layers.
When the number of layers is set to lower or higher than 4,
the performance of ATSE begins to decline. Therefore, according
to the learning curves in Figure 3, we set the number of GNN
layers to 4 in our model ATSE, which can achieve outstanding
performance.
Secondly, we investigated if introducing the attention
mechanism in our feature learning scheme could improve
the feature representation ability and predictive performance.
We did a series of comparison experiments and the detailed
comparative results of the six metrics are shown in Table 2.
As seen in Table 2, layer1, layer2, layer3 and layer4 methods
denote that they combine the output of the corresponding
GNN layer with the output of the CNN_BiLSTM network, and
directly feed the combined features to the output module to
predict the toxicity of a peptide. Concat_all method denotes that
it combines the output of every GNN layer and the output of
CNN_BiLSTM network linearly, and feed the combined features
to the output module. From Table 2, we can observe that amongst
the six methods, our proposed ATSE gives the best performance
in terms of six metrics, achieving SN of 0.965, SP of 0.940,
FDR of 0.068, FPR of 0.060, ACC of 0.952 and MCC of 0.903. It
is worth noting that combining all the features linearly (i.e.
Cancat_all method) not only does not improve the predictive
performance but makes it worse. The reason may be that directly
combining all features causes data redundancy to reduce
the effectiveness of the predictive model, and meanwhile, it
also increases the data dimension to result in longer training
time.

In addition, from ROC and PR curves of these six methods
shown in Figure 4, we can see that ROC and PR curves of ATSE are
closed to ones of other methods across the whole range, but the
auROC (0.976) and auPRC (0.965) of ATSE are still slightly higher
than ones of other methods, which further demonstrates the
effectiveness of attention mechanism. From the results above,
we can conclude that introducing the attention mechanism to
extract different aspects of peptides into a vector representation
not only reduces the dimension of features but also obtains


_ATSE: a peptide toxicity predictor by exploiting structural and evolutionary_ 7


**Figure 2.** ROC curves and PR curves of ATSE and baseline methods. ( **A** ) ROC curves of the proposed ATSE and baseline methods. ( **B** ) It shows the same as ( **A** ), but
zoomed-in on an interesting region. ( **C** ) PR curves of the proposed ATSE and baseline methods. ( **D** ) It shows the same as ( **C** ), but zoomed-in on an interesting region.


**Table 2.** The predictive performance of the proposed ATSE and other methods using different combining features


Methods SN SP FDR FPR ACC MCC


Layer1 0.962 (±0.002) 0.905 (±0.003) 0.103 (±0.003) 0.095 (±0.003) 0.933 (±0.002) 0.868 (±0.004)
Layer2 0.957 (±0.003) 0.903 (±0.003) 0.104 (±0.03) 0.097 (±0.003) 0.928 (±0.002) 0.858 (±0.004)
Layer3 0.950 (±0.004) 0.924 (±0.003) 0.085 (±0.003) 0.076 (±0.003) 0.936 (±0.002) 0.872 (±0.004)
Layer4 0.946 (±0.004) 0.918 (±0.003) 0.091 (±0.003) 0.082 (±0.003) 0.931 (±0.003) 0.863 (±0.006)
Concat_all 0.956 (±0.002) 0.898 (±0.003) 0.110 (±0.003) 0.102 (±0.003) 0.925 (±0.002) 0.852 (±0.004)
ATSE **0.965 (** ± **0** . **003)** **0.940 (** ± **0** . **003)** **0.068** (± **0** . **003** ) **0.060** (± **0** . **003** ) **0.952** (± **0** . **002** ) **0.903 (** ± **0** . **004)**


Note: we report the average after performing each experiment 20 times by splitting the dataset for each method. The boldface indicates the best performance among
the compared methods.


8 _Wei et al._


**Figure 3.** Learning curves of the various number of the GNN layers. ( **A** ) Learning curves of SN. ( **B** ) Learning curves of SP. ( **C** ) Learning curves of FDR. ( **D** ) Learning curves

of FPR. ( **E** ) Learning curves of ACC. ( **F** ) Learning curves of MCC.



discriminative features to further improve the predictive performance of our ATSE to some extent.


**Effectiveness and visualization of the features extracted**
**by the proposed ATSE and hand-crafted features**


The proposed ATSE mainly utilizes two modules to automatically learn and extract inherent features from molecular graphs
and evolutionary profiles. To evaluate the effectiveness of the
features learned by ATSE, we compared our features with five
typical hand-crafted features, including amino acid composition
(AAC), adaptive skip dipeptide composition (ASDC), Atomic and
bond composition (ATC) [65], pseudo-amino acid composition
(Pse-AAC) [66, 67] and amphiphilic pseudo amino acid composition (APAAC) [66–68]. Moreover, we used six basic classifiers (RF,
SVM (with RBF kernel), gaussian naive bayes (GNB), LightGBM

[69], logistic regression (LR) and k-nearest neighbors (KNN)) to
carry out the comparison experiments. Note that these comparison experiments were carried out on a randomly splitting
dataset, accounting for 15% of the entire dataset, by using the
tenfold cross-validation technique. Figure 5 shows the comparison results of the six features on the six basic classifiers in terms

of four metrics (SN, SP, ACC and MCC). The features generated by
our ATSE are denoted as ‘Proposed’.
As shown in Figure 5, we can see that as for each classifier,
the performance of Proposed significantly outperforms the other
five hand-crafted features in terms of the four metrics, especially on the classifiers LR and GNB, indicating that the features
generated by ATSE are more effective for peptide toxicity prediction and are more suitable for most of the common classifiers.

Compared with the traditional hand-crafted features that rely on
prior knowledge, the Proposed is automatically learned through
different networks to explore structural information and evolutionary information, which demonstrates that the high-latent
information learned from neural networks can better character
ize peptides and is critical for the predictive performance. The
possible reason is that the biological functions of peptides are



dependent on their structures and evolutionary information. In
our model, we considered extracting the structural information
instead of amino acid sequence information and employed the
GNN to extract potential discriminative structural information
from molecular graphs to characterize peptides. Moreover, an
evolutionary profile (e.g. PSSM) is usually derived from a set
of aligned sequences functionally related, which attempts to
capture the intrinsic evolutionary conservation characteristics
of sequence patterns. Here, we used both structural information
in molecular graphs and evolutionary information in PSSMs to
characterize the peptide toxicity, making the learned features
have more rich biological significance than traditional handcrafted sequence-based features. Therefore, the proposed ATSE
can capture the discriminative features of peptide toxicity, which
can provide useful information for researchers of interest in
exploring the toxicity of peptides.
To further intuitively explain the reason why the toxinspecific features extracted by ATSE show higher discriminative
power, we visualize all six features on the randomly splitting
dataset. Here, t-SNE [70] is employed to reduce the dimensions of
features into two-dimension (2D) to visualize their distributions.
The results are presented in Figure 6. From Figure 6, we can
observe that compared with the five manual features, the
features extracted by ATSE can greatly make more positive
and negative samples cluster and just make a lower number of
samples not apart from each other, indicating that the features
automatically extracted by ATSE are more discriminative for
peptide toxicity prediction.


**Comparing the effectiveness of PSSM and BLOSUM62**


The BLOcks SUbstitution Matrix (BLOSUM) is an amino acid
substitution matrix, first proposed by Steven Henikoff and Jorja
Henikoff [71]. The scores in the matrix represent the probability
of one amino acid to change into another in blocks of local alignments in related proteins. Each matrix is regarded as a particular
evolutionary distance. For example, the BLOSUM62 matrix is


_ATSE: a peptide toxicity predictor by exploiting structural and evolutionary_ 9


**Figure 4.** The ROC curves and PR curves of ATSE and other methods using different combined features. ( **A** ) ROC curves of the proposed ATSE and the compared methods.
( **B** ) It shows the same as ( **A** ), but zoomed-in on an interesting region. ( **C** ) PR curves of the proposed ATSE and the compared methods. ( **D** ) It shows the same as ( **C** ), but
zoomed-in on an interesting region.



built by using sequences sharing no more than 62% identity. The
sequences with more than 62% identity are eliminated to avoid
the bias of the result in favor of a certain protein. Then, the PSSM
can be taken as an extension of the substitution scoring matrix,
which considers the possibility of a certain amino acid appearing
in a specific position in the amino acid sequence. Therefore,
a pair of amino acids substitution at different positions may
receive different scores. This is in contrast to a BLOSUM matrix,
in which a pair of amino acids substitution receives the same
score no matter in what position it appears.
Therefore, to evaluate the effectiveness of PSSM, we used
BLOSUM62 instead of PSSM to characterize the evolutionary
information of the peptide sequences and did comparison experiments. Here, each amino acid of a peptide is encoded by the



corresponding row of BLOSUM62. Subsequently, each peptide is
encoded into a 50 × 20 matrix, where 50 is the length of a peptide
and 20 is the number of standard amino acids. Notably, if the
length of a peptide is less than 50, zero-padding is performed
on the encoded matrix to reach a fixed size. The results are

shown in Figure 7. As shown in Figure 7, We can see that compared with the model using BLOSUM62, our model using PSSM
achieves a significant improvement in terms of each metric.
Both the proposed ATSE and the method using BLOSUM62 are
based on the same architecture, but the proposed ATSE obviously outperformed the method using BLOSUM62, suggesting
that the features extracted from PSSM contain more effective

evolutionary information of peptides and mainly contribute to
the performance improvement.


10 _Wei et al._


**Figure 5.** The ten-fold cross validation results of Proposed, AAC, ASDC, ATC, Pse-AAC and APAAC are based on the six basic classifiers. ( **A** ) Results based on RF. ( **B** )
Results based on SVM. ( **C** ) Results based on GNB. ( **D** ) Results based on LightGBM. ( **E** ) Results based on LR. ( **F** ) Results based on KNN.


**Figure 6.** Feature visualization of ATSE and other five hand-crafted features. ( **A** ) Feature visualization of Proposed. ( **B** ) Feature visualization of AAC. ( **C** ) Feature
visualization of ASDC. ( **D** ) Feature visualization of ATC. ( **E** ) Feature visualization of Pse-AAC. ( **F** ) Feature visualization of APAAC. The positive samples and negative

samples are shown in red color and blue color, respectively.



**Web server establishment**


For the benefit of the research community in the prediction
of toxic peptides, we established a user-friendly web server to
implement the proposed ATSE. The server is currently available
[online at http://server.malab.cn/ATSE. In this web server, users](http://server.malab.cn/ATSE)
just need to enter query peptide sequences into the main box or
upload a FASTA file with peptide sequences, and next press the
‘Submit’ button and begin the classification. After that, wait a



few moments and the prediction results are presented on a new
web page.


Conclusion


Peptide toxicity prediction is essential for facilitating the designing of therapeutic peptides with desired toxicity. In this study, we
propose a predictor ATSE, a deep learning-based algorithm for


_ATSE: a peptide toxicity predictor by exploiting structural and evolutionary_ 11


Funding


This work was supported in part by the New Energy
and Industrial Technology Development Organization 265
(NEDO) and the Japan Society for the Promotion of Science
(JSPS), Grants-in-Aid for Scientific Research under Grant
18H03250 and the Natural Science Foundation of China (nos
62072329 and 62071278).


References



**Figure 7.** Predictive performance of models using PSSM and BLOSUM62.


peptide toxicity prediction, which can automatically extract the
discriminative features. This algorithm firstly extracts structural
and evolutionary information from the molecular graphs and
PSSMs of peptides, respectively. Secondly, it linearly combines
the output of each GNN layer with the output of the CNN_BiLSTM
network. Afterwards, it sends the set of combined feature vectors
to the attention module to optimize the feature vectors. Finally,
the optimized feature vector is fed into the output module to predict the toxicity of the peptide. Experimental results show that
the proposed ATSE achieves the top performance as compared
with several baseline methods. Besides, compared with manual
features used in traditional machine learning methods, the features learned by ATSE have more discriminative power. Finally,
we have established a user-friendly and freely accessible web
[server at http://server.malab.cn/ATSE. We believe that this study](http://server.malab.cn/ATSE)
provides new insights into automatic representation learning to
construct a robust predictor in bioinformatics.


**Key Points**


   - In this study, we propose a deep learning-based
method called ATSE to improve the prediction of peptide toxicity.

   - Unlike existing methods trained based on handcrafted features, the proposed ATSE automatically
learns and encodes structural and evolutionary information from molecular graphs and evolutionary profiles, respectively. Particularly, the attention mechanism introduced in our model is able to effectively
improve the feature representation capability.

   - Comparative studies show that the proposed ATSE
significantly outperforms existing predictors for the
prediction of peptide toxicity.

   - We develop a web server for the implementation of the
proposed ATSE, which can provide a high-throughput
prediction of peptide toxicity. It is publicly accessible
[at http://server.malab.cn/ATSE.](http://server.malab.cn/ATSE)


Supplementary Data


[Supplementary data are available online at https://academi](https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbab041#supplementary-data)
[c.oup.com/bib.](https://academic.oup.com/bib)



1. Craik DJ, Fairlie DP, Liras S, _et al._ The future of peptide-based
drugs. _Chem Biol Drug Des_ 2013; **81** :136–47.
2. Haggag YA, Donia AA, Osman MA, El-Gizawy SA. Peptides
as drug candidates: limitations and recent development
perspectives. _Biomed J_ 2018; **1** :3.
3. Buchwald H, Dorman RB, Rasmus NF, _et al._ Effects on GLP-1,
PYY, and leptin by direct stimulation of terminal ileum and
cecum in humans: implications for ileal transposition. _Surg_
_Obes Relat Dis_ 2014; **10** :780–6.
4. Fosgerau K, Hoffmann T. Peptide therapeutics: current status and future directions. _Drug Discov Today_ 2015; **20** :122–8.
5. Giordano C, Marchiò M, Timofeeva E, _et al._ Neuroactive peptides as putative mediators of antiepileptic ketogenic diets.
_Front Neurol_ 2014; **5** :63.
6. Padhi A, Sengupta M, Sengupta S, _et al._ Antimicrobial peptides and proteins in mycobacterial therapy: current status
and future prospects. _Tuberculosis_ 2014; **94** :363–73.
7. Lee CL, Harris JL, Khanna KK, _et al._ A comprehensive review
on current advances in peptide drug development and
design. _Int J Molecular Ences_ 2019; **20** :2383.
8. Benson N, Cucurull-Sanchez L, Demin O, _et al._ Reducing
systems biology to practice in pharmaceutical company
research; selected case studies. In: _Advances in Systems Biol-_
_ogy_ . Springer, 2012, 607–15.
9. Chames P, Van Regenmortel M, Weiss E, _et al._ Therapeutic
antibodies: successes, limitations and hopes for the future.
_Br J Pharmacol_ 2009; **157** :220–33.
10. Marqus S, Pirogova E, Piva TJ. Evaluation of the use of therapeutic peptides for cancer treatment. _J Biomed Sci_ 2017; **24** :21.
11. Vlieghe P, Lisowski V, Martinez J, _et al._ Synthetic therapeutic peptides: science and market. _Drug Discov Today_
2010; **15** :40–56.
12. Gupta S, Kapoor P, Chaudhary K, _et al._ Peptide toxicity prediction. _Computational Peptidology_ . New York, NY: Humana Press,
2015, 143–57.
13. Gentilucci L, De Marco R, Cerisoli L. Chemical modifications
designed to improve peptide stability: incorporation of nonnatural amino acids, pseudo-peptide bonds, and cyclization.
_Curr Pharm Des_ 2010; **16** :3185–203.
14. Chen F, Ma B, Yang Z-C, _et al._ Extraordinary metabolic stability of peptides containing _α_ -aminoxy acids. _Amino Acids_
2012; **43** :499–503.
15. Gupta S, Ansari HR, Gautam A, _et al._ Identification of Bcell epitopes in an antigen for inducing specific class of
antibodies. _Biol Direct_ 2013; **8** :27.
16. Riley TP, Keller GL, Smith A, _et al._ Structure based prediction
of neoantigen immunogenicity. _Front Immunol_ 2047; **2019** :10.
17. Yadav M, Jhunjhunwala S, Phung QT, _et al._ Predicting
immunogenic tumour mutations by combining mass spectrometry and exome sequencing. _Nature_ 2014; **515** :572–6.
18. Naamati G, Askenazi M, Linial M. ClanTox: a classifier of
short animal toxins. _Nucleic Acids Res_ 2009; **37** :W363–8.


12 _Wei et al._


19. Gupta S, Kapoor P, Chaudhary K, _et al._ In silico approach
for predicting toxicity of peptides and proteins. _PLoS One_
2013; **8** :e73957.
20. Wei L, Tang J, Zou Q. SkipCPP-Pred: an improved and promising sequence-based predictor for predicting cell-penetrating
peptides. _BMC Genomics_ 2017; **18** :1–11.
21. Zhang J, Liu B. A review on the recent developments of
sequence-based protein feature extraction methods. _Curr_
_Bioinforma_ 2019; **14** :190–9.
22. Tan JX, Li SH, Zhang ZM, _et al._ Identification of hormone
binding proteins based on machine learning methods. _Math_
_Biosci Eng_ 2019; **16** :2466–80.
23. Tang H, Zhao YW, Zou P, _et_ _al._ HBPred: a tool to
identify growth hormone-binding proteins. _Int J Biol Sci_
2018; **14** :957–64.
24. Yan K, Fang X, Xu Y, _et al._ Protein fold recognition based on
multi-view modeling. _Bioinformatics_ 2019; **35** :2982–90.
25. Yan K, Wen J, Xu Y, _et al._ Protein fold recognition based
on auto-weighted multi-view graph embedding learning
model. _IEEE/ACM Transactions on Computational Biology and_
_Bioinformatics_ . Washington DC, USA: IEEE Computer Society
Press, 2020.
26. He Y, Maisuradze GG, Yin Y, _et al._ Sequence-, structure-, and
dynamics-based comparisons of structurally homologous
CheY-like proteins. _Proc Natl Acad Sci_ 2017; **114** :1578–83.
27. Liu B, Li C-C, Yan K. DeepSVM-fold: protein fold recognition by combining support vector machines and pairwise
sequence similarity scores generated by deep learning networks. _Brief Bioinform_ 2020; **21** :1733–41.
28. Li CC, Liu B. MotifCNN-fold: protein fold recognition based
on fold-specific features extracted by motif-based convolutional neural networks. _Brief Bioinform_ 2020; **21** :2133–41.
29. Tsubaki M, Tomii K, Sese J. Compound–protein interaction prediction with end-to-end learning of neural networks for graphs and sequences. _Bioinformatics_ 2019; **35** :

309–18.

30. Zhu H, Du X, Yao Y. ConvsPPIS: identifying protein-protein
interaction sites by an ensemble convolutional neural network with feature graph. _Curr Bioinforma_ 2020; **15** :368–78.
31. Zeng X, Zhu S, Liu X, _et al._ deepDR: a network-based deep
learning approach to in silico drug repositioning. _Bioinformat-_
_ics_ 2019; **35** :5191–8.
32. Zeng X, Zhu S, Lu W, _et al._ Target identification among known
drugs by deep learning from heterogeneous networks. _Chem_
_Sci_ 2020; **11** :1775–97.
33. Zeng X, Zhu S, Hou Y, _et al._ Network-based prediction of
drug–target interactions using an arbitrary-order proximity
embedded deep forest. _Bioinformatics_ 2020; **36** :2805–12.
34. Senior AW, Evans R, Jumper J, _et al._ Improved protein structure prediction using potentials from deep learning. _Nature_
2020; **577** :706–10.
35. Kandathil SM, Greener JG, Jones DT. Recent developments
in deep learning applied to protein structure prediction. _Pro-_
_teins: Structure, Function, and Bioinformatics_ 2019; **87** :1179–89.
36. Smolarczyk T, Roterman-Konieczna I, Stapor K. Protein secondary structure prediction: a review of progress and directions. _Curr Bioinforma_ 2020; **15** :90–107.
37. Jin S, Zeng X, Xia F, _et al._ Application of deep learning
methods in biological networks. _Brief Bioinform_ 2020. doi:
[10.1093/bib/bbaa043.](https://doi.org/10.1093/bib/bbaa043.)

38. Wang J, Ma A, Ma Q, Xu D, Joshi T. Inductive inference
of gene regulatory network using supervised and semisupervised graph neural networks. _Comput Struct Biotechnol_
_J_ 2020; **18** :3335–43.



39. Wang J, Ma A, Chang Y, _et al._ scGNN: a novel graph neural
network framework for single-cell RNA-Seq analyses. _bioRxiv_
2020; **23** [. doi: 10.1101/2020.08.02.233569.](https://doi.org/10.1101/2020.08.02.233569)
40. Ye X, Zhang W, Futamura Y, _et al._ Detecting interactive gene
groups for single-cell RNA-Seq data based on co-expression
network analysis and subgraph learning. _Cell_ 2020; **9** :

1938.

41. Ye X, Sakurai T. Robust similarity measure for spectral
clustering based on shared Neighbors. _ETRI J_ 2016; **38** :540–50.
42. Li S, Chen J, Liu B. Protein remote homology detection based
on bidirectional long short-term memory. _BMC Bioinformatics_
2017; **18** :443.
43. Xing P, Su R, Guo F, _et al._ Identifying N 6-methyladenosine
sites using multi-interval nucleotide pair position specificity
and support vector machine. _Sci Rep_ 2017; **7** :46757.
44. Zhang T, Tan P, Wang L, _et al._ RNALocate: a resource for RNA
subcellular localizations. _Nucleic Acids Res_ 2017; **45** :D135–8.
45. Liang ZY, Lai HY, Yang H, _et al._ Pro54DB: a database for
experimentally verified sigma-54 promoters. _Bioinformatics_
2017; **33** :467–9.
46. Kaas Q, Yu R, Jin A-H, _et al._ ConoServer: updated content,
knowledge, and discovery tools in the conopeptide database.
_Nucleic Acids Res_ 2012; **40** :D325–30.
47. Wood DL, Miljenovic T, Cai S, _et al._ ArachnoServer: a database
of protein toxins from spiders. _BMC Genomics_ 2009; **10** :375.
48. Apweiler R, Bairoch A, Wu CH, _et al._ UniProt: the universal
protein knowledgebase. _Nucleic Acids Res_ 2004; **32** :D115–9.
49. Li W, Godzik A. Cd-hit: a fast program for clustering and
comparing large sets of protein or nucleotide sequences.
_Bioinformatics_ 2006; **22** :1658–9.
50. Landrum G. _RDKit: A software suite for cheminformatics, com-_
_putational chemistry, and predictive modeling_ . Cambridge, Massachusetts, USA: Academic Press, 2013.
51. Altschul SF, Madden TL, Schäffer AA, _et al._ Gapped BLAST
and PSI-BLAST: a new generation of protein database search
programs. _Nucleic Acids Res_ 1997; **25** :3389–402.
52. Zhu XJ, Feng CQ, Lai HY, _et al._ Predicting protein structural
classes for low-similarity sequences by evaluating different
features. _Knowl-Based Syst_ 2019; **163** :787–93.
53. An J-Y, Zhou Y, Zhang L, _et al._ Improving self-interacting proteins prediction accuracy using protein evolutionary information and weighed-extreme learning machine. _Curr Bioin-_
_forma_ 2019; **14** :115–22.
54. Morris C, Ritzert M, Fey M _et al._ Weisfeiler and leman go neural: Higher-order graph neural networks. In: _Proceedings of the_
_AAAI Conference on Artificial Intelligence_ . Palo Alto, California
USA: the AAAI Press, 2019, p. 4602–9.
55. Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. _Commun ACM_
2017; **60** :84–90.
56. Long H, Wang M, Fu H. Deep convolutional neural networks
for predicting Hydroxyproline in proteins. _Curr Bioinforma_
2017; **12** :233–8.
57. Hochreiter S, Schmidhuber J. Long short-term memory. _Neu-_
_ral Comput_ 1997; **9** :1735–80.
58. Lindsay GW. Attention in psychology. _Neuroscience, and_
_Machine_ _Learning, Frontiers_ _in_ _Computational_ _Neuroscience_
2020; **14** :29.
59. Hong Z, Zeng X, Wei L, _et_ _al._ Identifying enhancer–
promoter interactions with neural network based on pretrained DNA vectors and attention mechanism. _Bioinformat-_
_ics_ 2020; **36** :1037–43.
60. Charoenkwan P, Kanthawong S, Nantasenamat C, _et al._
iDPPIV-SCM: a sequence-based predictor for identifying and


_ATSE: a peptide toxicity predictor by exploiting structural and evolutionary_ 13



analyzing dipeptidyl peptidase IV (DPP-IV) inhibitory peptides using a scoring card method. _J Proteome Res_ 2020;

**19** :4125–36.

61. Charoenkwan P, Kanthawong S, Nantasenamat C,
_et al._ iAMY-SCM: improved prediction and analysis of
amyloid proteins using a scoring card method with
propensity scores of dipeptides. _Genomics_ 2021; **113** :

689–98.

62. Charoenkwan P, Yana J, Nantasenamat C, _et al._ iUmamiSCM: a novel sequence-based predictor for prediction and
analysis of umami peptides using a scoring card method
with propensity scores of dipeptides. _J Chem Inf Model_ 2020;

**60** :6666–78.

63. Charoenkwan P, Yana J, Schaduangrat N, _et al._ iBitter-SCM:
identification and characterization of bitter peptides using
a scoring card method with propensity scores of dipeptides.
_Genomics_ 2020; **112** :2813–22.
64. Ye X, Zhang W, Sakurai T. _Adaptive Unsupervised Feature_
_Learning for Gene Signature Identification in Non-small-cell Lung_
_Cancer_ . IEEE Access, 2020, 1–1.



65. Kumar R, Chaudhary K, Singh Chauhan J, _et al._ An in silico
platform for predicting, screening and designing of antihypertensive peptides. _LA Rep_ 2015; **5** :12512.
66. Jain S, Ranjan P, Sengupta D, _et al._ TpPred: a tool for hierarchical prediction of transport proteins using cluster of neural
networks and sequence derived features. _IJCB_ 2014; **1** :28–36.
67. Chou KC. Prediction of protein cellular attributes using
pseudo-amino acid composition, proteins: structure. _Func-_
_tion, and Bioinformatics_ 2001; **43** :246–55.
68. Chou K. Using amphiphilic pseudo amino acid composition to predict enzyme subfamily classes. _Bioinformatics_
2005; **21** :10–9.
69. Ke G, Meng Q, Finley T, _et al._ Lightgbm: A highly efficient gradient boosting decision tree. In: _Advances in neural informa-_
_tion processing systems_ . Long Beach, California, USA: Curran
Associates Inc., 2017, 3146–54.
70. LVD M, Hinton G. Visualizing data using t-SNE. _J Mach Learn_
_Res_ 2008; **9** :2579–605.
71. Henikoff S, Henikoff JG. Amino acid substitution matrices
from protein blocks. _Proc Natl Acad Sci_ 1992; **89** :10915–9.


